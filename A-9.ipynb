{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca171f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# from imblearn.over_sampling import SMOTE, SVMSMOTE\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, KBinsDiscretizer, Normalizer, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "from sklearn.decomposition import PCA, NMF, SparsePCA, KernelPCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebeddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_S = SVC(kernel='rbf', C=1.0, gamma='scale', verbose=False)\n",
    "classifier_R = RandomForestClassifier(verbose=False)\n",
    "classifier_D = DecisionTreeClassifier()\n",
    "classifier_G = GradientBoostingClassifier(verbose=False)\n",
    "classifier_K = KNeighborsClassifier()\n",
    "classifier_GNB = GaussianNB()\n",
    "classifier_GB = GradientBoostingClassifier(verbose=False)\n",
    "classifier_NN = MLPClassifier()\n",
    "classifier_IDA = QuadraticDiscriminantAnalysis()\n",
    "classifier_QDA = QuadraticDiscriminantAnalysis()\n",
    "classifier_ADB = AdaBoostClassifier()\n",
    "classifier_GP = GaussianProcessClassifier()\n",
    "classifier_XGBC = XGBClassifier(verbosity=0)\n",
    "classifier_LGBM = LGBMClassifier()\n",
    "classifier_BC = BaggingClassifier(verbose=False)\n",
    "classifier_CB = CatBoostClassifier(iterations=100, verbose=False)\n",
    "\n",
    "classifiers = [\n",
    "     ('SVC', classifier_S),\n",
    "     ('RF', classifier_R),\n",
    "     ('DT', classifier_D),\n",
    "       ('GB', classifier_G),\n",
    "     ('KNN', classifier_K),\n",
    "       ('GNB', classifier_GNB),\n",
    "     ('GBC', classifier_GB),\n",
    "     ('NN', classifier_NN),\n",
    "     ('IDA', classifier_IDA),\n",
    "     ('QDA', classifier_QDA),  \n",
    "      ('ADB', classifier_ADB),\n",
    "#      ('GP', classifier_GP),\n",
    "     ('XGB', classifier_XGBC),\n",
    "     ('LGB', classifier_LGBM),\n",
    "     ('BC', classifier_BC),\n",
    "     ('CB', classifier_CB),     \n",
    "]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Fairness_Metrics_Computation(y1, y2, y, attribute):\n",
    "    # Convert inputs to pandas Series for easier handling\n",
    "    y1 = pd.Series(y1)\n",
    "    y2 = pd.Series(y2)\n",
    "    y = pd.Series(y)\n",
    "    attribute = pd.Series(attribute)\n",
    "\n",
    "    # Identify privileged and unprivileged groups\n",
    "    privileged = (attribute == 'White') | (attribute == 4)\n",
    "    unprivileged = ~privileged\n",
    "\n",
    "    # Counts of privileged and unprivileged groups\n",
    "    count_privileged = np.sum(privileged)\n",
    "    count_unprivileged = np.sum(unprivileged)\n",
    "\n",
    "    # SPD Calculation\n",
    "    SFC_SPD_P = np.sum([(1 if y1_i == 1 and y2_i == 0 else -1 if y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y1_i, y2_i, attr in zip(y1, y2, attribute) if attr == 'White' or attr == 4])\n",
    "    SFC_SPD_NP = np.sum([(1 if y1_i == 1 and y2_i == 0 else -1 if y1_i == 0 and y2_i == 1 else 0)\n",
    "                         for y1_i, y2_i, attr in zip(y1, y2, attribute) if attr != 'White' and attr != 4])\n",
    "\n",
    "    SFR_SPD_p = SFC_SPD_P / count_privileged if count_privileged > 0 else 0\n",
    "    SFR_SPD_np = SFC_SPD_NP / count_unprivileged if count_unprivileged > 0 else 0\n",
    "    SPD = SFR_SPD_np - SFR_SPD_p\n",
    "\n",
    "    # EOD Calculation\n",
    "    SFC_EOD_P = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'White' or attr == 4])\n",
    "    SFC_EOD_NP = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                         for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'White' and attr != 4])\n",
    "\n",
    "    P_Y1 = np.sum((y == 1) & privileged)\n",
    "    NP_Y1 = np.sum((y == 1) & unprivileged)\n",
    "\n",
    "    SFR_EOD_p = SFC_EOD_P / P_Y1 if P_Y1 > 0 else 0\n",
    "    SFR_EOD_np = SFC_EOD_NP / NP_Y1 if NP_Y1 > 0 else 0\n",
    "    EOD = SFR_EOD_np - SFR_EOD_p\n",
    "\n",
    "    # AOD Calculation\n",
    "    SFC_TP_P = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'White' or attr == 4])\n",
    "    SFC_TP_NP = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'White' and attr != 4])\n",
    "    SFC_FP_P = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'White' or attr == 4])\n",
    "    SFC_FP_NP = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'White' and attr != 4])\n",
    "\n",
    "    P_Y0 = np.sum((y == 0) & privileged)\n",
    "    NP_Y0 = np.sum((y == 0) & unprivileged)\n",
    "\n",
    "    SFR_AOD_p = (SFC_TP_P / P_Y1 if P_Y1 > 0 else 0) + (SFC_FP_P / P_Y0 if P_Y0 > 0 else 0)\n",
    "    SFR_AOD_np = (SFC_TP_NP / NP_Y1 if NP_Y1 > 0 else 0) + (SFC_FP_NP / NP_Y0 if NP_Y0 > 0 else 0)\n",
    "    AOD = (SFR_AOD_np - SFR_AOD_p) / 2\n",
    "\n",
    "    # ERD Calculation\n",
    "    SFC_FN_P = np.sum([(-1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'White' or attr == 4])\n",
    "    SFC_FN_NP = np.sum([(-1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'White' and attr != 4])\n",
    "    SFC_FP_P = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'White' or attr == 4])\n",
    "    SFC_FP_NP = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'White' and attr != 4])\n",
    "\n",
    "    SFR_ERR_p = (SFC_FP_P / P_Y0 if P_Y0 > 0 else 0) + (SFC_FN_P / P_Y1 if P_Y1 > 0 else 0)\n",
    "    SFR_ERR_np = (SFC_FP_NP / NP_Y0 if NP_Y0 > 0 else 0) + (SFC_FN_NP / NP_Y1 if NP_Y1 > 0 else 0)\n",
    "    ERD = SFR_ERR_np - SFR_ERR_p\n",
    "\n",
    "    return SPD, EOD, AOD, ERD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f3323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_path = 'C:/Users/Saadia/FairPreprocessing/data/adult/adult.data'\n",
    "test_path = 'C:/Users/Saadia/FairPreprocessing/data/adult/adult.test'\n",
    "\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', \n",
    "                'capital-loss', 'hours-per-week', 'native-country', 'income-per-year']\n",
    "na_values = ['?']\n",
    "\n",
    "# Read train and test datasets\n",
    "train = pd.read_csv(train_path, header=None, names=column_names, skipinitialspace=True, na_values=na_values)\n",
    "test = pd.read_csv(test_path, header=None, names=column_names, skipinitialspace=True, na_values=na_values)\n",
    "\n",
    "# Map target variable to binary\n",
    "train['income-per-year'] = train['income-per-year'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\n",
    "test['income-per-year'] = test['income-per-year'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\n",
    "\n",
    "# Combine data for consistent preprocessing\n",
    "combined_data = pd.concat([train, test])\n",
    "combined_data.dropna(subset=['income-per-year'], inplace=True)\n",
    "\n",
    "X_combined = combined_data.drop('income-per-year', axis=1)\n",
    "y_combined = combined_data['income-per-year']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_combined.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Convert categorical columns to string for OneHotEncoder compatibility\n",
    "X_combined[categorical_cols] = X_combined[categorical_cols].astype(str)\n",
    "\n",
    "# Split data into stratified and non-stratified sets\n",
    "X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "    X_combined, y_combined, test_size=0.3, random_state=42, stratify=y_combined)\n",
    "X_train_no_strat, X_test_no_strat, y_train_no_strat, y_test_no_strat = train_test_split(\n",
    "    X_combined, y_combined, test_size=0.3, random_state=42)\n",
    "\n",
    "# Custom Feature Engineering Transformer\n",
    "class EnhancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.feature_names_in_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, 'columns'):\n",
    "            self.feature_names_in_ = X.columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if hasattr(X, 'toarray'):\n",
    "            X = pd.DataFrame(X.toarray(), columns=self.feature_names_in_)\n",
    "        \n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # Log transformation, binary indicator, and interaction term\n",
    "        if 'age' in X_transformed.columns:\n",
    "            X_transformed['age_log'] = np.log(X_transformed['age'].replace(0, np.nan).fillna(1))\n",
    "        if 'hours-per-week' in X_transformed.columns:\n",
    "            X_transformed['high_hours'] = (X_transformed['hours-per-week'] > 40).astype(int)\n",
    "        if 'capital-gain' in X_transformed.columns and 'capital-loss' in X_transformed.columns:\n",
    "            X_transformed['capital_interaction'] = X_transformed['capital-gain'] * X_transformed['capital-loss']\n",
    "        \n",
    "        X_transformed.fillna(0, inplace=True)\n",
    "        return X_transformed\n",
    "\n",
    "# Define preprocessors\n",
    "pca_ss_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=5))\n",
    "        ]), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "ss_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "pca_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', PCA(n_components=5), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the pipelines\n",
    "pipeline_1 = Pipeline(steps=[\n",
    "    ('preprocessor', pca_ss_preprocessor),\n",
    "    ('feature_engineering', EnhancedFeatureEngineer()),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=5, min_samples_split=10, criterion='gini'))\n",
    "])\n",
    "\n",
    "pipeline_2 = Pipeline(steps=[\n",
    "    ('preprocessor', ss_preprocessor),\n",
    "    ('feature_engineering', EnhancedFeatureEngineer()),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=6, min_samples_split=15, criterion='entropy'))\n",
    "])\n",
    "\n",
    "pipeline_3 = Pipeline(steps=[\n",
    "    ('preprocessor', pca_preprocessor),\n",
    "    ('feature_engineering', EnhancedFeatureEngineer()),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=4, min_samples_split=8, min_samples_leaf=5))\n",
    "])\n",
    "\n",
    "pipeline_4 = Pipeline(steps=[\n",
    "    ('preprocessor', pca_ss_preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5, max_features='sqrt', class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Train and evaluate the pipelines, storing results in separate variables\n",
    "X_train, X_test, y_train, y_test = X_train_strat, X_test_strat, y_train_strat, y_test_strat\n",
    "pipeline_1.fit(X_train, y_train)\n",
    "y_pred_1 = pipeline_1.predict(X_test)\n",
    "A1 = accuracy_score(y_test, y_pred_1)\n",
    "\n",
    "pipeline_2.fit(X_train, y_train)\n",
    "y_pred_2 = pipeline_2.predict(X_test)\n",
    "A2 = accuracy_score(y_test, y_pred_2)\n",
    "\n",
    "pipeline_3.fit(X_train, y_train)\n",
    "y_pred_3 = pipeline_3.predict(X_test)\n",
    "A3 = accuracy_score(y_test, y_pred_3)\n",
    "\n",
    "# For Pipeline 4, use non-stratified split\n",
    "X_train, X_test, y_train, y_test = X_train_no_strat, X_test_no_strat, y_train_no_strat, y_test_no_strat\n",
    "pipeline_4.fit(X_train, y_train)\n",
    "y_pred_4 = pipeline_4.predict(X_test)\n",
    "A4 = accuracy_score(y_test, y_pred_4)\n",
    "\n",
    "# Print results\n",
    "print(f\"Pipeline 1 Accuracy: {A1}\")\n",
    "print(f\"Pipeline 2 Accuracy: {A2}\")\n",
    "print(f\"Pipeline 3 Accuracy: {A3}\")\n",
    "print(f\"Pipeline 4 Accuracy: {A4}\")\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12861c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = accuracy_score(y_test, y_pred_1)\n",
    "A2 = accuracy_score(y_test, y_pred_2)\n",
    "A3 = accuracy_score(y_test, y_pred_3)\n",
    "A4 = accuracy_score(y_test, y_pred_4)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_1)\n",
    "f2 = f1_score(y_test, y_pred_2)\n",
    "f3 = f1_score(y_test, y_pred_3)\n",
    "f4 = f1_score(y_test, y_pred_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "pipeline_baseline = DummyClassifier(strategy='most_frequent')\n",
    "pipeline_baseline.fit(X_train, y_train)\n",
    "y_pred_baseline = pipeline_baseline.predict(X_test)\n",
    "mispred_baseline = (y_test != y_pred_baseline).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "def compute_propensity_scores(X, treatment_column):\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Define the treatment variable\n",
    "    treatment = X_imputed[treatment_column].apply(lambda x: 1 if x == 4 or x == 'White' else 0)\n",
    "\n",
    "    # Drop the treatment column to create the covariate matrix\n",
    "    X_covariates = X_imputed.drop(columns=[treatment_column])\n",
    "\n",
    "    # One-hot encode the covariate matrix\n",
    "    X_encoded = pd.get_dummies(X_covariates, drop_first=True)\n",
    "\n",
    "    # Estimate propensity scores using logistic regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_encoded, treatment)\n",
    "    propensity_scores = lr.predict_proba(X_encoded)[:, 1]\n",
    "\n",
    "    return propensity_scores, treatment\n",
    "\n",
    "def perform_matching(propensity_scores, treatment):\n",
    "    # Define treated and control indices based on the treatment variable\n",
    "    treated_indices = np.where((treatment == 1))[0]  # 'White' or 4 are considered treated\n",
    "    control_indices = np.where((treatment == 0))[0]  # All others are control\n",
    "\n",
    "    # Fit the nearest neighbors model on control propensity scores\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(propensity_scores[control_indices].reshape(-1, 1))\n",
    "\n",
    "    # Find the nearest neighbors for treated units\n",
    "    distances, indices = nn.kneighbors(propensity_scores[treated_indices].reshape(-1, 1))\n",
    "\n",
    "    # Map control indices to matched treated indices\n",
    "    matched_control_indices = control_indices[indices.flatten()]\n",
    "\n",
    "    # Combine treated and matched control indices\n",
    "    matched_indices = np.concatenate([treated_indices, matched_control_indices])\n",
    "\n",
    "    # Ensure indices are unique and within bounds\n",
    "    matched_indices = np.unique(matched_indices)\n",
    "    matched_indices = matched_indices[matched_indices < len(treatment)]\n",
    "\n",
    "    return matched_indices\n",
    "\n",
    "\n",
    "def flip_race(attribute):\n",
    "    return ['Non-White' if x == '4' else 'White' for x in attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=X['age'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "def calculate_biases(mispred_p1, mispred_p2, mispred_p3,mispred_p4, mispred_baseline):\n",
    "    \"\"\"\n",
    "    Calculate individual biases and total bias.\n",
    "\n",
    "    Parameters:\n",
    "    mispred_p1 (int): Mispredictions for P1 (LE+SS+Classifier)\n",
    "    mispred_p2 (int): Mispredictions for P2 (SS+Classifier)\n",
    "    mispred_p3 (int): Mispredictions for P3 (LE+Classifier)\n",
    "    mispred_baseline (int): Mispredictions for the baseline model\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the individual biases and total bias\n",
    "    \"\"\"\n",
    "    # Calculating individual biases\n",
    "    BD = mispred_baseline\n",
    "    BLE = mispred_p1 - mispred_p2\n",
    "    LE_SS_Interaction = mispred_p1 - (mispred_p2 + mispred_p3 - mispred_baseline)\n",
    "\n",
    "    BSS = mispred_p1 - mispred_p3\n",
    "    SS_PC_Interaction = mispred_p1 - (mispred_p2 + mispred_p4 - mispred_baseline)\n",
    "    \n",
    "    BPC = mispred_p1 - mispred_p4\n",
    "    PC_Classifier_Interaction = mispred_p1 - (mispred_p3 + mispred_p4 - mispred_baseline)\n",
    "\n",
    "    # Calculate BC to ensure TB equals mispred_p1\n",
    "    BC = mispred_p1 - (BD + BLE + BSS + BPC + LE_SS_Interaction + SS_PC_Interaction + PC_Classifier_Interaction)\n",
    "\n",
    "    # Calculate the total bias\n",
    "    TB = BD + BLE + BSS + BPC + BC + LE_SS_Interaction + SS_PC_Interaction + PC_Classifier_Interaction\n",
    "\n",
    "    # Returning the results as a dictionary\n",
    "    return {\n",
    "        'BD': BD,\n",
    "\n",
    "        'BSt-1': BLE,\n",
    "        'St1-St2_Interaction': LE_SS_Interaction,\n",
    "        'BSt-2': BSS,\n",
    "        'St2-St3_Interaction': SS_PC_Interaction,\n",
    "        'BSt-3': BPC,\n",
    "              \n",
    "        'St3_Classifier_Interaction': PC_Classifier_Interaction,\n",
    "        'BC': BC,\n",
    "        'TB': TB\n",
    "    }\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_biases(biases, mispred_p1):\n",
    "    \"\"\"\n",
    "    Plot biases and observed mispredictions.\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        'Data (BD)', \n",
    "        \n",
    "        'Preprocessing Stage 1', \n",
    "        'Stage 1-2 Interaction',\n",
    "        'Preprocessing Stage 2', \n",
    "         \n",
    "       \n",
    "        'Stage 2-3 Interaction',\n",
    "        'Preprocessing Stage 3',\n",
    "        'Stage 3-Classifier Interaction',\n",
    "        'Classifier (BC)', \n",
    "        'Total Bias (TB)', \n",
    "        'Observed Bias (mispred_p1)'\n",
    "    ]\n",
    "    \n",
    "    values = [\n",
    "        biases['BD'], \n",
    "         \n",
    "        biases['BSt-1'],\n",
    "        biases['St1-St2_Interaction'],\n",
    "        biases['BSt-2'], \n",
    "        \n",
    "        \n",
    "        biases['BSt-3'],\n",
    "        biases['St2-St3_Interaction'], \n",
    "        biases['St3_Classifier_Interaction'], \n",
    "        biases['BC'],\n",
    "        biases['TB'], \n",
    "        mispred_p1\n",
    "    ]\n",
    "\n",
    "    colors = [\n",
    "        'blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray'\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(labels, values, color=colors)\n",
    "    plt.xlabel('Bias / Mispredictions')\n",
    "    plt.title('Bias Analysis and Observed Mispredictions')\n",
    "\n",
    "    # Adding annotations\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        label_x_pos = width if width < 0 else width\n",
    "        plt.text(\n",
    "            label_x_pos, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{width:.2f}', ha='center', va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.axvline(0, color='black', linewidth=0.8)\n",
    "    plt.xlim(min(values) - 10, max(values) + 10)  # Adding some padding to the x-axis\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with hypothetical values\n",
    "\n",
    "\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test, y_pred_1, y_pred_2, y_pred_3 are defined\n",
    "mispred_p1 = sum(y_test != y_pred_1)\n",
    "mispred_p2 = sum(y_test != y_pred_2)\n",
    "mispred_p3 = sum(y_test != y_pred_3)\n",
    "mispred_p4 = sum(y_test != y_pred_4)\n",
    "\n",
    "\n",
    "\n",
    "# biases = calculate_biases_multiple(mispred_p1, mispred_p2, mispred_p3, mispred_p4, mispred_baseline)\n",
    "\n",
    "# Calculate biases\n",
    "biases = calculate_biases(mispred_p1, mispred_p2, mispred_p3,mispred_p4, mispred_baseline)\n",
    "plot_biases(biases, mispred_p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aaa361",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_LE = np.zeros(len(y_test))\n",
    "volatility_SS = np.zeros(len(y_test))\n",
    "volatility_3 = np.zeros(len(y_test))\n",
    "volatility_classifier = np.zeros(len(y_test))\n",
    "\n",
    "# Volatility score calculation for PS1 (Preprocessing Stage 1)\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1n[i] == y_test.iloc[i] and y_pred_2n[i] == y_test.iloc[i]:\n",
    "        volatility_LE[i] = 0\n",
    "    elif y_pred_1n[i] == y_test.iloc[i] and y_pred_2n[i] != y_test.iloc[i]:\n",
    "        volatility_LE[i] = 1\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_2n[i] == y_test.iloc[i]:\n",
    "        volatility_LE[i] = 2\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_2n[i] != y_test.iloc[i] and y_pred_1n[i] == y_pred_2n[i]:\n",
    "        volatility_LE[i] = 3\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_2n[i] != y_test.iloc[i] and y_pred_1n[i] != y_pred_2n[i]:\n",
    "        volatility_LE[i] = 4\n",
    "\n",
    "# Volatility score calculation for PS2 (Preprocessing Stage 2)\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1n[i] == y_test.iloc[i] and y_pred_3n[i] == y_test.iloc[i]:\n",
    "        volatility_SS[i] = 0\n",
    "    elif y_pred_1n[i] == y_test.iloc[i] and y_pred_3n[i] != y_test.iloc[i]:\n",
    "        volatility_SS[i] = 1\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_3n[i] == y_test.iloc[i]:\n",
    "        volatility_SS[i] = 2\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_3n[i] != y_test.iloc[i] and y_pred_1n[i] == y_pred_3n[i]:\n",
    "        volatility_SS[i] = 3\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_3n[i] != y_test.iloc[i] and y_pred_1n[i] != y_pred_3n[i]:\n",
    "        volatility_SS[i] = 4\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1n[i] == y_test.iloc[i] and y_pred_4n[i] == y_test.iloc[i]:\n",
    "        volatility_3[i] = 0\n",
    "    elif y_pred_1n[i] == y_test.iloc[i] and y_pred_4n[i] != y_test.iloc[i]:\n",
    "        volatility_3[i] = 1\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_4n[i] == y_test.iloc[i]:\n",
    "        volatility_3[i] = 2\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_4n[i] != y_test.iloc[i] and y_pred_1n[i] == y_pred_4n[i]:\n",
    "        volatility_3[i] = 3\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_4n[i] != y_test.iloc[i] and y_pred_1n[i] != y_pred_4n[i]:\n",
    "        volatility_3[i] = 4        \n",
    "        \n",
    "# Volatility score calculation for classifier\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1n[i] == y_test.iloc[i] and y_pred_baseline[i] == y_test.iloc[i]:\n",
    "        volatility_classifier[i] = 0\n",
    "    elif y_pred_1n[i] == y_test.iloc[i] and y_pred_baseline[i] != y_test.iloc[i]:\n",
    "        volatility_classifier[i] = 1\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_baseline[i] == y_test.iloc[i]:\n",
    "        volatility_classifier[i] = 2\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_baseline[i] != y_test.iloc[i] and y_pred_1n[i] == y_pred_baseline[i]:\n",
    "        volatility_classifier[i] = 3\n",
    "    elif y_pred_1n[i] != y_test.iloc[i] and y_pred_baseline[i] != y_test.iloc[i] and y_pred_1n[i] != y_pred_baseline[i]:\n",
    "        volatility_classifier[i] = 4\n",
    "\n",
    "# Summarize volatility scores\n",
    "volatility_summary = {\n",
    "    'Condition': ['Vol_PS1=0', 'Vol_PS1=1', 'Vol_PS1=2', 'Vol_PS1=3', 'Vol_PS1=4',\n",
    "                  'Vol_PS2=0', 'Vol_PS2=1', 'Vol_PS2=2', 'Vol_PS2=3', 'Vol_PS2=4',\n",
    "                  'Vol_PS3=0', 'Vol_PS3=1', 'Vol_PS3=2', 'Vol_PS3=3', 'Vol_PS3=4',\n",
    "                  'Vol_Cl=0', 'Vol_Cl=1', 'Vol_Cl=2', 'Vol_Cl=3', 'Vol_Cl=4'],\n",
    "    'Count': [np.sum(volatility_LE == 0), np.sum(volatility_LE == 1), np.sum(volatility_LE == 2), np.sum(volatility_LE == 3), np.sum(volatility_LE == 4),\n",
    "              np.sum(volatility_SS == 0), np.sum(volatility_SS == 1), np.sum(volatility_SS == 2), np.sum(volatility_SS == 3), np.sum(volatility_SS == 4),\n",
    "              np.sum(volatility_3 == 0), np.sum(volatility_3 == 1), np.sum(volatility_3 == 2), np.sum(volatility_3 == 3), np.sum(volatility_3 == 4),\n",
    "\n",
    "              np.sum(volatility_classifier == 0), np.sum(volatility_classifier == 1), np.sum(volatility_classifier == 2), np.sum(volatility_classifier == 3), np.sum(volatility_classifier == 4)]\n",
    "}\n",
    "\n",
    "df_volatility = pd.DataFrame(volatility_summary)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x='Condition', y='Count', data=df_volatility, palette='viridis')\n",
    "\n",
    "plt.title('Volatility Score Analysis - After Mitigation')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=75)\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%.0f', label_type='edge')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "77+574\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_LE = np.zeros(len(y_test))\n",
    "volatility_SS = np.zeros(len(y_test))\n",
    "volatility_3 = np.zeros(len(y_test))\n",
    "volatility_classifier = np.zeros(len(y_test))\n",
    "\n",
    "# Volatility score calculation for PS1 (Preprocessing Stage 1)\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1[i] == y_test.iloc[i] and y_pred_2[i] == y_test.iloc[i]:\n",
    "        volatility_LE[i] = 0\n",
    "    elif y_pred_1[i] == y_test.iloc[i] and y_pred_2[i] != y_test.iloc[i]:\n",
    "        volatility_LE[i] = 1\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_2[i] == y_test.iloc[i]:\n",
    "        volatility_LE[i] = 2\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_2[i] != y_test.iloc[i] and y_pred_1[i] == y_pred_2[i]:\n",
    "        volatility_LE[i] = 3\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_2[i] != y_test.iloc[i] and y_pred_1[i] != y_pred_2[i]:\n",
    "        volatility_LE[i] = 4\n",
    "\n",
    "# Volatility score calculation for PS2 (Preprocessing Stage 2)\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1[i] == y_test.iloc[i] and y_pred_3[i] == y_test.iloc[i]:\n",
    "        volatility_SS[i] = 0\n",
    "    elif y_pred_1[i] == y_test.iloc[i] and y_pred_3[i] != y_test.iloc[i]:\n",
    "        volatility_SS[i] = 1\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_3[i] == y_test.iloc[i]:\n",
    "        volatility_SS[i] = 2\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_3[i] != y_test.iloc[i] and y_pred_1[i] == y_pred_3[i]:\n",
    "        volatility_SS[i] = 3\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_3[i] != y_test.iloc[i] and y_pred_1[i] != y_pred_3[i]:\n",
    "        volatility_SS[i] = 4\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1[i] == y_test.iloc[i] and y_pred_4[i] == y_test.iloc[i]:\n",
    "        volatility_3[i] = 0\n",
    "    elif y_pred_1[i] == y_test.iloc[i] and y_pred_4[i] != y_test.iloc[i]:\n",
    "        volatility_3[i] = 1\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_4[i] == y_test.iloc[i]:\n",
    "        volatility_3[i] = 2\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_4[i] != y_test.iloc[i] and y_pred_1[i] == y_pred_4[i]:\n",
    "        volatility_3[i] = 3\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_4[i] != y_test.iloc[i] and y_pred_1[i] != y_pred_4[i]:\n",
    "        volatility_3[i] = 4        \n",
    "        \n",
    "# Volatility score calculation for classifier\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1[i] == y_test.iloc[i] and y_pred_baseline[i] == y_test.iloc[i]:\n",
    "        volatility_classifier[i] = 0\n",
    "    elif y_pred_1[i] == y_test.iloc[i] and y_pred_baseline[i] != y_test.iloc[i]:\n",
    "        volatility_classifier[i] = 1\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_baseline[i] == y_test.iloc[i]:\n",
    "        volatility_classifier[i] = 2\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_baseline[i] != y_test.iloc[i] and y_pred_1[i] == y_pred_baseline[i]:\n",
    "        volatility_classifier[i] = 3\n",
    "    elif y_pred_1[i] != y_test.iloc[i] and y_pred_baseline[i] != y_test.iloc[i] and y_pred_1[i] != y_pred_baseline[i]:\n",
    "        volatility_classifier[i] = 4\n",
    "\n",
    "# Summarize volatility scores\n",
    "volatility_summary = {\n",
    "    'Condition': ['Vol_PS1=0', 'Vol_PS1=1', 'Vol_PS1=2', 'Vol_PS1=3', 'Vol_PS1=4',\n",
    "                  'Vol_PS2=0', 'Vol_PS2=1', 'Vol_PS2=2', 'Vol_PS2=3', 'Vol_PS2=4',\n",
    "                  'Vol_PS3=0', 'Vol_PS3=1', 'Vol_PS3=2', 'Vol_PS3=3', 'Vol_PS3=4',\n",
    "                  'Vol_Cl=0', 'Vol_Cl=1', 'Vol_Cl=2', 'Vol_Cl=3', 'Vol_Cl=4'],\n",
    "    'Count': [np.sum(volatility_LE == 0), np.sum(volatility_LE == 1), np.sum(volatility_LE == 2), np.sum(volatility_LE == 3), np.sum(volatility_LE == 4),\n",
    "              np.sum(volatility_SS == 0), np.sum(volatility_SS == 1), np.sum(volatility_SS == 2), np.sum(volatility_SS == 3), np.sum(volatility_SS == 4),\n",
    "              np.sum(volatility_3 == 0), np.sum(volatility_3 == 1), np.sum(volatility_3 == 2), np.sum(volatility_3 == 3), np.sum(volatility_3 == 4),\n",
    "\n",
    "              np.sum(volatility_classifier == 0), np.sum(volatility_classifier == 1), np.sum(volatility_classifier == 2), np.sum(volatility_classifier == 3), np.sum(volatility_classifier == 4)]\n",
    "}\n",
    "\n",
    "df_volatility = pd.DataFrame(volatility_summary)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x='Condition', y='Count', data=df_volatility, palette='viridis')\n",
    "\n",
    "plt.title('Volatility Score Analysis')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=75)\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%.0f', label_type='edge')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9167f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple, Any, Optional\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Optional: SMOTE if you plan to oversample (safe fallback if not installed)\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE  # noqa\n",
    "except Exception:\n",
    "    SMOTE = None  # oversampling step will be skipped if None\n",
    "\n",
    "\n",
    "# ---------- Mitigation framework (no hard-coding of stages) ----------\n",
    "MitigationFn = Callable[\n",
    "    [Any, np.ndarray, np.ndarray, Dict[str, Any]],\n",
    "    Tuple[Any, np.ndarray, np.ndarray, Dict[str, Any]]\n",
    "]\n",
    "# Signature: fn(pipeline, X_train, y_train, context) -> (pipeline, X_train, y_train, fit_params_delta)\n",
    "\n",
    "def run_mitigations_and_predict(\n",
    "    pipeline: Any,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test:  np.ndarray,\n",
    "    y_test:  Optional[np.ndarray] = None,\n",
    "    mitigations: Optional[List[Dict[str, Any]]] = None,\n",
    "    base_fit_params: Optional[Dict[str, Any]] = None,\n",
    "    context: Optional[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fits pipeline, applies mitigation callables, then refits and re-predicts.\n",
    "    Returns pre/post metrics & predictions. No random post-hoc tweaks.\n",
    "    \"\"\"\n",
    "    base_fit_params = dict(base_fit_params or {})\n",
    "    context = dict(context or {})\n",
    "    mitigations = mitigations or []\n",
    "\n",
    "    # 1) Baseline fit & predict (pre-mitigation)\n",
    "    pipeline.fit(X_train, y_train, **base_fit_params)\n",
    "    y_pred_pre = pipeline.predict(X_test)\n",
    "\n",
    "    metrics_pre = {}\n",
    "    if y_test is not None:\n",
    "        metrics_pre[\"accuracy\"] = float(accuracy_score(y_test, y_pred_pre))\n",
    "\n",
    "    # 2) Apply mitigations (may update pipeline/data/fit_params)\n",
    "    applied = []\n",
    "    X_train_m, y_train_m = X_train, y_train\n",
    "    fit_params = dict(base_fit_params)\n",
    "\n",
    "    for step in mitigations:\n",
    "        name = step.get(\"name\", \"<unnamed>\")\n",
    "        fn: MitigationFn = step[\"fn\"]\n",
    "        params = step.get(\"params\", {})\n",
    "        ctx = dict(context); ctx.update(params)\n",
    "\n",
    "        pipeline, X_train_m, y_train_m, delta = fn(pipeline, X_train_m, y_train_m, ctx)\n",
    "        if delta:\n",
    "            fit_params.update(delta)\n",
    "        applied.append({\"name\": name, \"fit_params_delta_keys\": list((delta or {}).keys())})\n",
    "\n",
    "    # 3) Refit post-mitigation & predict\n",
    "    pipeline.fit(X_train_m, y_train_m, **fit_params)\n",
    "    y_pred_post = pipeline.predict(X_test)\n",
    "\n",
    "    metrics_post = {}\n",
    "    if y_test is not None:\n",
    "        metrics_post[\"accuracy\"] = float(accuracy_score(y_test, y_pred_post))\n",
    "\n",
    "    return {\n",
    "        \"y_pred_pre\": y_pred_pre,\n",
    "        \"y_pred_post\": y_pred_post,\n",
    "        \"metrics_pre\": metrics_pre,\n",
    "        \"metrics_post\": metrics_post,\n",
    "        \"applied_mitigations\": applied,\n",
    "        \"pipeline_final\": pipeline\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Example mitigation primitives (compose as you like) ----------\n",
    "def mitigation_reweighing(pipeline, X_train, y_train, context):\n",
    "    \"\"\"\n",
    "    Adds per-sample weights into fit params (no data mutation).\n",
    "    Provide either:\n",
    "      - context[\"sample_weights\"] (np.ndarray aligned with y_train), or\n",
    "      - context[\"weight_fn\"](X_train, y_train, context) -> np.ndarray\n",
    "    You can change which estimator receives weights via context[\"weight_param\"]\n",
    "      e.g., \"classifier__sample_weight\" or \"clf__sample_weight\".\n",
    "    \"\"\"\n",
    "    if \"sample_weights\" in context:\n",
    "        w = np.asarray(context[\"sample_weights\"])\n",
    "    elif callable(context.get(\"weight_fn\", None)):\n",
    "        w = np.asarray(context[\"weight_fn\"](X_train, y_train, context))\n",
    "    else:\n",
    "        raise ValueError(\"mitigation_reweighing needs 'sample_weights' or 'weight_fn' in context.\")\n",
    "\n",
    "    target_param = context.get(\"weight_param\", \"classifier__sample_weight\")\n",
    "    return pipeline, X_train, y_train, {target_param: w}\n",
    "\n",
    "\n",
    "def mitigation_oversample(pipeline, X_train, y_train, context):\n",
    "    \"\"\"\n",
    "    Uses a provided oversampler (e.g., SMOTE) from context[\"oversampler\"].\n",
    "    \"\"\"\n",
    "    oversampler = context.get(\"oversampler\", None)\n",
    "    if oversampler is None:\n",
    "        # If SMOTE not available or not provided, leave data unchanged\n",
    "        return pipeline, X_train, y_train, {}\n",
    "    X_res, y_res = oversampler.fit_resample(X_train, y_train)\n",
    "    return pipeline, X_res, y_res, {}\n",
    "\n",
    "\n",
    "def mitigation_encoder_tweak(pipeline, X_train, y_train, context):\n",
    "    \"\"\"\n",
    "    Placeholder to swap/modify an encoder inside your pipeline if needed.\n",
    "    Example:\n",
    "      pipeline.set_params(preprocess__encoder=YourEncoder(**context.get(\"encoder_params\", {})))\n",
    "    \"\"\"\n",
    "    return pipeline, X_train, y_train, {}\n",
    "\n",
    "\n",
    "# ---------- A simple, sensible reweighting function (group × label inverse prevalence) ----------\n",
    "def inverse_prevalence_weights(X, y, ctx):\n",
    "    \"\"\"\n",
    "    Compute weights ~ 1 / P(y, s) using ctx[\"s_train\"] (sensitive attribute, aligned with y).\n",
    "    \"\"\"\n",
    "    s = np.asarray(ctx.get(\"s_train\", None))\n",
    "    if s is None:\n",
    "        raise ValueError(\"inverse_prevalence_weights requires 's_train' in context.\")\n",
    "    y = np.asarray(y)\n",
    "    pairs, counts = np.unique(np.column_stack([y, s]), axis=0, return_counts=True)\n",
    "    freq = {tuple(k): v for k, v in zip(map(tuple, pairs), counts)}\n",
    "    total = len(y)\n",
    "    w = np.empty_like(y, dtype=float)\n",
    "    for i, (yy, ss) in enumerate(zip(y, s)):\n",
    "        p = freq[(yy, ss)] / total\n",
    "        w[i] = 1.0 / max(p, 1e-12)\n",
    "    # Normalize to mean 1.0 (optional)\n",
    "    w /= w.mean()\n",
    "    return w\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PRODUCE THE NEW PREDICTIONS\n",
    "# =========================\n",
    "# Assumptions: you already have:\n",
    "#   pipeline_1, pipeline_2, pipeline_3  (sklearn Pipelines/estimators)\n",
    "#   X_train, y_train, X_test, y_test     (arrays/Series)\n",
    "#   s_train                               (sensitive attribute aligned with y_train)\n",
    "#\n",
    "# If you don't want oversampling, just remove the mitigation_oversample step below.\n",
    "\n",
    "mitigations_cfg = [\n",
    "    {\"name\": \"Reweighing\", \"fn\": mitigation_reweighing,\n",
    "     \"params\": {\n",
    "         \"weight_param\": \"classifier__sample_weight\",   # adjust to your pipeline\n",
    "         \"weight_fn\": inverse_prevalence_weights\n",
    "     }},\n",
    "    {\"name\": \"Oversample\", \"fn\": mitigation_oversample,\n",
    "     \"params\": {\"oversampler\": SMOTE(random_state=42) if SMOTE is not None else None}},\n",
    "    {\"name\": \"Encoder tweak\", \"fn\": mitigation_encoder_tweak, \"params\": {}},\n",
    "]\n",
    "\n",
    "common_context = {\"s_train\": s_train, \"seed\": 42}\n",
    "\n",
    "res_1 = run_mitigations_and_predict(\n",
    "    pipeline=pipeline_1,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,  y_test=y_test,\n",
    "    mitigations=mitigations_cfg,\n",
    "    base_fit_params={},\n",
    "    context=common_context\n",
    ")\n",
    "y_pred_1n = res_1[\"y_pred_post\"]\n",
    "\n",
    "res_2 = run_mitigations_and_predict(\n",
    "    pipeline=pipeline_2,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,  y_test=y_test,\n",
    "    mitigations=mitigations_cfg,\n",
    "    base_fit_params={},\n",
    "    context=common_context\n",
    ")\n",
    "y_pred_2n = res_2[\"y_pred_post\"]\n",
    "\n",
    "res_3 = run_mitigations_and_predict(\n",
    "    pipeline=pipeline_3,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,  y_test=y_test,\n",
    "    mitigations=mitigations_cfg,\n",
    "    base_fit_params={},\n",
    "    context=common_context\n",
    ")\n",
    "y_pred_3n = res_3[\"y_pred_post\"]\n",
    "\n",
    "res_4 = run_mitigations_and_predict(\n",
    "    pipeline=pipeline_4,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,  y_test=y_test,\n",
    "    mitigations=mitigations_cfg,\n",
    "    base_fit_params={},\n",
    "    context=common_context\n",
    ")\n",
    "y_pred_4n = res_4[\"y_pred_post\"]\n",
    "\n",
    "# (Optional) Inspect honest pre/post metrics:\n",
    "# print(res_1[\"metrics_pre\"], res_1[\"metrics_post\"])\n",
    "# print(res_2[\"metrics_pre\"], res_2[\"metrics_post\"])\n",
    "# print(res_3[\"metrics_pre\"], res_3[\"metrics_post\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1fa661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional, Sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers for stage outputs\n",
    "# =========================\n",
    "def _truncate_pipeline(pipeline, stage_name: str):\n",
    "    \"\"\"\n",
    "    Return a shallow-cloned pipeline that runs up to and including `stage_name`.\n",
    "    Special names:\n",
    "      - \"__raw__\": no transform, just return raw X\n",
    "      - \"__end__\": full pipeline\n",
    "    \"\"\"\n",
    "    if stage_name == \"__raw__\":\n",
    "        return None\n",
    "    if stage_name == \"__end__\":\n",
    "        return clone(pipeline)\n",
    "    steps = []\n",
    "    for name, est in pipeline.steps:\n",
    "        steps.append((name, est))\n",
    "        if name == stage_name:\n",
    "            break\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    return Pipeline(steps=steps)\n",
    "\n",
    "def _fit_transform_at_stage(pipeline, X: pd.DataFrame, stage_name: str) -> pd.DataFrame:\n",
    "    if stage_name == \"__raw__\":\n",
    "        return _ensure_df(X)\n",
    "    tp = _truncate_pipeline(pipeline, stage_name)\n",
    "    if tp is None:\n",
    "        return _ensure_df(X)\n",
    "    try:\n",
    "        tp.set_output(transform=\"pandas\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    Xt = tp.fit_transform(X)\n",
    "    return _ensure_df(Xt)\n",
    "\n",
    "def _ensure_df(X) -> pd.DataFrame:\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        X = X.toarray()\n",
    "    if isinstance(X, np.ndarray):\n",
    "        return pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "    return pd.DataFrame(X)\n",
    "\n",
    "def _expand_attributes_in_transformed(X_after: pd.DataFrame, attributes: Sequence[str]) -> List[str]:\n",
    "    \"\"\"Map requested raw attributes to transformed columns (handles one-hot by substring match).\"\"\"\n",
    "    cols = []\n",
    "    for a in attributes:\n",
    "        a_low = str(a).lower()\n",
    "        for c in X_after.columns:\n",
    "            if a_low in str(c).lower():\n",
    "                cols.append(c)\n",
    "    if not cols:\n",
    "        cols = [c for c in X_after.columns if c in attributes]\n",
    "    # dedupe\n",
    "    seen, out = set(), []\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            out.append(c); seen.add(c)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 1) Correlations before vs after any stage\n",
    "# ===========================================\n",
    "def compare_attribute_correlations(\n",
    "    pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    attributes: Sequence[str],\n",
    "    stage_name: str,\n",
    "    method: str = \"pearson\",\n",
    "    figsize: Tuple[int, int] = (10, 4)\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute & display correlation matrices for selected `attributes`\n",
    "    BEFORE vs AFTER a given preprocessing `stage_name`.\n",
    "    stage_name: \"__raw__\" | any pipeline step name | \"__end__\"\n",
    "    \"\"\"\n",
    "    X_before = _ensure_df(X)\n",
    "    X_after  = _fit_transform_at_stage(pipeline, X, stage_name)\n",
    "\n",
    "    before_cols = [c for c in X_before.columns if c in attributes]\n",
    "    after_cols  = _expand_attributes_in_transformed(X_after, attributes)\n",
    "\n",
    "    corr_before = _ensure_df(X_before[before_cols]).corr(method=method) if before_cols else pd.DataFrame()\n",
    "    corr_after  = _ensure_df(X_after[after_cols]).corr(method=method)   if after_cols  else pd.DataFrame()\n",
    "\n",
    "    if not corr_before.empty:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(corr_before, interpolation=\"nearest\")\n",
    "        plt.xticks(range(len(corr_before.columns)), corr_before.columns, rotation=90)\n",
    "        plt.yticks(range(len(corr_before.index)), corr_before.index)\n",
    "        plt.title(f\"Correlation BEFORE stage '{stage_name}' ({method})\")\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if not corr_after.empty:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(corr_after, interpolation=\"nearest\")\n",
    "        plt.xticks(range(len(corr_after.columns)), corr_after.columns, rotation=90)\n",
    "        plt.yticks(range(len(corr_after.index)), corr_after.index)\n",
    "        plt.title(f\"Correlation AFTER stage '{stage_name}' ({method})\")\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\"before\": corr_before, \"after\": corr_after}\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 2) Feature distribution before vs after any stage\n",
    "# ==================================================\n",
    "def compare_feature_distribution(\n",
    "    pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    feature: str,\n",
    "    stage_name: str,\n",
    "    bins: int = 20,\n",
    "    top_k_categories: int = 25,\n",
    "    figsize: Tuple[int, int] = (8, 4)\n",
    ") -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Display the distribution of a single `feature` BEFORE vs AFTER `stage_name`.\n",
    "    Numeric => histogram; Categorical => top-k bar counts.\n",
    "    If transformed into multiple one-hot columns, aggregate those columns into a categorical count vector.\n",
    "    \"\"\"\n",
    "    X_before = _ensure_df(X).copy()\n",
    "    X_after  = _fit_transform_at_stage(pipeline, X, stage_name)\n",
    "\n",
    "    # BEFORE\n",
    "    if feature in X_before.columns:\n",
    "        series_before = X_before[feature]\n",
    "    else:\n",
    "        matches = [c for c in X_before.columns if c.lower() == feature.lower()]\n",
    "        series_before = X_before[matches[0]] if matches else pd.Series(dtype=float, name=feature)\n",
    "\n",
    "    # AFTER (map to columns)\n",
    "    after_cols = _expand_attributes_in_transformed(X_after, [feature])\n",
    "\n",
    "    if len(after_cols) == 0:\n",
    "        series_after = pd.Series(dtype=float, name=feature)\n",
    "    elif len(after_cols) == 1 and after_cols[0] in X_after.columns:\n",
    "        series_after = X_after[after_cols[0]]\n",
    "    else:\n",
    "        sub = X_after[after_cols]\n",
    "        cat_names = [c.replace(f\"{feature}_\", \"\") for c in sub.columns]\n",
    "        counts = sub.apply(pd.Series.sum, axis=0)\n",
    "        counts.index = cat_names\n",
    "        series_after = counts\n",
    "\n",
    "    # Plot BEFORE\n",
    "    if pd.api.types.is_numeric_dtype(series_before):\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.hist(series_before.dropna(), bins=bins)\n",
    "        plt.title(f\"Distribution BEFORE stage '{stage_name}' — {feature}\")\n",
    "        plt.xlabel(feature); plt.ylabel(\"count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "        dist_before = series_before\n",
    "    else:\n",
    "        vc = series_before.astype(\"object\").fillna(\"<NA>\").value_counts().sort_values(ascending=False)\n",
    "        vc = vc.head(top_k_categories)\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.bar(vc.index.astype(str), vc.values)\n",
    "        plt.title(f\"Distribution BEFORE stage '{stage_name}' — {feature}\")\n",
    "        plt.xticks(rotation=90); plt.ylabel(\"count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "        dist_before = vc\n",
    "\n",
    "    # Plot AFTER\n",
    "    if isinstance(series_after, pd.Series) and pd.api.types.is_numeric_dtype(series_after) and series_after.shape[0] == X_after.shape[0]:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.hist(series_after.dropna(), bins=bins)\n",
    "        plt.title(f\"Distribution AFTER stage '{stage_name}' — {feature}\")\n",
    "        plt.xlabel(feature); plt.ylabel(\"count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "        dist_after = series_after\n",
    "    else:\n",
    "        if isinstance(series_after, pd.Series) and series_after.index.size > 0:\n",
    "            counts = series_after.sort_values(ascending=False).head(top_k_categories)\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.bar(counts.index.astype(str), counts.values)\n",
    "            plt.title(f\"Distribution AFTER stage '{stage_name}' — {feature} (aggregated)\")\n",
    "            plt.xticks(rotation=90); plt.ylabel(\"count\")\n",
    "            plt.tight_layout(); plt.show()\n",
    "            dist_after = counts\n",
    "        else:\n",
    "            dist_after = pd.Series(dtype=float, name=feature)\n",
    "\n",
    "    return {\"before\": dist_before, \"after\": dist_after}\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3) Threshold guard that calls YOUR mitigation function\n",
    "#    - Uses your Fairness_Metrics_Computation(...)\n",
    "#    - Calls your run_mitigations_and_predict(...) if violated\n",
    "# ==========================================================\n",
    "def _check_thresholds(metrics: Dict[str, float], thresholds: Dict[str, Tuple]) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if ANY threshold is violated.\n",
    "    thresholds[k] = (op, val) or (\"between\", lo, hi)\n",
    "    \"\"\"\n",
    "    def violated_one(metric, rule):\n",
    "        op = rule[0]\n",
    "        vals = rule[1:]\n",
    "        m = metrics.get(metric, None)\n",
    "        if m is None:\n",
    "            return False\n",
    "        if op == \">=\":   return not (m >= vals[0])\n",
    "        if op == \"<=\":   return not (m <= vals[0])\n",
    "        if op == \">\":    return not (m >  vals[0])\n",
    "        if op == \"<\":    return not (m <  vals[0])\n",
    "        if op == \"between\":\n",
    "            lo, hi = vals\n",
    "            return not (lo <= m <= hi)\n",
    "        raise ValueError(f\"Unknown threshold op: {op}\")\n",
    "    return any(violated_one(k, v) for k, v in thresholds.items())\n",
    "\n",
    "def guard_and_mitigate_if_needed(\n",
    "    pipeline,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    sensitive_attr_test: np.ndarray,\n",
    "    thresholds: Dict[str, Tuple],\n",
    "    # ex:\n",
    "    # thresholds = {\n",
    "    #   \"accuracy\": (\">=\", 0.85),\n",
    "    #   \"disparate_impact_ratio\": (\">=\", 0.8),\n",
    "    #   \"demographic_parity_diff\": (\"between\", -0.1, 0.1)\n",
    "    # }\n",
    "    mitigations_cfg: List[Dict[str, Any]],\n",
    "    context: Dict[str, Any],\n",
    "    base_fit_params: Optional[Dict[str, Any]] = None,\n",
    "    fairness_compute_type: str = \"global\",\n",
    "    y2_scores: Optional[np.ndarray] = None,\n",
    "    positive_label=1\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Fit the baseline pipeline and compute metrics\n",
    "       - Accuracy via sklearn\n",
    "       - Fairness via your Fairness_Metrics_Computation(y1, y2, y, attribute, compute_type)\n",
    "         (y1 = y_pred, y2 = y2_scores or None, y = y_test, attribute = sensitive_attr_test)\n",
    "    2) If ANY threshold is violated, call your run_mitigations_and_predict(...)\n",
    "    3) Return both pre/post metrics and final predictions\n",
    "    \"\"\"\n",
    "    base_fit_params = dict(base_fit_params or {})\n",
    "    model = clone(pipeline)\n",
    "    model.fit(X_train, y_train, **base_fit_params)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # --- metrics (accuracy + fairness) ---\n",
    "    metrics = {\"accuracy\": float(accuracy_score(y_test, y_pred))}\n",
    "    fairness_dict = Fairness_Metrics_Computation(\n",
    "        y1=y_pred, y2=y2_scores, y=y_test, attribute=sensitive_attr_test, compute_type=fairness_compute_type\n",
    "    )\n",
    "    # Merge fairness metrics into metrics dict\n",
    "    metrics.update({k: float(v) for k, v in fairness_dict.items()})\n",
    "\n",
    "    violated = _check_thresholds(metrics, thresholds)\n",
    "\n",
    "    result = {\n",
    "        \"violated\": violated,\n",
    "        \"metrics_pre\": metrics,\n",
    "        \"y_pred_pre\": y_pred,\n",
    "        \"pipeline_pre\": model\n",
    "    }\n",
    "\n",
    "    if not violated:\n",
    "        result.update({\n",
    "            \"used_post_mitigation\": False,\n",
    "            \"y_pred_final\": y_pred,\n",
    "            \"metrics_final\": metrics,\n",
    "            \"pipeline_final\": model\n",
    "        })\n",
    "        return result\n",
    "\n",
    "    # --- run your mitigation routine (already defined in your codebase) ---\n",
    "    mit_res = run_mitigations_and_predict(\n",
    "        pipeline=model,\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test,  y_test=y_test,\n",
    "        mitigations=mitigations_cfg,\n",
    "        base_fit_params=base_fit_params,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # recompute fairness on post-mitigation predictions for apples-to-apples\n",
    "    y_pred_post = mit_res[\"y_pred_post\"]\n",
    "    metrics_post = {\"accuracy\": float(accuracy_score(y_test, y_pred_post))}\n",
    "    fairness_post = Fairness_Metrics_Computation(\n",
    "        y1=y_pred_post, y2=y2_scores, y=y_test, attribute=sensitive_attr_test, compute_type=fairness_compute_type\n",
    "    )\n",
    "    metrics_post.update({k: float(v) for k, v in fairness_post.items()})\n",
    "\n",
    "    result.update({\n",
    "        \"used_post_mitigation\": True,\n",
    "        \"mitigation_detail\": mit_res.get(\"applied_mitigations\", []),\n",
    "        \"y_pred_post\": y_pred_post,\n",
    "        \"metrics_post\": metrics_post,\n",
    "        \"y_pred_final\": y_pred_post,\n",
    "        \"metrics_final\": metrics_post,\n",
    "        \"pipeline_final\": mit_res[\"pipeline_final\"],\n",
    "    })\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d393bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_attribute_correlations(\n",
    "    pipeline=pipeline_1,\n",
    "    X=X_train,\n",
    "    attributes=[\"sex\"],\n",
    "    stage_name=\"preprocess\"    # or \"__raw__\", \"__end__\", or any step name in your pipeline\n",
    ")\n",
    "\n",
    "\n",
    "compare_feature_distribution(\n",
    "    pipeline=pipeline_2,\n",
    "    X=X_train,\n",
    "    feature=\"education\",\n",
    "    stage_name=\"preprocess\"\n",
    ")\n",
    "\n",
    "thresholds = {\n",
    "    \"accuracy\": (\">=\", 0.85),\n",
    "    \"disparate_impact_ratio\": (\">=\", 0.8),\n",
    "    \"demographic_parity_diff\": (\"between\", -0.1, 0.1),\n",
    "}\n",
    "\n",
    "res1 = guard_and_mitigate_if_needed(\n",
    "    pipeline=pipeline_1,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,   y_test=y_test,\n",
    "    sensitive_attr_test=s_test,\n",
    "    thresholds=thresholds,\n",
    "    mitigations_cfg=mitigations_cfg,  # the same list you pass to run_mitigations_and_predict\n",
    "    context=common_context,           # e.g., {\"s_train\": s_train, ...}\n",
    "    fairness_compute_type=\"global\",   # or \"local\"\n",
    "    y2_scores=None                    # provide scores if your fairness function needs them\n",
    ")\n",
    "y_pred_1n = res1[\"y_pred_final\"]\n",
    "\n",
    "res2 = guard_and_mitigate_if_needed(\n",
    "    pipeline=pipeline_2, X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test, sensitive_attr_test=s_test,\n",
    "    thresholds=thresholds, mitigations_cfg=mitigations_cfg, context=common_context\n",
    ")\n",
    "y_pred_2n = res2[\"y_pred_final\"]\n",
    "\n",
    "res3 = guard_and_mitigate_if_needed(\n",
    "    pipeline=pipeline_3, X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test, sensitive_attr_test=s_test,\n",
    "    thresholds=thresholds, mitigations_cfg=mitigations_cfg, context=common_context\n",
    ")\n",
    "y_pred_3n = res3[\"y_pred_final\"]\n",
    "\n",
    "\n",
    "res4 = guard_and_mitigate_if_needed(\n",
    "    pipeline=pipeline_4, X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test, sensitive_attr_test=s_test,\n",
    "    thresholds=thresholds, mitigations_cfg=mitigations_cfg, context=common_context\n",
    ")\n",
    "y_pred_4n = res4[\"y_pred_final\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = accuracy_score(y_test, y_pred_1)\n",
    "A2 = accuracy_score(y_test, y_pred_2)\n",
    "A3 = accuracy_score(y_test, y_pred_3)\n",
    "A4 = accuracy_score(y_test, y_pred_4)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_1)\n",
    "f2 = f1_score(y_test, y_pred_2)\n",
    "f3 = f1_score(y_test, y_pred_3)\n",
    "f4 = f1_score(y_test, y_pred_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77c77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1n = accuracy_score(y_test, y_pred_1n)\n",
    "A2n = accuracy_score(y_test, y_pred_2n)\n",
    "A3n = accuracy_score(y_test, y_pred_3n)\n",
    "A4n = accuracy_score(y_test, y_pred_4n)\n",
    "\n",
    "f1n = f1_score(y_test, y_pred_1n)\n",
    "f2n = f1_score(y_test, y_pred_2n)\n",
    "f3n = f1_score(y_test, y_pred_3n)\n",
    "f4n = f1_score(y_test, y_pred_4n)\n",
    "\n",
    "A1, A1n, A2, A2n, A3, A3n , A4,A4n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=X['age'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "def calculate_biases(mispred_p1, mispred_p2, mispred_p3,mispred_p4, mispred_baseline):\n",
    "    \"\"\"\n",
    "    Calculate individual biases and total bias.\n",
    "\n",
    "    Parameters:\n",
    "    mispred_p1 (int): Mispredictions for P1 (LE+SS+Classifier)\n",
    "    mispred_p2 (int): Mispredictions for P2 (SS+Classifier)\n",
    "    mispred_p3 (int): Mispredictions for P3 (LE+Classifier)\n",
    "    mispred_baseline (int): Mispredictions for the baseline model\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the individual biases and total bias\n",
    "    \"\"\"\n",
    "    # Calculating individual biases\n",
    "    BD = mispred_baseline\n",
    "    BLE = mispred_p1 - mispred_p2\n",
    "    LE_SS_Interaction = mispred_p1 - (mispred_p2 + mispred_p3 - mispred_baseline)\n",
    "\n",
    "    BSS = mispred_p1 - mispred_p3\n",
    "    SS_PC_Interaction = mispred_p1 - (mispred_p2 + mispred_p4 - mispred_baseline)\n",
    "    \n",
    "    BPC = mispred_p1 - mispred_p4\n",
    "    PC_Classifier_Interaction = mispred_p1 - (mispred_p3 + mispred_p4 - mispred_baseline)\n",
    "\n",
    "    # Calculate BC to ensure TB equals mispred_p1\n",
    "    BC = mispred_p1 - (BD + BLE + BSS + BPC + LE_SS_Interaction + SS_PC_Interaction + PC_Classifier_Interaction)\n",
    "\n",
    "    # Calculate the total bias\n",
    "    TB = BD + BLE + BSS + BPC + BC + LE_SS_Interaction + SS_PC_Interaction + PC_Classifier_Interaction\n",
    "\n",
    "    # Returning the results as a dictionary\n",
    "    return {\n",
    "        'BD': BD,\n",
    "\n",
    "        'BSt-1': BLE,\n",
    "        'St1-St2_Interaction': LE_SS_Interaction,\n",
    "        'BSt-2': BSS,\n",
    "        'St2-St3_Interaction': SS_PC_Interaction,\n",
    "        'BSt-3': BPC,\n",
    "              \n",
    "        'St3_Classifier_Interaction': PC_Classifier_Interaction,\n",
    "        'BC': BC,\n",
    "        'TB': TB\n",
    "    }\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_biases(biases, mispred_p1):\n",
    "    \"\"\"\n",
    "    Plot biases and observed mispredictions.\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        'Data (BD)', \n",
    "        \n",
    "        'Preprocessing Stage 1', \n",
    "        'Stage 1-2 Interaction',\n",
    "        'Preprocessing Stage 2', \n",
    "         \n",
    "       \n",
    "        'Stage 2-3 Interaction',\n",
    "        'Preprocessing Stage 3',\n",
    "        'Stage 3-Classifier Interaction',\n",
    "        'Classifier (BC)', \n",
    "        'Total Bias (TB)', \n",
    "        'Observed Bias (mispred_p1)'\n",
    "    ]\n",
    "    \n",
    "    values = [\n",
    "        biases['BD'], \n",
    "         \n",
    "        biases['BSt-1'],\n",
    "        biases['St1-St2_Interaction'],\n",
    "        biases['BSt-2'], \n",
    "        \n",
    "        \n",
    "        biases['BSt-3'],\n",
    "        biases['St2-St3_Interaction'], \n",
    "        biases['St3_Classifier_Interaction'], \n",
    "        biases['BC'],\n",
    "        biases['TB'], \n",
    "        mispred_p1\n",
    "    ]\n",
    "\n",
    "    colors = [\n",
    "        'blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray'\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(labels, values, color=colors)\n",
    "    plt.xlabel('Bias / Mispredictions')\n",
    "    plt.title('Bias Analysis and Observed Mispredictions - After Mitigation')\n",
    "\n",
    "    # Adding annotations\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        label_x_pos = width if width < 0 else width\n",
    "        plt.text(\n",
    "            label_x_pos, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{width:.2f}', ha='center', va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.axvline(0, color='black', linewidth=0.8)\n",
    "    plt.xlim(min(values) - 10, max(values) + 10)  # Adding some padding to the x-axis\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with hypothetical values\n",
    "\n",
    "\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test, y_pred_1, y_pred_2, y_pred_3 are defined\n",
    "mispred_p1 = sum(y_test != y_pred_1n)\n",
    "mispred_p2 = sum(y_test != y_pred_2n)\n",
    "mispred_p3 = sum(y_test != y_pred_3n)\n",
    "mispred_p4 = sum(y_test != y_pred_4n)\n",
    "\n",
    "\n",
    "\n",
    "# biases = calculate_biases_multiple(mispred_p1, mispred_p2, mispred_p3, mispred_p4, mispred_baseline)\n",
    "\n",
    "# Calculate biases\n",
    "biases = calculate_biases(mispred_p1, mispred_p2, mispred_p3,mispred_p4, mispred_baseline)\n",
    "plot_biases(biases, mispred_p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bcf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize volatility arrays\n",
    "volatility_LE = np.zeros(len(y_test))\n",
    "volatility_SS = np.zeros(len(y_test))\n",
    "volatility_classifier = np.zeros(len(y_test))\n",
    "\n",
    "def calculate_volatility(y_test, y_pred1, y_pred2):\n",
    "    volatility = np.zeros(len(y_test))\n",
    "    for i in range(len(y_test)):\n",
    "        if y_pred1[i] == y_test.iloc[i] and y_pred2[i] == y_test.iloc[i]:\n",
    "            volatility[i] = 0\n",
    "        elif y_pred1[i] == y_test.iloc[i] and y_pred2[i] != y_test.iloc[i]:\n",
    "            volatility[i] = 1\n",
    "        elif y_pred1[i] != y_test.iloc[i] and y_pred2[i] == y_test.iloc[i]:\n",
    "            volatility[i] = 2\n",
    "        elif y_pred1[i] != y_test.iloc[i] and y_pred2[i] != y_test.iloc[i] and y_pred1[i] == y_pred2[i]:\n",
    "            volatility[i] = 3\n",
    "        elif y_pred1[i] != y_test.iloc[i] and y_pred2[i] != y_test.iloc[i] and y_pred1[i] != y_pred2[i]:\n",
    "            volatility[i] = 4\n",
    "    return volatility\n",
    "\n",
    "# Calculate volatility scores\n",
    "volatility_LE = calculate_volatility(y_test, y_pred_1n, y_pred_2n)\n",
    "volatility_SS = calculate_volatility(y_test, y_pred_1n, y_pred_3n)\n",
    "volatility_classifier = calculate_volatility(y_test, y_pred_1n, y_pred_baseline)\n",
    "volatility_y_pred_4n = np.zeros(len(y_test))\n",
    "\n",
    "# Calculate volatility for y_pred_4n\n",
    "volatility_y_pred_4n = calculate_volatility(y_test, y_pred_1n, y_pred_4n)\n",
    "\n",
    "# Update the summary with y_pred_4n values\n",
    "volatility_summary = {\n",
    "    'Condition': [\n",
    "        'Vol_PS1=0', 'Vol_PS1=1', 'Vol_PS1=2', 'Vol_PS1=3', 'Vol_PS1=4',\n",
    "        'Vol_PS2=0', 'Vol_PS2=1', 'Vol_PS2=2', 'Vol_PS2=3', 'Vol_PS2=4',\n",
    "        'Vol_Cl=0', 'Vol_Cl=1', 'Vol_Cl=2', 'Vol_Cl=3', 'Vol_Cl=4',\n",
    "        'Vol_P4=0', 'Vol_P4=1', 'Vol_P4=2', 'Vol_P4=3', 'Vol_P4=4'\n",
    "    ],\n",
    "    'Count': [\n",
    "        np.sum(volatility_LE == 0), np.sum(volatility_LE == 1), np.sum(volatility_LE == 2), np.sum(volatility_LE == 3), np.sum(volatility_LE == 4),\n",
    "        np.sum(volatility_SS == 0), np.sum(volatility_SS == 1), np.sum(volatility_SS == 2), np.sum(volatility_SS == 3), np.sum(volatility_SS == 4),\n",
    "        np.sum(volatility_classifier == 0), np.sum(volatility_classifier == 1), np.sum(volatility_classifier == 2), np.sum(volatility_classifier == 3), np.sum(volatility_classifier == 4),\n",
    "        np.sum(volatility_y_pred_4n == 0), np.sum(volatility_y_pred_4n == 1), np.sum(volatility_y_pred_4n == 2), np.sum(volatility_y_pred_4n == 3), np.sum(volatility_y_pred_4n == 4)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_volatility = pd.DataFrame(volatility_summary)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.barplot(x='Condition', y='Count', data=df_volatility, palette='viridis')\n",
    "\n",
    "plt.title('Volatility Score Analysis - After Mitigation ')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%.0f', label_type='edge')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "207+1144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c67a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6715a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Example computed metrics for illustration\n",
    "accuracy_1= A1\n",
    "accuracy_2 = np.abs(A1- A2)\n",
    "accuracy_3 = np.abs(A1- A3)\n",
    "accuracy_4 = np.abs(A1- A4)\n",
    "\n",
    "accuracy_1n= A1n\n",
    "accuracy_2n= np.abs(A1n- A2n)\n",
    "accuracy_3n = np.abs(A1n- A3n)\n",
    "accuracy_4n = np.abs(A1n- A4n)\n",
    "\n",
    "f1_1 = f1\n",
    "f1_2 = np.abs(f1-f2)\n",
    "f1_3 = np.abs(f1-f3)\n",
    "f1_4 = np.abs(f1-f4)\n",
    "\n",
    "f1_1n = f1n\n",
    "f1_2n = np.abs(f1n-f2n)\n",
    "f1_3n = np.abs(f1n-f3n)\n",
    "f1_4n = np.abs(f1n-f4n)\n",
    "\n",
    "\n",
    "# Function to plot the accuracy and F1 scores for pipelines and components before and after mitigation\n",
    "def plot_metrics():\n",
    "    labels = ['Pipeline', 'SS Component' , 'Custom FE Component' ,'Stratify Component']  # Update this if needed for component names\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    before_mitigation_accuracy = [accuracy_1, accuracy_2, accuracy_3, accuracy_4]\n",
    "    after_mitigation_accuracy = [accuracy_1n, accuracy_2n,accuracy_3n, accuracy_3n]\n",
    "    \n",
    "    bar_width = 0.35  # Width of the bars\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    plt.bar(x - bar_width / 2, before_mitigation_accuracy, width=bar_width, label='Before Mitigation', color='blue')\n",
    "    plt.bar(x + bar_width / 2, after_mitigation_accuracy, width=bar_width, label='After Mitigation', color='green')\n",
    "    \n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Before vs After Mitigation')\n",
    "    plt.xticks(rotation=70)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # F1 Score plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    before_mitigation_f1 = [f1_1, f1_2,f1_3, f1_4]\n",
    "    after_mitigation_f1 = [f1_1n, f1_2n,f1_3n , f1_4n]\n",
    "    \n",
    "    plt.bar(x - bar_width / 2, before_mitigation_f1, width=bar_width, label='Before Mitigation', color='blue')\n",
    "    plt.bar(x + bar_width / 2, after_mitigation_f1, width=bar_width, label='After Mitigation', color='green')\n",
    "    \n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score Before vs After Mitigation')\n",
    "    plt.xticks(rotation=70)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f59cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422830c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2078370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Initialize lists to store results\n",
    "spd_list = []\n",
    "eod_list = []\n",
    "aod_list = []\n",
    "erd_list = []\n",
    "\n",
    "spd_cas_list = []\n",
    "eod_cas_list = []\n",
    "aod_cas_list = []\n",
    "erd_cas_list = []\n",
    "\n",
    "spd_cf_list = []\n",
    "eod_cf_list = []\n",
    "aod_cf_list = []\n",
    "erd_cf_list = []\n",
    "\n",
    "def compute_propensity_scores(X, treatment_column):\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Convert to dense if X is sparse\n",
    "    if isinstance(X, csr_matrix):\n",
    "        X = X.toarray()\n",
    "    \n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    # Use the provided treatment column directly\n",
    "    treatment = np.where((treatment_column == 4) | (treatment_column == 'White'), 1, 0)\n",
    "\n",
    "    # Use the imputed covariate matrix without the treatment column\n",
    "    X_covariates = X_imputed\n",
    "\n",
    "    # One-hot encode the covariate matrix (if needed)\n",
    "    X_encoded = pd.get_dummies(pd.DataFrame(X_covariates), drop_first=True).values\n",
    "\n",
    "    # Estimate propensity scores using logistic regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_encoded, treatment)\n",
    "    propensity_scores = lr.predict_proba(X_encoded)[:, 1]\n",
    "\n",
    "    return propensity_scores, treatment\n",
    "\n",
    "def perform_matching(propensity_scores, treatment):\n",
    "    # Define treated and control indices based on the treatment variable\n",
    "    treated_indices = np.where((treatment == 1))[0]  # 'White' or 4 are considered treated\n",
    "    control_indices = np.where((treatment == 0))[0]  # All others are control\n",
    "\n",
    "    # Fit the nearest neighbors model on control propensity scores\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(propensity_scores[control_indices].reshape(-1, 1))\n",
    "\n",
    "    # Find the nearest neighbors for treated units\n",
    "    distances, indices = nn.kneighbors(propensity_scores[treated_indices].reshape(-1, 1))\n",
    "\n",
    "    # Map control indices to matched treated indices\n",
    "    matched_control_indices = control_indices[indices.flatten()]\n",
    "\n",
    "    # Combine treated and matched control indices\n",
    "    matched_indices = np.concatenate([treated_indices, matched_control_indices])\n",
    "\n",
    "    # Ensure indices are unique and within bounds\n",
    "    matched_indices = np.unique(matched_indices)\n",
    "    matched_indices = matched_indices[matched_indices < len(treatment)]\n",
    "\n",
    "    return matched_indices\n",
    "\n",
    "def flip_race(attribute):\n",
    "    return ['Non-White' if x == '4' else 'White' for x in attribute]\n",
    "\n",
    "# Loading and preprocessing dataset as in your provided code\n",
    "train_path = 'C:/Users/Saadia/FairPreprocessing/data/adult/adult.data'\n",
    "test_path = 'C:/Users/Saadia/FairPreprocessing/data/adult/adult.test'\n",
    "\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', \n",
    "                'capital-loss', 'hours-per-week', 'native-country', 'income-per-year']\n",
    "na_values = ['?']\n",
    "\n",
    "train = pd.read_csv(train_path, header=None, names=column_names, \n",
    "                    skipinitialspace=True, na_values=na_values)\n",
    "test = pd.read_csv(test_path, header=None, names=column_names,\n",
    "                   skipinitialspace=True, na_values=na_values)\n",
    "\n",
    "train['income-per-year'] = train['income-per-year'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\n",
    "test['income-per-year'] = test['income-per-year'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\n",
    "\n",
    "combined_data = pd.concat([train, test])\n",
    "combined_data = combined_data.dropna(subset=['income-per-year'])\n",
    "\n",
    "X_combined = combined_data.drop('income-per-year', axis=1)\n",
    "y_combined = combined_data['income-per-year']\n",
    "\n",
    "categorical_cols = X_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_combined.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "X_combined[categorical_cols] = X_combined[categorical_cols].astype(str)\n",
    "X_combined['age'] = pd.to_numeric(X_combined['age'], errors='coerce')\n",
    "\n",
    "# Custom transformers\n",
    "\n",
    "# Main loop with updated function usage\n",
    "for i in range(10):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42, stratify=y_combined)\n",
    "    X_train_no_strat, X_test_no_strat, y_train_no_strat, y_test_no_strat = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42)\n",
    "\n",
    "    pipeline_1.fit(X_train_strat, y_train_strat)\n",
    "    pipeline_2.fit(X_train_strat, y_train_strat)\n",
    "    pipeline_3.fit(X_train_strat, y_train_strat)\n",
    "\n",
    "    # For P4\n",
    "    pipeline_4.fit(X_train_no_strat, y_train_no_strat)\n",
    "\n",
    "    y_pred_1 = pipeline_1.predict(X_test_strat)\n",
    "    y_pred_2 = pipeline_2.predict(X_test_strat)\n",
    "    y_pred_3 = pipeline_3.predict(X_test_strat)\n",
    "\n",
    "# For P4\n",
    "    y_pred_4 = pipeline_4.predict(X_test_no_strat)\n",
    "    # Placeholder for Fairness_Metrics_Computation function\n",
    "    # Assuming it returns the necessary SPD, EOD, AOD, ERD metrics\n",
    "    SPD_mv, EOD_mv, AOD_mv, ERD_mv = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['race'])\n",
    "    SPD_fe, EOD_fe, AOD_fe, ERD_fe = Fairness_Metrics_Computation(y_pred_1, y_pred_3, y_test, X_test['race']) \n",
    "    SPD_3, EOD_3, AOD_3, ERD_3 = Fairness_Metrics_Computation(y_pred_1, y_pred_4, y_test, X_test['race'])\n",
    "    \n",
    "   \n",
    "    spd_list.append((SPD_mv, SPD_fe,SPD_3))\n",
    "    eod_list.append((EOD_mv, EOD_fe,EOD_3))\n",
    "    aod_list.append((AOD_mv, AOD_fe,AOD_3))\n",
    "    erd_list.append((ERD_mv, ERD_fe, ERD_3))\n",
    "\n",
    "    X_test_flipped = X_test.copy()\n",
    "    X_test_flipped['race'] = flip_race(X_test['race'])\n",
    "\n",
    "    X_test_flipped_transformed1 = pipeline_1.named_steps['preprocessor'].transform(X_test_flipped)\n",
    "    X_test_flipped_transformed2 = pipeline_2.named_steps['preprocessor'].transform(X_test_flipped)\n",
    "    X_test_flipped_transformed3 = pipeline_3.named_steps['preprocessor'].transform(X_test_flipped)\n",
    "    X_test_flipped_transformed4 = pipeline_4.named_steps['preprocessor'].transform(X_test_flipped)\n",
    "\n",
    "    y_pred_cf_1 = pipeline_1.named_steps['classifier'].predict(X_test_flipped_transformed1)\n",
    "    y_pred_cf_2 = pipeline_2.named_steps['classifier'].predict(X_test_flipped_transformed2)\n",
    "    y_pred_cf_3 = pipeline_3.named_steps['classifier'].predict(X_test_flipped_transformed3)\n",
    "    y_pred_cf_4 = pipeline_3.named_steps['classifier'].predict(X_test_flipped_transformed3)\n",
    "\n",
    "    \n",
    "    SPD_cf_mv, EOD_cf_mv, AOD_cf_mv, ERD_cf_mv = Fairness_Metrics_Computation(y_pred_cf_1, y_pred_cf_2, y_test, X_test['race'])\n",
    "    SPD_cf_fe, EOD_cf_fe, AOD_cf_fe, ERD_cf_fe = Fairness_Metrics_Computation(y_pred_cf_1, y_pred_cf_3, y_test, X_test['race'])\n",
    "    SPD_cf_3, EOD_cf_3, AOD_cf_3, ERD_cf_3 = Fairness_Metrics_Computation(y_pred_cf_1, y_pred_cf_4, y_test, X_test['race'])\n",
    "\n",
    "    spd_cf_list.append((SPD_cf_mv, SPD_cf_fe, SPD_cf_3))\n",
    "    eod_cf_list.append((EOD_cf_mv, EOD_cf_fe,EOD_cf_3))\n",
    "    aod_cf_list.append((AOD_cf_mv, AOD_cf_fe,AOD_cf_3))\n",
    "    erd_cf_list.append((ERD_cf_mv, ERD_cf_fe,ERD_cf_3))\n",
    "    \n",
    "    # Use the race column from X_test for computing propensity scores\n",
    "    propensity_scores1, treatment1 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices1 = perform_matching(propensity_scores1, treatment1)\n",
    "\n",
    "    propensity_scores2, treatment2 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices2 = perform_matching(propensity_scores2, treatment2)\n",
    "\n",
    "    propensity_scores3, treatment3 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices3 = perform_matching(propensity_scores3, treatment3)\n",
    "\n",
    "    propensity_scores4, treatment4 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices4 = perform_matching(propensity_scores4, treatment4)\n",
    "    \n",
    "    common_matched_indices = set(matched_indices1).intersection(matched_indices2).intersection(matched_indices3).intersection(matched_indices4)\n",
    "    common_matched_indices = list(common_matched_indices)\n",
    "    common_matched_indices.sort()\n",
    "\n",
    "    y1_matched = y_pred_1[common_matched_indices]\n",
    "    y2_matched = y_pred_2[common_matched_indices]\n",
    "    y3_matched = y_pred_3[common_matched_indices]\n",
    "    y4_matched = y_pred_4[common_matched_indices]\n",
    "    \n",
    "    y_test_matched = y_test.reset_index(drop=True)[common_matched_indices]\n",
    "    sex_matched = X_test['race'].reset_index(drop=True)[common_matched_indices]\n",
    "\n",
    "    SPD_cas_mv, EOD_cas_mv, AOD_cas_mv, ERD_cas_mv = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched)\n",
    "    SPD_cas_fe, EOD_cas_fe, AOD_cas_fe, ERD_cas_fe = Fairness_Metrics_Computation(y1_matched, y3_matched, y_test_matched, sex_matched)\n",
    "    SPD_cas_3, EOD_cas_3, AOD_cas_3, ERD_cas_3 = Fairness_Metrics_Computation(y1_matched, y4_matched, y_test_matched, sex_matched)\n",
    "\n",
    "    spd_cas_list.append((SPD_cas_mv, SPD_cas_fe,SPD_cas_3))\n",
    "    eod_cas_list.append((EOD_cas_mv, EOD_cas_fe,EOD_cas_3))\n",
    "    aod_cas_list.append((AOD_cas_mv, AOD_cas_fe,AOD_cas_3))\n",
    "    erd_cas_list.append((ERD_cas_mv, ERD_cas_fe,ERD_cas_3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_array = np.array(spd_list)\n",
    "eod_array = np.array(eod_list)\n",
    "aod_array = np.array(aod_list)\n",
    "erd_array = np.array(erd_list)\n",
    "\n",
    "spd_cas_array = np.array(spd_cas_list)\n",
    "eod_cas_array = np.array(eod_cas_list)\n",
    "aod_cas_array = np.array(aod_cas_list)\n",
    "erd_cas_array = np.array(erd_cas_list)\n",
    "\n",
    "spd_cf_array = np.array(spd_cf_list)\n",
    "eod_cf_array = np.array(eod_cf_list)\n",
    "aod_cf_array = np.array(aod_cf_list)\n",
    "erd_cf_array = np.array(erd_cf_list)\n",
    "\n",
    "\n",
    "spd_mean_mv = spd_array[:, 0].mean()\n",
    "spd_mean_fe = spd_array[:, 1].mean()\n",
    "spd_mean_enc = spd_array[:, 2].mean()\n",
    "\n",
    "eod_mean_mv = eod_array[:, 0].mean()\n",
    "eod_mean_fe = eod_array[:, 1].mean()\n",
    "eod_mean_enc = eod_array[:, 2].mean()\n",
    "\n",
    "aod_mean_mv = aod_array[:, 0].mean()\n",
    "aod_mean_fe = aod_array[:, 1].mean()\n",
    "aod_mean_enc = aod_array[:, 2].mean()\n",
    "\n",
    "erd_mean_mv = erd_array[:, 0].mean()\n",
    "erd_mean_fe = erd_array[:, 1].mean()\n",
    "erd_mean_enc = erd_array[:, 2].mean()\n",
    "\n",
    "spd_cas_mean_mv = spd_cas_array[:, 0].mean()\n",
    "spd_cas_mean_fe = spd_cas_array[:, 1].mean()\n",
    "spd_cas_mean_enc = spd_cas_array[:, 2].mean()\n",
    "\n",
    "eod_cas_mean_mv = eod_cas_array[:, 0].mean()\n",
    "eod_cas_mean_fe = eod_cas_array[:, 1].mean()\n",
    "eod_cas_mean_enc = eod_cas_array[:, 2].mean()\n",
    "\n",
    "aod_cas_mean_mv = aod_cas_array[:, 0].mean()\n",
    "aod_cas_mean_fe = aod_cas_array[:, 1].mean()\n",
    "aod_cas_mean_enc = aod_cas_array[:, 2].mean()\n",
    "\n",
    "erd_cas_mean_mv = erd_cas_array[:, 0].mean()\n",
    "erd_cas_mean_fe = erd_cas_array[:, 1].mean()\n",
    "erd_cas_mean_enc = erd_cas_array[:, 2].mean()\n",
    "\n",
    "spd_cf_mean_mv = spd_cf_array[:, 0].mean()\n",
    "spd_cf_mean_fe = spd_cf_array[:, 1].mean()\n",
    "spd_cf_mean_enc = spd_cf_array[:, 2].mean()\n",
    "\n",
    "eod_cf_mean_mv = eod_cf_array[:, 0].mean()\n",
    "eod_cf_mean_fe = eod_cf_array[:, 1].mean()\n",
    "eod_cf_mean_enc = eod_cf_array[:, 2].mean()\n",
    "\n",
    "aod_cf_mean_mv = aod_cf_array[:, 0].mean()\n",
    "aod_cf_mean_fe = aod_cf_array[:, 1].mean()\n",
    "aod_cf_mean_enc = aod_cf_array[:, 2].mean()\n",
    "\n",
    "erd_cf_mean_mv = erd_cf_array[:, 0].mean()\n",
    "erd_cf_mean_fe = erd_cf_array[:, 1].mean()\n",
    "erd_cf_mean_enc = erd_cf_array[:, 2].mean()\n",
    "\n",
    "\n",
    "spd_mean = spd_array.mean(axis=0)\n",
    "eod_mean = eod_array.mean(axis=0)\n",
    "aod_mean = aod_array.mean(axis=0)\n",
    "erd_mean = erd_array.mean(axis=0)\n",
    "\n",
    "spd_cas_mean = spd_cas_array.mean(axis=0)\n",
    "eod_cas_mean = eod_cas_array.mean(axis=0)\n",
    "aod_cas_mean = aod_cas_array.mean(axis=0)\n",
    "erd_cas_mean = erd_cas_array.mean(axis=0)\n",
    "\n",
    "spd_cf_mean = spd_cf_array.mean(axis=0)\n",
    "eod_cf_mean = eod_cf_array.mean(axis=0)\n",
    "aod_cf_mean = aod_cf_array.mean(axis=0)\n",
    "erd_cf_mean = erd_cf_array.mean(axis=0)\n",
    "\n",
    "# Calculate the standard errors for each metric and stage\n",
    "spd_se = spd_array.std(axis=0) / np.sqrt(spd_array.shape[0])\n",
    "eod_se = eod_array.std(axis=0) / np.sqrt(eod_array.shape[0])\n",
    "aod_se = aod_array.std(axis=0) / np.sqrt(aod_array.shape[0])\n",
    "erd_se = erd_array.std(axis=0) / np.sqrt(erd_array.shape[0])\n",
    "\n",
    "spd_cas_se = spd_cas_array.std(axis=0) / np.sqrt(spd_cas_array.shape[0])\n",
    "eod_cas_se = eod_cas_array.std(axis=0) / np.sqrt(eod_cas_array.shape[0])\n",
    "aod_cas_se = aod_cas_array.std(axis=0) / np.sqrt(aod_cas_array.shape[0])\n",
    "erd_cas_se = erd_cas_array.std(axis=0) / np.sqrt(erd_cas_array.shape[0])\n",
    "\n",
    "spd_cf_se = spd_cf_array.std(axis=0) / np.sqrt(spd_cf_array.shape[0])\n",
    "eod_cf_se = eod_cf_array.std(axis=0) / np.sqrt(eod_cf_array.shape[0])\n",
    "aod_cf_se = aod_cf_array.std(axis=0) / np.sqrt(aod_cf_array.shape[0])\n",
    "erd_cf_se = erd_cf_array.std(axis=0) / np.sqrt(erd_cf_array.shape[0])\n",
    "\n",
    "# Plotting function\n",
    "def plot_with_error_bars(ax, means, std_errors, title, metric_labels, colors, labels):\n",
    "    bar_width = 0.25\n",
    "    indices = np.arange(len(metric_labels))  # Fix this to use metric_labels length\n",
    "    \n",
    "    for i, (mean, std_error, color, label) in enumerate(zip(means, std_errors, colors, labels)):\n",
    "        ax.bar(indices + i * bar_width, mean, yerr=std_error, capsize=5, width=bar_width, align='center', color=color, label=label)\n",
    "    \n",
    "    ax.set_xticks(indices + bar_width)\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.set_xlabel('Fairness Metrics')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Metric labels\n",
    "metric_labels = ['SPD', 'EOD', 'AOD', 'ERD']\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Colors and labels for the bars\n",
    "colors = ['blue', 'orange', 'green']\n",
    "labels = ['Statistical', 'Causal', 'Counterfactual']\n",
    "\n",
    "# Prepare means and standard errors for plotting\n",
    "means_mv = [spd_mean[0], eod_mean[0], aod_mean[0], erd_mean[0]]\n",
    "std_errors_mv = [spd_se[0], eod_se[0], aod_se[0], erd_se[0]]\n",
    "means_cas_mv = [spd_cas_mean[0], eod_cas_mean[0], aod_cas_mean[0], erd_cas_mean[0]]\n",
    "std_errors_cas_mv = [spd_cas_se[0], eod_cas_se[0], aod_cas_se[0], erd_cas_se[0]]\n",
    "means_cf_mv = [spd_cf_mean[0], eod_cf_mean[0], aod_cf_mean[0], erd_cf_mean[0]]\n",
    "std_errors_cf_mv = [spd_cf_se[0], eod_cf_se[0], aod_cf_se[0], erd_cf_se[0]]\n",
    "\n",
    "# Plot for MV\n",
    "plot_with_error_bars(axs[0], [means_mv, means_cas_mv, means_cf_mv], [std_errors_mv, std_errors_cas_mv, std_errors_cf_mv], 'SS', metric_labels, colors, labels)\n",
    "\n",
    "# Prepare means and standard errors for plotting FE\n",
    "means_fe = [spd_mean[1], eod_mean[1], aod_mean[1], erd_mean[1]]\n",
    "std_errors_fe = [spd_se[1], eod_se[1], aod_se[1], erd_se[1]]\n",
    "means_cas_fe = [spd_cas_mean[1], eod_cas_mean[1], aod_cas_mean[1], erd_cas_mean[1]]\n",
    "std_errors_cas_fe = [spd_cas_se[1], eod_cas_se[1], aod_cas_se[1], erd_cas_se[1]]\n",
    "means_cf_fe = [spd_cf_mean[1], eod_cf_mean[1], aod_cf_mean[1], erd_cf_mean[1]]\n",
    "std_errors_cf_fe = [spd_cf_se[1], eod_cf_se[1], aod_cf_se[1], erd_cf_se[1]]\n",
    "\n",
    "# Plot for FE\n",
    "plot_with_error_bars(axs[1], [means_fe, means_cas_fe, means_cf_fe], [std_errors_fe, std_errors_cas_fe, std_errors_cf_fe], 'Custom FE', metric_labels, colors, labels)\n",
    "\n",
    "# Prepare means and standard errors for plotting Encoding\n",
    "means_enc = [spd_mean[2], eod_mean[2], aod_mean[2], erd_mean[2]]\n",
    "std_errors_enc = [spd_se[2], eod_se[2], aod_se[2], erd_se[2]]\n",
    "means_cas_enc = [spd_cas_mean[2], eod_cas_mean[2], aod_cas_mean[2], erd_cas_mean[2]]\n",
    "std_errors_cas_enc = [spd_cas_se[2], eod_cas_se[2], aod_cas_se[2], erd_cas_se[2]]\n",
    "means_cf_enc = [spd_cf_mean[2], eod_cf_mean[2], aod_cf_mean[2], erd_cf_mean[2]]\n",
    "std_errors_cf_enc = [spd_cf_se[2], eod_cf_se[2], aod_cf_se[2], erd_cf_se[2]]\n",
    "\n",
    "# Plot for Encoding\n",
    "plot_with_error_bars(axs[2], [means_enc, means_cas_enc, means_cf_enc], [std_errors_enc, std_errors_cas_enc, std_errors_cf_enc], 'Stratify', metric_labels, colors, labels)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2dcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Fairness_Metrics_Computation(y1, y2, y, attribute, compute_type='global'):\n",
    "    \"\"\"\n",
    "    Compute local or global fairness metrics: SPD, EOD, AOD, ERD\n",
    "    y1: Predictions from original pipeline (Y(P))\n",
    "    y2: Predictions from modified pipeline (Y(P*))\n",
    "    y: Ground truth labels\n",
    "    attribute: Sensitive attribute (e.g., race or gender)\n",
    "    compute_type: 'local' for local metrics (only instances where y1 != y2),\n",
    "                  'global' for global metrics (all instances)\n",
    "    \"\"\"\n",
    "    # Convert inputs to pandas Series for easier handling\n",
    "    y1 = pd.Series(y1)\n",
    "    y2 = pd.Series(y2)\n",
    "    y = pd.Series(y)\n",
    "    attribute = pd.Series(attribute)\n",
    "\n",
    "    # Identify privileged and unprivileged groups\n",
    "    privileged = (attribute == 'White') | (attribute == 1)\n",
    "    unprivileged = ~privileged\n",
    "\n",
    "    # Set conditions for local or global calculation\n",
    "    condition = (y1 != y2) if compute_type == 'local' else (y1 == y1)  # y1 == y1 includes all rows\n",
    "\n",
    "    # Counts of privileged and unprivileged groups under the selected condition\n",
    "    count_privileged = np.sum(privileged & condition)\n",
    "    count_unprivileged = np.sum(unprivileged & condition)\n",
    "\n",
    "    # SPD Calculation\n",
    "    SFC_SPD_P = np.sum([(1 if y1_i == 1 and y2_i == 0 else -1 if y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y1_i, y2_i, attr, cond in zip(y1, y2, attribute, condition) if (attr == 'White' or attr == 1) and cond])\n",
    "    SFC_SPD_NP = np.sum([(1 if y1_i == 1 and y2_i == 0 else -1 if y1_i == 0 and y2_i == 1 else 0)\n",
    "                         for y1_i, y2_i, attr, cond in zip(y1, y2, attribute, condition) if (attr != 'White' and attr != 1) and cond])\n",
    "\n",
    "    SFR_SPD_p = SFC_SPD_P / count_privileged if count_privileged > 0 else 0\n",
    "    SFR_SPD_np = SFC_SPD_NP / count_unprivileged if count_unprivileged > 0 else 0\n",
    "    SPD = SFR_SPD_np - SFR_SPD_p\n",
    "\n",
    "    # EOD Calculation\n",
    "    P_Y1 = np.sum((y == 1) & privileged & condition)\n",
    "    NP_Y1 = np.sum((y == 1) & unprivileged & condition)\n",
    "\n",
    "    SFC_EOD_P = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr, cond in zip(y, y1, y2, attribute, condition) if (attr == 'White' or attr == 1) and cond])\n",
    "    SFC_EOD_NP = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                         for y_true_i, y1_i, y2_i, attr, cond in zip(y, y1, y2, attribute, condition) if (attr != 'White' and attr != 1) and cond])\n",
    "\n",
    "    SFR_EOD_p = SFC_EOD_P / P_Y1 if P_Y1 > 0 else 0\n",
    "    SFR_EOD_np = SFC_EOD_NP / NP_Y1 if NP_Y1 > 0 else 0\n",
    "    EOD = SFR_EOD_np - SFR_EOD_p\n",
    "\n",
    "    # AOD Calculation\n",
    "    P_Y0 = np.sum((y == 0) & privileged & condition)\n",
    "    NP_Y0 = np.sum((y == 0) & unprivileged & condition)\n",
    "\n",
    "    SFC_TP_P = SFC_EOD_P  # True Positives for privileged group\n",
    "    SFC_FP_P = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr, cond in zip(y, y1, y2, attribute, condition) if (attr == 'White' or attr == 1) and cond])\n",
    "    SFC_TP_NP = SFC_EOD_NP  # True Positives for unprivileged group\n",
    "    SFC_FP_NP = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr, cond in zip(y, y1, y2, attribute, condition) if (attr != 'White' and attr != 1) and cond])\n",
    "\n",
    "    SFR_AOD_p = (SFC_TP_P / P_Y1 if P_Y1 > 0 else 0) + (SFC_FP_P / P_Y0 if P_Y0 > 0 else 0)\n",
    "    SFR_AOD_np = (SFC_TP_NP / NP_Y1 if NP_Y1 > 0 else 0) + (SFC_FP_NP / NP_Y0 if NP_Y0 > 0 else 0)\n",
    "    AOD = (SFR_AOD_np - SFR_AOD_p) / 2\n",
    "\n",
    "    # ERD Calculation\n",
    "    SFC_FN_P = np.sum([(-1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr, cond in zip(y, y1, y2, attribute, condition) if (attr == 'White' or attr == 1) and cond])\n",
    "    SFC_FN_NP = np.sum([(-1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr, cond in zip(y, y1, y2, attribute, condition) if (attr != 'White' and attr != 1) and cond])\n",
    "    SFC_FP_P = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr, cond in zip(y, y1, y2, attribute, condition) if (attr == 'White' or attr == 1) and cond])\n",
    "    SFC_FP_NP = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr, cond in zip(y, y1, y2, attribute, condition) if (attr != 'White' and attr != 1) and cond])\n",
    "\n",
    "    SFR_ERR_p = (SFC_FP_P / P_Y0 if P_Y0 > 0 else 0) + (SFC_FN_P / P_Y1 if P_Y1 > 0 else 0)\n",
    "    SFR_ERR_np = (SFC_FP_NP / NP_Y0 if NP_Y0 > 0 else 0) + (SFC_FN_NP / NP_Y1 if NP_Y1 > 0 else 0)\n",
    "    ERD = SFR_ERR_np - SFR_ERR_p\n",
    "\n",
    "    return SPD, EOD, AOD, ERD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def flip_race(attribute):\n",
    "    return ['Non-White' if x == '4' else 'White' for x in attribute]\n",
    "\n",
    "class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, handle_unknown='ignore'):\n",
    "        self.columns = columns\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            for col in self.columns:\n",
    "                self.encoders[col] = LabelEncoder()\n",
    "                self.encoders[col].fit(X[col])\n",
    "        else:\n",
    "            for i, col in enumerate(self.columns):\n",
    "                self.encoders[col] = LabelEncoder()\n",
    "                self.encoders[col].fit(X[:, i])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        if isinstance(X_copy, pd.DataFrame):\n",
    "            for col in self.columns:\n",
    "                X_copy[col] = X_copy[col].map(lambda s: self.encoders[col].transform([s])[0] \n",
    "                                              if s in self.encoders[col].classes_ else -1)\n",
    "        else:\n",
    "            for i, col in enumerate(self.columns):\n",
    "                X_copy[:, i] = np.array([self.encoders[col].transform([s])[0] \n",
    "                                         if s in self.encoders[col].classes_ else -1 \n",
    "                                         for s in X_copy[:, i]])\n",
    "        return X_copy\n",
    "\n",
    "# The rest of your code remains the same\n",
    "\n",
    "# Paths to train and test datasets\n",
    "train_path = 'C:/Users/Saadia/FairPreprocessing/data/adult/adult.data'\n",
    "test_path = 'C:/Users/Saadia/FairPreprocessing/data/adult/adult.test'\n",
    "\n",
    "# Define column names and missing values\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', \n",
    "                'capital-loss', 'hours-per-week', 'native-country', 'income-per-year']\n",
    "na_values = ['?']\n",
    "\n",
    "# Read the train and test datasets\n",
    "train = pd.read_csv(train_path, header=None, names=column_names, \n",
    "                    skipinitialspace=True, na_values=na_values)\n",
    "test = pd.read_csv(test_path, header=None, names=column_names,\n",
    "                   skipinitialspace=True, na_values=na_values)\n",
    "\n",
    "# Map target variable\n",
    "train['income-per-year'] = train['income-per-year'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\n",
    "test['income-per-year'] = test['income-per-year'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\n",
    "\n",
    "# Combine train and test data for re-splitting\n",
    "combined_data = pd.concat([train, test])\n",
    "\n",
    "# Drop rows with missing target values\n",
    "combined_data = combined_data.dropna(subset=['income-per-year'])\n",
    "\n",
    "X_combined = combined_data.drop('income-per-year', axis=1)\n",
    "y_combined = combined_data['income-per-year']\n",
    "\n",
    "# Categorical and numerical columns\n",
    "categorical_cols = X_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_combined.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Convert all categorical columns to strings to avoid type issues\n",
    "X_combined[categorical_cols] = X_combined[categorical_cols].astype(str)\n",
    "\n",
    "# Initialize lists to store results\n",
    "spd_list = []\n",
    "eod_list = []\n",
    "aod_list = []\n",
    "erd_list = []\n",
    "\n",
    "spd_cas_list = []\n",
    "eod_cas_list = []\n",
    "aod_cas_list = []\n",
    "erd_cas_list = []\n",
    "\n",
    "gspd_list = []\n",
    "geod_list = []\n",
    "gaod_list = []\n",
    "gerd_list = []\n",
    "\n",
    "gspd_cas_list = []\n",
    "geod_cas_list = []\n",
    "gaod_cas_list = []\n",
    "gerd_cas_list = []\n",
    "\n",
    "# Perform 10 iterations\n",
    "for i in range(10):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    # Randomly split the data into training and test sets\n",
    "    X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42, stratify=y_combined)\n",
    "    X_train_no_strat, X_test_no_strat, y_train_no_strat, y_test_no_strat = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42)\n",
    "\n",
    "    pipeline_1.fit(X_train_strat, y_train_strat)\n",
    "    pipeline_2.fit(X_train_strat, y_train_strat)\n",
    "    pipeline_3.fit(X_train_strat, y_train_strat)\n",
    "\n",
    "    # For P4\n",
    "    pipeline_4.fit(X_train_no_strat, y_train_no_strat)\n",
    "\n",
    "    y_pred_1 = pipeline_1.predict(X_test_strat)\n",
    "    y_pred_2 = pipeline_2.predict(X_test_strat)\n",
    "    y_pred_3 = pipeline_3.predict(X_test_strat)\n",
    "\n",
    "# For P4\n",
    "    y_pred_4 = pipeline_4.predict(X_test_no_strat)\n",
    "    \n",
    "    SPD_1, EOD_1, AOD_1, ERD_1 = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['sex'],compute_type='local')\n",
    "    SPD_2, EOD_2, AOD_2, ERD_2 = Fairness_Metrics_Computation(y_pred_1, y_pred_3, y_test, X_test['sex'],compute_type='local')\n",
    "    SPD_3, EOD_3, AOD_3, ERD_3 = Fairness_Metrics_Computation(y_pred_1, y_pred_4, y_test, X_test['sex'],compute_type='local')\n",
    "    \n",
    "    # Store results\n",
    "    spd_list.append((SPD_1, SPD_2, SPD_3))\n",
    "    eod_list.append((EOD_1, EOD_2, EOD_3))\n",
    "    aod_list.append((AOD_1, AOD_2, AOD_3))\n",
    "    erd_list.append((ERD_1, ERD_2, ERD_3))\n",
    "    \n",
    "    gSPD_1, gEOD_1, gAOD_1, gERD_1 = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['sex'],compute_type='global')\n",
    "    gSPD_2, gEOD_2, gAOD_2, gERD_2 = Fairness_Metrics_Computation(y_pred_1, y_pred_3, y_test, X_test['sex'],compute_type='global')\n",
    "    gSPD_3, gEOD_3, gAOD_3, gERD_3 = Fairness_Metrics_Computation(y_pred_1, y_pred_4, y_test, X_test['sex'],compute_type='global')\n",
    "    \n",
    "    # Store results\n",
    "    gspd_list.append((gSPD_1, gSPD_2, gSPD_3))\n",
    "    geod_list.append((gEOD_1, gEOD_2, gEOD_3))\n",
    "    gaod_list.append((gAOD_1, gAOD_2, gAOD_3))\n",
    "    gerd_list.append((gERD_1, gERD_2, gERD_3))\n",
    "       \n",
    "    propensity_scores1, treatment1 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices1 = perform_matching(propensity_scores1, treatment1)\n",
    "\n",
    "    propensity_scores2, treatment2 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices2 = perform_matching(propensity_scores2, treatment2)\n",
    "\n",
    "    propensity_scores3, treatment3 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices3 = perform_matching(propensity_scores3, treatment3)\n",
    "\n",
    "    propensity_scores4, treatment4 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices4 = perform_matching(propensity_scores4, treatment4)\n",
    "    \n",
    "    common_matched_indices = set(matched_indices1).intersection(matched_indices2).intersection(matched_indices3).intersection(matched_indices4)\n",
    "    common_matched_indices = list(common_matched_indices)\n",
    "    common_matched_indices.sort()\n",
    "\n",
    "    y1_matched = y_pred_1[common_matched_indices]\n",
    "    y2_matched = y_pred_2[common_matched_indices]\n",
    "    y3_matched = y_pred_3[common_matched_indices]\n",
    "    y4_matched = y_pred_4[common_matched_indices]\n",
    "    \n",
    "    y_test_matched = y_test.reset_index(drop=True)[common_matched_indices]\n",
    "    sex_matched = X_test['race'].reset_index(drop=True)[common_matched_indices]\n",
    "    \n",
    "    # Compute causal fairness metrics\n",
    "    SPD_1_cas, EOD_1_cas, AOD_1_cas, ERD_1_cas = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched ,compute_type='local')\n",
    "    SPD_2_cas, EOD_2_cas, AOD_2_cas, ERD_2_cas = Fairness_Metrics_Computation(y1_matched, y3_matched, y_test_matched, sex_matched,compute_type='local')\n",
    "    SPD_3_cas, EOD_3_cas, AOD_3_cas, ERD_3_cas = Fairness_Metrics_Computation(y1_matched, y4_matched, y_test_matched, sex_matched,compute_type='local')\n",
    "    \n",
    "    # Store causal results\n",
    "    spd_cas_list.append((SPD_1_cas, SPD_2_cas, SPD_3_cas))\n",
    "    eod_cas_list.append((EOD_1_cas, EOD_2_cas, EOD_3_cas))\n",
    "    aod_cas_list.append((AOD_1_cas, AOD_2_cas, AOD_3_cas))\n",
    "    erd_cas_list.append((ERD_1_cas, ERD_2_cas, ERD_3_cas))\n",
    "\n",
    "    gSPD_1_cas, gEOD_1_cas, gAOD_1_cas, gERD_1_cas = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched ,compute_type='global')\n",
    "    gSPD_2_cas, gEOD_2_cas, gAOD_2_cas, gERD_2_cas = Fairness_Metrics_Computation(y1_matched, y3_matched, y_test_matched, sex_matched,compute_type='global')\n",
    "    gSPD_3_cas, gEOD_3_cas, gAOD_3_cas, gERD_3_cas = Fairness_Metrics_Computation(y1_matched, y4_matched, y_test_matched, sex_matched,compute_type='global')\n",
    "    \n",
    "    # Store causal results\n",
    "    gspd_cas_list.append((gSPD_1_cas, gSPD_2_cas, gSPD_3_cas))\n",
    "    geod_cas_list.append((gEOD_1_cas, gEOD_2_cas, gEOD_3_cas))\n",
    "    gaod_cas_list.append((gAOD_1_cas, gAOD_2_cas, gAOD_3_cas))\n",
    "    gerd_cas_list.append((gERD_1_cas, gERD_2_cas, gERD_3_cas))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfcba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means_and_errors(array):\n",
    "    mean = array.mean(axis=0) * 0.1\n",
    "    se = (array.std(axis=0) / np.sqrt(array.shape[0])) * 0.1\n",
    "    return np.round(mean, 3), np.round(se, 3)\n",
    "\n",
    "\n",
    "spd_array = np.array(spd_list)\n",
    "eod_array = np.array(eod_list)\n",
    "aod_array = np.array(aod_list)\n",
    "erd_array = np.array(erd_list)\n",
    "\n",
    "gspd_array = np.array(gspd_list)\n",
    "geod_array = np.array(geod_list)\n",
    "gaod_array = np.array(gaod_list)\n",
    "gerd_array = np.array(gerd_list)\n",
    "\n",
    "spd_cas_array = np.array(spd_cas_list)\n",
    "eod_cas_array = np.array(eod_cas_list)\n",
    "aod_cas_array = np.array(aod_cas_list)\n",
    "erd_cas_array = np.array(erd_cas_list)\n",
    "\n",
    "gspd_cas_array = np.array(gspd_cas_list)\n",
    "geod_cas_array = np.array(geod_cas_list)\n",
    "gaod_cas_array = np.array(gaod_cas_list)\n",
    "gerd_cas_array = np.array(gerd_cas_list)\n",
    "\n",
    "\n",
    "spd_mean, spd_se = compute_means_and_errors(spd_array)\n",
    "eod_mean, eod_se = compute_means_and_errors(eod_array)\n",
    "aod_mean, aod_se = compute_means_and_errors(aod_array)\n",
    "erd_mean, erd_se = compute_means_and_errors(erd_array)\n",
    "\n",
    "gspd_mean, gspd_se = compute_means_and_errors(gspd_array)\n",
    "geod_mean, geod_se = compute_means_and_errors(geod_array)\n",
    "gaod_mean, gaod_se = compute_means_and_errors(gaod_array)\n",
    "gerd_mean, gerd_se = compute_means_and_errors(gerd_array)\n",
    "\n",
    "spd_cas_mean, spd_cas_se = compute_means_and_errors(spd_cas_array)\n",
    "eod_cas_mean, eod_cas_se = compute_means_and_errors(eod_cas_array)\n",
    "aod_cas_mean, aod_cas_se = compute_means_and_errors(aod_cas_array)\n",
    "erd_cas_mean, erd_cas_se = compute_means_and_errors(erd_cas_array)\n",
    "\n",
    "gspd_cas_mean, gspd_cas_se = compute_means_and_errors(gspd_cas_array)\n",
    "geod_cas_mean, geod_cas_se = compute_means_and_errors(geod_cas_array)\n",
    "gaod_cas_mean, gaod_cas_se = compute_means_and_errors(gaod_cas_array)\n",
    "gerd_cas_mean, gerd_cas_se = compute_means_and_errors(gerd_cas_array)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your fairness metrics arrays are correctly formatted\n",
    "# Metric labels\n",
    "metric_labels = ['SPD', 'EOD', 'AOD', 'ERD']\n",
    "\n",
    "# Colors for the bars\n",
    "colors = ['blue', 'orange']\n",
    "labels = ['Local', 'Global']\n",
    "\n",
    "# Plot function for fairness metrics\n",
    "def plot_fairness_metrics(ax, local_means, global_means, local_se, global_se, title, metric_labels):\n",
    "    x = np.arange(len(metric_labels))  # label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, local_means, width, label='Local', yerr=local_se, capsize=5)\n",
    "    rects2 = ax.bar(x + width/2, global_means, width, label='Global', yerr=global_se, capsize=5)\n",
    "\n",
    "    # Add labels, title, and custom ticks\n",
    "    ax.set_xlabel('Fairness Metrics')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.legend()\n",
    "\n",
    "# Replace these with the actual means and errors for statistical and causal fairness\n",
    "# Statistical Local and Global for PCA\n",
    "local_mean_stage_1 = [spd_mean[0], eod_mean[0], aod_mean[0], erd_mean[0]]\n",
    "local_se_stage_1 = [spd_se[0], eod_se[0], aod_se[0], erd_se[0]]\n",
    "\n",
    "global_mean_stage_1 = [gspd_mean[0], geod_mean[0], gaod_mean[0], gerd_mean[0]]\n",
    "global_se_stage_1 = [gspd_se[0], geod_se[0], gaod_se[0], gerd_se[0]]\n",
    "\n",
    "# Causal Local and Global for PCA\n",
    "causal_means_stage_1 = [spd_cas_mean[0], eod_cas_mean[0], aod_cas_mean[0], erd_cas_mean[0]]\n",
    "causal_se_stage_1 = [spd_cas_se[0], eod_cas_se[0], aod_cas_se[0], erd_cas_se[0]]\n",
    "\n",
    "gcausal_means_stage_1 = [gspd_cas_mean[0], geod_cas_mean[0], gaod_cas_mean[0], gerd_cas_mean[0]]\n",
    "gcausal_se_stage_1 = [gspd_cas_se[0], geod_cas_se[0], gaod_cas_se[0], gerd_cas_se[0]]\n",
    "\n",
    "# Statistical Local and Global for SelectKBest\n",
    "local_mean_stage_2 = [spd_mean[1], eod_mean[1], aod_mean[1], erd_mean[1]]\n",
    "local_se_stage_2 = [spd_se[1], eod_se[1], aod_se[1], erd_se[1]]\n",
    "\n",
    "global_mean_stage_2 = [gspd_mean[1], geod_mean[1], gaod_mean[1], gerd_mean[1]]\n",
    "global_se_stage_2 = [gspd_se[1], geod_se[1], gaod_se[1], gerd_se[1]]\n",
    "\n",
    "# Causal Local and Global for SelectKBest\n",
    "causal_means_stage_2 = [spd_cas_mean[1], eod_cas_mean[1], aod_cas_mean[1], erd_cas_mean[1]]\n",
    "causal_se_stage_2 = [spd_cas_se[1], eod_cas_se[1], aod_cas_se[1], erd_cas_se[1]]\n",
    "\n",
    "gcausal_means_stage_2 = [gspd_cas_mean[1], geod_cas_mean[1], gaod_cas_mean[1], gerd_cas_mean[1]]\n",
    "gcausal_se_stage_2 = [gspd_cas_se[1], geod_cas_se[1], gaod_cas_se[1], gerd_cas_se[1]]\n",
    "\n",
    "\n",
    "local_mean_stage_3 = [spd_mean[2], eod_mean[2], aod_mean[2], erd_mean[2]]\n",
    "local_se_stage_3 = [spd_se[2], eod_se[2], aod_se[2], erd_se[2]]\n",
    "\n",
    "global_mean_stage_3 = [gspd_mean[2], geod_mean[2], gaod_mean[2], gerd_mean[2]]\n",
    "global_se_stage_3 = [gspd_se[2], geod_se[2], gaod_se[2], gerd_se[2]]\n",
    "\n",
    "# Causal Local and Global for SelectKBest\n",
    "causal_means_stage_3 = [spd_cas_mean[2], eod_cas_mean[2], aod_cas_mean[2], erd_cas_mean[2]]\n",
    "causal_se_stage_3 = [spd_cas_se[2], eod_cas_se[2], aod_cas_se[2], erd_cas_se[2]]\n",
    "\n",
    "gcausal_means_stage_3 = [gspd_cas_mean[2], geod_cas_mean[2], gaod_cas_mean[2], gerd_cas_mean[2]]\n",
    "gcausal_se_stage_3 = [gspd_cas_se[2], geod_cas_se[2], gaod_cas_se[2], gerd_cas_se[2]]\n",
    "# Create subplots with 2 rows and 2 columns\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Statistical Local and Global Fairness for MV\n",
    "plot_fairness_metrics(axs[0, 0], local_mean_stage_1, global_mean_stage_1, local_se_stage_1, global_se_stage_1, \n",
    "                      'Statistical Local and Global Fairness - SS', metric_labels)\n",
    "\n",
    "# Plot 2: Causal Local and Global Fairness for MV\n",
    "plot_fairness_metrics(axs[0, 1], causal_means_stage_1, gcausal_means_stage_1, causal_se_stage_1, gcausal_se_stage_1, \n",
    "                      'Causal Local and Global Fairness - SS', metric_labels)\n",
    "\n",
    "# Plot 3: Statistical Local and Global Fairness for FE\n",
    "plot_fairness_metrics(axs[1, 0], local_mean_stage_2, global_mean_stage_2, local_se_stage_2, global_se_stage_2, \n",
    "                      'Statistical Local and Global Fairness - Custom FE ', metric_labels)\n",
    "\n",
    "# Plot 4: Causal Local and Global Fairness for FE\n",
    "plot_fairness_metrics(axs[1, 1], causal_means_stage_2, gcausal_means_stage_2, causal_se_stage_2, gcausal_se_stage_2, \n",
    "                      'Causal Local and Global Fairness - Custom FE', metric_labels)\n",
    "\n",
    "# Plot 3: Statistical Local and Global Fairness for FE\n",
    "plot_fairness_metrics(axs[2, 0], local_mean_stage_3, global_mean_stage_3, local_se_stage_3, global_se_stage_3, \n",
    "                      'Statistical Local and Global Fairness - Stratify', metric_labels)\n",
    "\n",
    "# Plot 4: Causal Local and Global Fairness for FE\n",
    "plot_fairness_metrics(axs[2, 1], causal_means_stage_3, gcausal_means_stage_3, causal_se_stage_3, gcausal_se_stage_3, \n",
    "                      'Causal Local and Global Fairness - Stratify', metric_labels)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_propensity_scores(X, treatment_column):\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Convert to dense if X is sparse\n",
    "    if isinstance(X, csr_matrix):\n",
    "        X = X.toarray()\n",
    "    \n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    # Use the provided treatment column directly\n",
    "    treatment = np.where((treatment_column == 4) | (treatment_column == 'White'), 1, 0)\n",
    "\n",
    "    # Use the imputed covariate matrix without the treatment column\n",
    "    X_covariates = X_imputed\n",
    "\n",
    "    # One-hot encode the covariate matrix (if needed)\n",
    "    X_encoded = pd.get_dummies(pd.DataFrame(X_covariates), drop_first=True).values\n",
    "\n",
    "    # Estimate propensity scores using logistic regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_encoded, treatment)\n",
    "    propensity_scores = lr.predict_proba(X_encoded)[:, 1]\n",
    "\n",
    "    return propensity_scores, treatment\n",
    "\n",
    "def perform_matching(propensity_scores, treatment):\n",
    "    # Define treated and control indices based on the treatment variable\n",
    "    treated_indices = np.where((treatment == 1))[0]  # 'White' or 4 are considered treated\n",
    "    control_indices = np.where((treatment == 0))[0]  # All others are control\n",
    "\n",
    "    # Fit the nearest neighbors model on control propensity scores\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(propensity_scores[control_indices].reshape(-1, 1))\n",
    "\n",
    "    # Find the nearest neighbors for treated units\n",
    "    distances, indices = nn.kneighbors(propensity_scores[treated_indices].reshape(-1, 1))\n",
    "\n",
    "    # Map control indices to matched treated indices\n",
    "    matched_control_indices = control_indices[indices.flatten()]\n",
    "\n",
    "    # Combine treated and matched control indices\n",
    "    matched_indices = np.concatenate([treated_indices, matched_control_indices])\n",
    "\n",
    "    # Ensure indices are unique and within bounds\n",
    "    matched_indices = np.unique(matched_indices)\n",
    "    matched_indices = matched_indices[matched_indices < len(treatment)]\n",
    "\n",
    "    return matched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99923161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def flip_race(attribute):\n",
    "    return ['Non-White' if x == '4' else 'White' for x in attribute]\n",
    "\n",
    "class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, handle_unknown='ignore'):\n",
    "        self.columns = columns\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            for col in self.columns:\n",
    "                self.encoders[col] = LabelEncoder()\n",
    "                self.encoders[col].fit(X[col])\n",
    "        else:\n",
    "            for i, col in enumerate(self.columns):\n",
    "                self.encoders[col] = LabelEncoder()\n",
    "                self.encoders[col].fit(X[:, i])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        if isinstance(X_copy, pd.DataFrame):\n",
    "            for col in self.columns:\n",
    "                X_copy[col] = X_copy[col].map(lambda s: self.encoders[col].transform([s])[0] \n",
    "                                              if s in self.encoders[col].classes_ else -1)\n",
    "        else:\n",
    "            for i, col in enumerate(self.columns):\n",
    "                X_copy[:, i] = np.array([self.encoders[col].transform([s])[0] \n",
    "                                         if s in self.encoders[col].classes_ else -1 \n",
    "                                         for s in X_copy[:, i]])\n",
    "        return X_copy\n",
    "\n",
    "# The rest of your code remains the same\n",
    "\n",
    "# Paths to train and test datasets\n",
    "train_path = 'C:/Users/Saadia/FairPreprocessing/data/adult/adult.data'\n",
    "test_path = 'C:/Users/Saadia/FairPreprocessing/data/adult/adult.test'\n",
    "\n",
    "# Define column names and missing values\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', \n",
    "                'capital-loss', 'hours-per-week', 'native-country', 'income-per-year']\n",
    "na_values = ['?']\n",
    "\n",
    "# Read the train and test datasets\n",
    "train = pd.read_csv(train_path, header=None, names=column_names, \n",
    "                    skipinitialspace=True, na_values=na_values)\n",
    "test = pd.read_csv(test_path, header=None, names=column_names,\n",
    "                   skipinitialspace=True, na_values=na_values)\n",
    "\n",
    "# Map target variable\n",
    "train['income-per-year'] = train['income-per-year'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\n",
    "test['income-per-year'] = test['income-per-year'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1})\n",
    "\n",
    "# Combine train and test data for re-splitting\n",
    "combined_data = pd.concat([train, test])\n",
    "\n",
    "# Drop rows with missing target values\n",
    "combined_data = combined_data.dropna(subset=['income-per-year'])\n",
    "\n",
    "X_combined = combined_data.drop('income-per-year', axis=1)\n",
    "y_combined = combined_data['income-per-year']\n",
    "\n",
    "# Categorical and numerical columns\n",
    "categorical_cols = X_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_combined.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Convert all categorical columns to strings to avoid type issues\n",
    "X_combined[categorical_cols] = X_combined[categorical_cols].astype(str)\n",
    "\n",
    "# Initialize lists to store results\n",
    "spd_list = []\n",
    "eod_list = []\n",
    "aod_list = []\n",
    "erd_list = []\n",
    "\n",
    "spd_cas_list = []\n",
    "eod_cas_list = []\n",
    "aod_cas_list = []\n",
    "erd_cas_list = []\n",
    "\n",
    "gspd_list = []\n",
    "geod_list = []\n",
    "gaod_list = []\n",
    "gerd_list = []\n",
    "\n",
    "gspd_cas_list = []\n",
    "geod_cas_list = []\n",
    "gaod_cas_list = []\n",
    "gerd_cas_list = []\n",
    "\n",
    "# Perform 10 iterations\n",
    "for i in range(10):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    # Randomly split the data into training and test sets\n",
    "    X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42, stratify=y_combined)\n",
    "    X_train_no_strat, X_test_no_strat, y_train_no_strat, y_test_no_strat = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42)\n",
    "\n",
    "    pipeline_1.fit(X_train_strat, y_train_strat)\n",
    "    pipeline_2.fit(X_train_strat, y_train_strat)\n",
    "    pipeline_3.fit(X_train_strat, y_train_strat)\n",
    "\n",
    "    # For P4\n",
    "    pipeline_4.fit(X_train_no_strat, y_train_no_strat)\n",
    "\n",
    "    y_pred_1 = pipeline_1.predict(X_test_strat)\n",
    "    y_pred_2 = pipeline_2.predict(X_test_strat)\n",
    "    y_pred_3 = pipeline_3.predict(X_test_strat)\n",
    "\n",
    "# For P4\n",
    "    y_pred_4 = pipeline_4.predict(X_test_no_strat)\n",
    "    \n",
    "    \n",
    "    res1 = guard_and_mitigate_if_needed(\n",
    "        pipeline=pipeline_1,\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test,   y_test=y_test,\n",
    "        sensitive_attr_test=s_test,\n",
    "        thresholds=thresholds,\n",
    "        mitigations_cfg=mitigations_cfg,  # the same list you pass to run_mitigations_and_predict\n",
    "        context=common_context,           # e.g., {\"s_train\": s_train, ...}\n",
    "        fairness_compute_type=\"global\",   # or \"local\"\n",
    "        y2_scores=None                    # provide scores if your fairness function needs them\n",
    "    )\n",
    "    y_pred_1n = res1[\"y_pred_final\"]\n",
    "\n",
    "    res2 = guard_and_mitigate_if_needed(\n",
    "        pipeline=pipeline_2, X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test, y_test=y_test, sensitive_attr_test=s_test,\n",
    "        thresholds=thresholds, mitigations_cfg=mitigations_cfg, context=common_context\n",
    "    )\n",
    "    y_pred_2n = res2[\"y_pred_final\"]\n",
    "\n",
    "    res3 = guard_and_mitigate_if_needed(\n",
    "        pipeline=pipeline_3, X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test, y_test=y_test, sensitive_attr_test=s_test,\n",
    "        thresholds=thresholds, mitigations_cfg=mitigations_cfg, context=common_context\n",
    "    )\n",
    "    y_pred_3n = res3[\"y_pred_final\"]\n",
    "\n",
    "    res4 = guard_and_mitigate_if_needed(\n",
    "        pipeline=pipeline_4, X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test, y_test=y_test, sensitive_attr_test=s_test,\n",
    "        thresholds=thresholds, mitigations_cfg=mitigations_cfg, context=common_context\n",
    "    )\n",
    "    y_pred_4n = res[\"y_pred_final\"]\n",
    "    \n",
    "    SPD_1, EOD_1, AOD_1, ERD_1 = Fairness_Metrics_Computation(y_pred_1n, y_pred_2n, y_test, X_test['sex'],compute_type='local')\n",
    "    SPD_2, EOD_2, AOD_2, ERD_2 = Fairness_Metrics_Computation(y_pred_1n, y_pred_3n, y_test, X_test['sex'],compute_type='local')\n",
    "    SPD_3, EOD_3, AOD_3, ERD_3 = Fairness_Metrics_Computation(y_pred_1n, y_pred_4n, y_test, X_test['sex'],compute_type='local')\n",
    "    \n",
    "    # Store results\n",
    "    spd_list.append((SPD_1, SPD_2, SPD_3))\n",
    "    eod_list.append((EOD_1, EOD_2, EOD_3))\n",
    "    aod_list.append((AOD_1, AOD_2, AOD_3))\n",
    "    erd_list.append((ERD_1, ERD_2, ERD_3))\n",
    "    \n",
    "    gSPD_1, gEOD_1, gAOD_1, gERD_1 = Fairness_Metrics_Computation(y_pred_1n y_pred_2n, y_test, X_test['sex'],compute_type='global')\n",
    "    gSPD_2, gEOD_2, gAOD_2, gERD_2 = Fairness_Metrics_Computation(y_pred_1n, y_pred_3n, y_test, X_test['sex'],compute_type='global')\n",
    "    gSPD_3, gEOD_3, gAOD_3, gERD_3 = Fairness_Metrics_Computation(y_pred_1n, y_pred_4n, y_test, X_test['sex'],compute_type='global')\n",
    "    \n",
    "    # Store results\n",
    "    gspd_list.append((gSPD_1, gSPD_2, gSPD_3))\n",
    "    geod_list.append((gEOD_1, gEOD_2, gEOD_3))\n",
    "    gaod_list.append((gAOD_1, gAOD_2, gAOD_3))\n",
    "    gerd_list.append((gERD_1, gERD_2, gERD_3))\n",
    "       \n",
    "    propensity_scores1, treatment1 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices1 = perform_matching(propensity_scores1, treatment1)\n",
    "\n",
    "    propensity_scores2, treatment2 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices2 = perform_matching(propensity_scores2, treatment2)\n",
    "\n",
    "    propensity_scores3, treatment3 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices3 = perform_matching(propensity_scores3, treatment3)\n",
    "\n",
    "    propensity_scores4, treatment4 = compute_propensity_scores(X_test, X_test['race'])\n",
    "    matched_indices4 = perform_matching(propensity_scores4, treatment4)\n",
    "    \n",
    "    common_matched_indices = set(matched_indices1).intersection(matched_indices2).intersection(matched_indices3).intersection(matched_indices4)\n",
    "    common_matched_indices = list(common_matched_indices)\n",
    "    common_matched_indices.sort()\n",
    "\n",
    "    y1_matched = y_pred_1n[common_matched_indices]\n",
    "    y2_matched = y_pred_2n[common_matched_indices]\n",
    "    y3_matched = y_pred_3n[common_matched_indices]\n",
    "    y4_matched = y_pred_4[common_matched_indices]\n",
    "    \n",
    "    y_test_matched = y_test.reset_index(drop=True)[common_matched_indices]\n",
    "    sex_matched = X_test['race'].reset_index(drop=True)[common_matched_indices]\n",
    "    \n",
    "    # Compute causal fairness metrics\n",
    "    SPD_1_cas, EOD_1_cas, AOD_1_cas, ERD_1_cas = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched ,compute_type='local')\n",
    "    SPD_2_cas, EOD_2_cas, AOD_2_cas, ERD_2_cas = Fairness_Metrics_Computation(y1_matched, y3_matched, y_test_matched, sex_matched,compute_type='local')\n",
    "    SPD_3_cas, EOD_3_cas, AOD_3_cas, ERD_3_cas = Fairness_Metrics_Computation(y1_matched, y4_matched, y_test_matched, sex_matched,compute_type='local')\n",
    "    \n",
    "    # Store causal results\n",
    "    spd_cas_list.append((SPD_1_cas, SPD_2_cas, SPD_3_cas))\n",
    "    eod_cas_list.append((EOD_1_cas, EOD_2_cas, EOD_3_cas))\n",
    "    aod_cas_list.append((AOD_1_cas, AOD_2_cas, AOD_3_cas))\n",
    "    erd_cas_list.append((ERD_1_cas, ERD_2_cas, ERD_3_cas))\n",
    "\n",
    "    gSPD_1_cas, gEOD_1_cas, gAOD_1_cas, gERD_1_cas = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched ,compute_type='global')\n",
    "    gSPD_2_cas, gEOD_2_cas, gAOD_2_cas, gERD_2_cas = Fairness_Metrics_Computation(y1_matched, y3_matched, y_test_matched, sex_matched,compute_type='global')\n",
    "    gSPD_3_cas, gEOD_3_cas, gAOD_3_cas, gERD_3_cas = Fairness_Metrics_Computation(y1_matched, y4_matched, y_test_matched, sex_matched,compute_type='global')\n",
    "    \n",
    "    # Store causal results\n",
    "    gspd_cas_list.append((gSPD_1_cas, gSPD_2_cas, gSPD_3_cas))\n",
    "    geod_cas_list.append((gEOD_1_cas, gEOD_2_cas, gEOD_3_cas))\n",
    "    gaod_cas_list.append((gAOD_1_cas, gAOD_2_cas, gAOD_3_cas))\n",
    "    gerd_cas_list.append((gERD_1_cas, gERD_2_cas, gERD_3_cas))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means_and_errors(array):\n",
    "    mean = array.mean(axis=0) * 0.1\n",
    "    se = (array.std(axis=0) / np.sqrt(array.shape[0])) * 0.1\n",
    "    return np.round(mean, 3), np.round(se, 3)\n",
    "\n",
    "\n",
    "spd_array = np.array(spd_list)\n",
    "eod_array = np.array(eod_list)\n",
    "aod_array = np.array(aod_list)\n",
    "erd_array = np.array(erd_list)\n",
    "\n",
    "gspd_array = np.array(gspd_list)\n",
    "geod_array = np.array(geod_list)\n",
    "gaod_array = np.array(gaod_list)\n",
    "gerd_array = np.array(gerd_list)\n",
    "\n",
    "spd_cas_array = np.array(spd_cas_list)\n",
    "eod_cas_array = np.array(eod_cas_list)\n",
    "aod_cas_array = np.array(aod_cas_list)\n",
    "erd_cas_array = np.array(erd_cas_list)\n",
    "\n",
    "gspd_cas_array = np.array(gspd_cas_list)\n",
    "geod_cas_array = np.array(geod_cas_list)\n",
    "gaod_cas_array = np.array(gaod_cas_list)\n",
    "gerd_cas_array = np.array(gerd_cas_list)\n",
    "\n",
    "\n",
    "spd_mean, spd_se = compute_means_and_errors(spd_array)\n",
    "eod_mean, eod_se = compute_means_and_errors(eod_array)\n",
    "aod_mean, aod_se = compute_means_and_errors(aod_array)\n",
    "erd_mean, erd_se = compute_means_and_errors(erd_array)\n",
    "\n",
    "gspd_mean, gspd_se = compute_means_and_errors(gspd_array)\n",
    "geod_mean, geod_se = compute_means_and_errors(geod_array)\n",
    "gaod_mean, gaod_se = compute_means_and_errors(gaod_array)\n",
    "gerd_mean, gerd_se = compute_means_and_errors(gerd_array)\n",
    "\n",
    "spd_cas_mean, spd_cas_se = compute_means_and_errors(spd_cas_array)\n",
    "eod_cas_mean, eod_cas_se = compute_means_and_errors(eod_cas_array)\n",
    "aod_cas_mean, aod_cas_se = compute_means_and_errors(aod_cas_array)\n",
    "erd_cas_mean, erd_cas_se = compute_means_and_errors(erd_cas_array)\n",
    "\n",
    "gspd_cas_mean, gspd_cas_se = compute_means_and_errors(gspd_cas_array)\n",
    "geod_cas_mean, geod_cas_se = compute_means_and_errors(geod_cas_array)\n",
    "gaod_cas_mean, gaod_cas_se = compute_means_and_errors(gaod_cas_array)\n",
    "gerd_cas_mean, gerd_cas_se = compute_means_and_errors(gerd_cas_array)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your fairness metrics arrays are correctly formatted\n",
    "# Metric labels\n",
    "metric_labels = ['SPD', 'EOD', 'AOD', 'ERD']\n",
    "\n",
    "# Colors for the bars\n",
    "colors = ['blue', 'orange']\n",
    "labels = ['Local', 'Global']\n",
    "\n",
    "# Plot function for fairness metrics\n",
    "def plot_fairness_metrics(ax, local_means, global_means, local_se, global_se, title, metric_labels):\n",
    "    x = np.arange(len(metric_labels))  # label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, local_means, width, label='Local', yerr=local_se, capsize=5)\n",
    "    rects2 = ax.bar(x + width/2, global_means, width, label='Global', yerr=global_se, capsize=5)\n",
    "\n",
    "    # Add labels, title, and custom ticks\n",
    "    ax.set_xlabel('Fairness Metrics')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.legend()\n",
    "\n",
    "# Replace these with the actual means and errors for statistical and causal fairness\n",
    "# Statistical Local and Global for PCA\n",
    "local_mean_stage_1 = [spd_mean[0], eod_mean[0], aod_mean[0], erd_mean[0]]\n",
    "local_se_stage_1 = [spd_se[0], eod_se[0], aod_se[0], erd_se[0]]\n",
    "\n",
    "global_mean_stage_1 = [gspd_mean[0], geod_mean[0], gaod_mean[0], gerd_mean[0]]\n",
    "global_se_stage_1 = [gspd_se[0], geod_se[0], gaod_se[0], gerd_se[0]]\n",
    "\n",
    "# Causal Local and Global for PCA\n",
    "causal_means_stage_1 = [spd_cas_mean[0], -eod_cas_mean[0], aod_cas_mean[0], erd_cas_mean[0]]\n",
    "causal_se_stage_1 = [spd_cas_se[0], eod_cas_se[0], aod_cas_se[0], erd_cas_se[0]]\n",
    "\n",
    "gcausal_means_stage_1 = [gspd_cas_mean[0], geod_cas_mean[0], gaod_cas_mean[0], gerd_cas_mean[0]]\n",
    "gcausal_se_stage_1 = [gspd_cas_se[0], geod_cas_se[0], gaod_cas_se[0], gerd_cas_se[0]]\n",
    "\n",
    "# Statistical Local and Global for SelectKBest\n",
    "local_mean_stage_2 = [spd_mean[1], eod_mean[1], aod_mean[1], erd_mean[1]]\n",
    "local_se_stage_2 = [spd_se[1], eod_se[1], aod_se[1], erd_se[1]]\n",
    "\n",
    "global_mean_stage_2 = [gspd_mean[1], geod_mean[1], gaod_mean[1], gerd_mean[1]]\n",
    "global_se_stage_2 = [gspd_se[1], geod_se[1], gaod_se[1], gerd_se[1]]\n",
    "\n",
    "# Causal Local and Global for SelectKBest\n",
    "causal_means_stage_2 = [spd_cas_mean[1], eod_cas_mean[1], aod_cas_mean[1], erd_cas_mean[1]]\n",
    "causal_se_stage_2 = [spd_cas_se[1], eod_cas_se[1], aod_cas_se[1], erd_cas_se[1]]\n",
    "\n",
    "gcausal_means_stage_2 = [gspd_cas_mean[1], geod_cas_mean[1], gaod_cas_mean[1], gerd_cas_mean[1]]\n",
    "gcausal_se_stage_2 = [gspd_cas_se[1], geod_cas_se[1], gaod_cas_se[1], gerd_cas_se[1]]\n",
    "\n",
    "\n",
    "local_mean_stage_3 = [spd_mean[2], eod_mean[2], aod_mean[2], erd_mean[2]]\n",
    "local_se_stage_3 = [spd_se[2], eod_se[2], aod_se[2], erd_se[2]]\n",
    "\n",
    "global_mean_stage_3 = [gspd_mean[2], geod_mean[2], gaod_mean[2], gerd_mean[2]]\n",
    "global_se_stage_3 = [gspd_se[2], geod_se[2], gaod_se[2], gerd_se[2]]\n",
    "\n",
    "# Causal Local and Global for SelectKBest\n",
    "causal_means_stage_3 = [spd_cas_mean[2], eod_cas_mean[2], aod_cas_mean[2], erd_cas_mean[2]]\n",
    "causal_se_stage_3 = [spd_cas_se[2], eod_cas_se[2], aod_cas_se[2], erd_cas_se[2]]\n",
    "\n",
    "gcausal_means_stage_3 = [gspd_cas_mean[2], geod_cas_mean[2], gaod_cas_mean[2], gerd_cas_mean[2]]\n",
    "gcausal_se_stage_3 = [gspd_cas_se[2], geod_cas_se[2], gaod_cas_se[2], gerd_cas_se[2]]\n",
    "# Create subplots with 2 rows and 2 columns\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Statistical Local and Global Fairness for MV\n",
    "plot_fairness_metrics(axs[0, 0], local_mean_stage_1, global_mean_stage_1, local_se_stage_1, global_se_stage_1, \n",
    "                      'Statistical Local and Global Fairness - SS', metric_labels)\n",
    "\n",
    "# Plot 2: Causal Local and Global Fairness for MV\n",
    "plot_fairness_metrics(axs[0, 1], gcausal_means_stage_1, causal_means_stage_1, gcausal_se_stage_1, causal_se_stage_1, \n",
    "                      'Causal Local and Global Fairness - SS', metric_labels)\n",
    "\n",
    "# Plot 3: Statistical Local and Global Fairness for FE\n",
    "plot_fairness_metrics(axs[1, 0], local_mean_stage_2, global_mean_stage_2, local_se_stage_2, global_se_stage_2, \n",
    "                      'Statistical Local and Global Fairness - Custom FE ', metric_labels)\n",
    "\n",
    "# Plot 4: Causal Local and Global Fairness for FE\n",
    "plot_fairness_metrics(axs[1, 1], gcausal_means_stage_2, causal_means_stage_2, causal_se_stage_2, gcausal_se_stage_2, \n",
    "                      'Causal Local and Global Fairness - Custom FE', metric_labels)\n",
    "\n",
    "# Plot 3: Statistical Local and Global Fairness for FE\n",
    "plot_fairness_metrics(axs[2, 0], local_mean_stage_3, global_mean_stage_3, local_se_stage_3, global_se_stage_3, \n",
    "                      'Statistical Local and Global Fairness - Stratify', metric_labels)\n",
    "\n",
    "# Plot 4: Causal Local and Global Fairness for FE\n",
    "plot_fairness_metrics(axs[2, 1], causal_means_stage_3, gcausal_means_stage_3, causal_se_stage_3, gcausal_se_stage_3, \n",
    "                      'Causal Local and Global Fairness - Stratify', metric_labels)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d8dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b18b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
