{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a22ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# from imblearn.over_sampling import SMOTE, SVMSMOTE\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, KBinsDiscretizer, Normalizer, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "from sklearn.decomposition import PCA, NMF, SparsePCA, KernelPCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "# !pip install scikit-learn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_S = SVC(kernel='rbf', C=1.0, gamma='scale', verbose=False)\n",
    "classifier_R = RandomForestClassifier(verbose=False)\n",
    "classifier_D = DecisionTreeClassifier()\n",
    "classifier_G = GradientBoostingClassifier(verbose=False)\n",
    "classifier_K = KNeighborsClassifier()\n",
    "classifier_GNB = GaussianNB()\n",
    "classifier_GB = GradientBoostingClassifier(verbose=False)\n",
    "classifier_NN = MLPClassifier()\n",
    "classifier_IDA = QuadraticDiscriminantAnalysis()\n",
    "classifier_QDA = QuadraticDiscriminantAnalysis()\n",
    "classifier_ADB = AdaBoostClassifier()\n",
    "classifier_GP = GaussianProcessClassifier()\n",
    "classifier_XGBC = XGBClassifier(verbosity=0)\n",
    "classifier_LGBM = LGBMClassifier()\n",
    "classifier_BC = BaggingClassifier(verbose=False)\n",
    "classifier_CB = CatBoostClassifier(iterations=100, verbose=False)\n",
    "\n",
    "classifiers = [\n",
    "     ('SVC', classifier_S),\n",
    "     ('RF', classifier_R),\n",
    "     ('DT', classifier_D),\n",
    "       ('GB', classifier_G),\n",
    "     ('KNN', classifier_K),\n",
    "       ('GNB', classifier_GNB),\n",
    "     ('GBC', classifier_GB),\n",
    "     ('NN', classifier_NN),\n",
    "     ('IDA', classifier_IDA),\n",
    "     ('QDA', classifier_QDA),  \n",
    "      ('ADB', classifier_ADB),\n",
    "#      ('GP', classifier_GP),\n",
    "     ('XGB', classifier_XGBC),\n",
    "     ('LGB', classifier_LGBM),\n",
    "     ('BC', classifier_BC),\n",
    "     ('CB', classifier_CB),     \n",
    "]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eac6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Fairness_Metrics_Computation(y1, y2, y, attribute):\n",
    "    # Convert inputs to pandas Series for easier handling\n",
    "    y1 = pd.Series(y1)\n",
    "    y2 = pd.Series(y2)\n",
    "    y = pd.Series(y)\n",
    "    attribute = pd.Series(attribute)\n",
    "\n",
    "    # Identify privileged and unprivileged groups\n",
    "    privileged = (attribute == 'male') | (attribute == 1)\n",
    "    unprivileged = ~privileged\n",
    "\n",
    "    # Counts of privileged and unprivileged groups\n",
    "    count_privileged = np.sum(privileged)\n",
    "    count_unprivileged = np.sum(unprivileged)\n",
    "\n",
    "    # SPD Calculation\n",
    "    SFC_SPD_P = np.sum([(1 if y1_i == 1 and y2_i == 0 else -1 if y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y1_i, y2_i, attr in zip(y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_SPD_NP = np.sum([(1 if y1_i == 1 and y2_i == 0 else -1 if y1_i == 0 and y2_i == 1 else 0)\n",
    "                         for y1_i, y2_i, attr in zip(y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "\n",
    "    SFR_SPD_p = SFC_SPD_P / count_privileged if count_privileged > 0 else 0\n",
    "    SFR_SPD_np = SFC_SPD_NP / count_unprivileged if count_unprivileged > 0 else 0\n",
    "    SPD = SFR_SPD_np - SFR_SPD_p\n",
    "\n",
    "    # EOD Calculation\n",
    "    SFC_EOD_P = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_EOD_NP = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                         for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "\n",
    "    P_Y1 = np.sum((y == 1) & privileged)\n",
    "    NP_Y1 = np.sum((y == 1) & unprivileged)\n",
    "\n",
    "    SFR_EOD_p = SFC_EOD_P / P_Y1 if P_Y1 > 0 else 0\n",
    "    SFR_EOD_np = SFC_EOD_NP / NP_Y1 if NP_Y1 > 0 else 0\n",
    "    EOD = SFR_EOD_np - SFR_EOD_p\n",
    "\n",
    "    # AOD Calculation\n",
    "    SFC_TP_P = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_TP_NP = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "    SFC_FP_P = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_FP_NP = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "\n",
    "    P_Y0 = np.sum((y == 0) & privileged)\n",
    "    NP_Y0 = np.sum((y == 0) & unprivileged)\n",
    "\n",
    "    SFR_AOD_p = (SFC_TP_P / P_Y1 if P_Y1 > 0 else 0) + (SFC_FP_P / P_Y0 if P_Y0 > 0 else 0)\n",
    "    SFR_AOD_np = (SFC_TP_NP / NP_Y1 if NP_Y1 > 0 else 0) + (SFC_FP_NP / NP_Y0 if NP_Y0 > 0 else 0)\n",
    "    AOD = (SFR_AOD_np - SFR_AOD_p) / 2\n",
    "\n",
    "    # ERD Calculation\n",
    "    SFC_FN_P = np.sum([(-1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_FN_NP = np.sum([(-1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "    SFC_FP_P = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_FP_NP = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "\n",
    "    SFR_ERR_p = (SFC_FP_P / P_Y0 if P_Y0 > 0 else 0) + (SFC_FN_P / P_Y1 if P_Y1 > 0 else 0)\n",
    "    SFR_ERR_np = (SFC_FP_NP / NP_Y0 if NP_Y0 > 0 else 0) + (SFC_FN_NP / NP_Y1 if NP_Y1 > 0 else 0)\n",
    "    ERD = SFR_ERR_np - SFR_ERR_p\n",
    "\n",
    "    return SPD, EOD, AOD, ERD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59403abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/Saadia/FairPreprocessing/data/bank/bank-additional-full.csv'\n",
    "na_values = ['unknown', 'nan', '']  # More clearly defined missing value indicators\n",
    "df = pd.read_csv(file_path, sep=';', na_values=na_values)\n",
    "\n",
    "# Apply custom transformations\n",
    "df['age'] = df['age'].apply(lambda x: float(x >= 25))  # Transforming 'age' into a binary feature\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "# Convert 'y' labels to numeric\n",
    "y = y.map({'no': 0, 'yes': 1})\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Define a column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), numerical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Manually compute sample weights for handling class imbalance\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# Define a pipeline incorporating Random Forest with class weight balancing\n",
    "pipeline_1 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=200,  # Increased estimators\n",
    "        max_depth=15,      # Increased depth for more complex trees\n",
    "        min_samples_split=10,  # Require more samples to split\n",
    "        class_weight='balanced'  # Handle class imbalance\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define a second pipeline without class weight balancing and different parameters\n",
    "pipeline_2 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=80,   # Reduced estimators\n",
    "        max_depth=10,      # Shallower depth\n",
    "        min_samples_leaf=4 # Increased leaf samples to prevent overfitting\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the pipelines using the training data\n",
    "pipeline_1.fit(X_train, y_train, classifier__sample_weight=sample_weights)\n",
    "pipeline_2.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred_1 = pipeline_1.predict(X_test)\n",
    "y_pred_2 = pipeline_2.predict(X_test)\n",
    "\n",
    "accuracy_class_weight = accuracy_score(y_test, y_pred_1)\n",
    "accuracy_no_class_weight = accuracy_score(y_test, y_pred_2)\n",
    "\n",
    "print(f'Accuracy with Class Weighting: {accuracy_class_weight:.5f}')\n",
    "print(f'Accuracy without Class Weighting: {accuracy_no_class_weight:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = accuracy_score(y_test, y_pred_1)\n",
    "A2 = accuracy_score(y_test,y_pred_2)\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_1)\n",
    "f2 = f1_score(y_test, y_pred_2)\n",
    "A1,A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "pipeline_baseline = DummyClassifier(strategy='most_frequent')\n",
    "pipeline_baseline.fit(X_train, y_train)\n",
    "y_pred_baseline = pipeline_baseline.predict(X_test)\n",
    "mispred_baseline = (y_test != y_pred_baseline).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74201249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_biases(mispred_p1, mispred_p2, mispred_baseline):\n",
    "    \"\"\"\n",
    "    Calculate individual biases and total bias.\n",
    "    \"\"\"\n",
    "    # Calculating individual biases\n",
    "    BD = mispred_baseline\n",
    "    BStage = mispred_p1 - mispred_p2  # Preprocessing stage bias\n",
    "    BC = mispred_p2 - mispred_baseline  # Classifier bias\n",
    "\n",
    "    # Calculating Stage-Classifier Interaction term\n",
    "    Stage_Classifier_Interaction = mispred_p1 - (BD + BStage + BC)\n",
    "\n",
    "    # Total bias calculation\n",
    "    TB = BD + BStage + BC + Stage_Classifier_Interaction\n",
    "\n",
    "    return {\n",
    "        'BD': BD,\n",
    "        'BC': BC,\n",
    "        'BStage': BStage,\n",
    "        'Stage_Classifier_Interaction': Stage_Classifier_Interaction,\n",
    "        'TB': TB,\n",
    "        'TB matches mispred_p1': TB == mispred_p1\n",
    "    }\n",
    "\n",
    "def plot_biases(biases, mispred_p1):\n",
    "    \"\"\"\n",
    "    Plot the biases and observed mispredictions.\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        'Data (BD)', \n",
    "         \n",
    "        'Preprocessing Stage (BStage)', \n",
    "        'Stage-Classifier Interaction', \n",
    "        'Classifier (BC)',\n",
    "        'Total Bias (TB)', \n",
    "        'Observed Bias (mispred_p1)'\n",
    "    ]\n",
    "    \n",
    "    values = [\n",
    "        biases['BD'], \n",
    "         \n",
    "        biases['BStage'], \n",
    "        biases['Stage_Classifier_Interaction'], \n",
    "        biases['BC'],\n",
    "        biases['TB'], \n",
    "        mispred_p1\n",
    "    ]\n",
    "\n",
    "    colors = ['blue', 'orange', 'green', 'red', 'pink', 'gray']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(labels, values, color=colors)\n",
    "    plt.xlabel('Bias / Mispredictions')\n",
    "    plt.title('Bias Analysis and Observed Mispredictions')\n",
    "\n",
    "    # Adding annotations\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        label_x_pos = width if width < 0 else width\n",
    "        plt.text(\n",
    "            label_x_pos, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{width:.2f}', ha='center', va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.axvline(0, color='black', linewidth=0.8)\n",
    "    plt.xlim(min(values) - 10, max(values) + 10)  # Adding some padding to the x-axis\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Baseline model for comparison\n",
    "\n",
    "\n",
    "# Calculate mispredictions\n",
    "aligned_indices = X_test.notna().all(axis=1)\n",
    "\n",
    "mispred_p1 = sum(y_test != y_pred_1)\n",
    "mispred_p2 = sum(y_test != y_pred_2)\n",
    "# mispred_baseline = sum(y_test != y_pred_baseline)\n",
    "\n",
    "# Calculate biases\n",
    "biases = calculate_biases(mispred_p1, mispred_p2, mispred_baseline)\n",
    "\n",
    "# Plot the biases\n",
    "plot_biases(biases, mispred_p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming y_pred_1, y_pred_2, y_pred_baseline, and y_test are already defined\n",
    "# Convert y_test and predictions to numpy arrays to avoid indexing issues\n",
    "y_test = np.array(y_test)\n",
    "y_pred_1 = np.array(y_pred_1)\n",
    "y_pred_2 = np.array(y_pred_2)\n",
    "y_pred_baseline = np.array(y_pred_baseline)\n",
    "\n",
    "# Initialize volatility arrays\n",
    "volatility_LE = np.zeros(len(y_test))\n",
    "volatility_classifier = np.zeros(len(y_test))\n",
    "\n",
    "# Calculate Volatility Scores for LE (Vol_St1)\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1[i] == y_test[i] and y_pred_2[i] == y_test[i]:\n",
    "        volatility_LE[i] = 0\n",
    "    elif y_pred_1[i] == y_test[i] and y_pred_2[i] != y_test[i]:\n",
    "        volatility_LE[i] = 1\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_2[i] == y_test[i]:\n",
    "        volatility_LE[i] = 2\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_2[i] != y_test[i] and y_pred_1[i] == y_pred_2[i]:\n",
    "        volatility_LE[i] = 3\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_2[i] != y_test[i] and y_pred_1[i] != y_pred_2[i]:\n",
    "        volatility_LE[i] = 4\n",
    "\n",
    "# Volatility score calculation for Classifier (Vol_Cl)\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1[i] == y_test[i] and y_pred_baseline[i] == y_test[i]:\n",
    "        volatility_classifier[i] = 0\n",
    "    elif y_pred_1[i] == y_test[i] and y_pred_baseline[i] != y_test[i]:\n",
    "        volatility_classifier[i] = 1\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_baseline[i] == y_test[i]:\n",
    "        volatility_classifier[i] = 2\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_baseline[i] != y_test[i] and y_pred_1[i] == y_pred_baseline[i]:\n",
    "        volatility_classifier[i] = 3\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_baseline[i] != y_test[i] and y_pred_1[i] != y_pred_baseline[i]:\n",
    "        volatility_classifier[i] = 4\n",
    "\n",
    "# Summarize volatility scores\n",
    "volatility_summary = {\n",
    "    'Condition': ['Vol_St1=0', 'Vol_St1=1', 'Vol_St1=2', 'Vol_St1=3', 'Vol_St1=4',\n",
    "                  'Vol_Cl=0', 'Vol_Cl=1', 'Vol_Cl=2', 'Vol_Cl=3', 'Vol_Cl=4'],\n",
    "    'Count': [np.sum(volatility_LE == 0), np.sum(volatility_LE == 1), np.sum(volatility_LE == 2), np.sum(volatility_LE == 3), np.sum(volatility_LE == 4),\n",
    "              np.sum(volatility_classifier == 0), np.sum(volatility_classifier == 1), np.sum(volatility_classifier == 2), np.sum(volatility_classifier == 3), np.sum(volatility_classifier == 4)]\n",
    "}\n",
    "\n",
    "df_volatility = pd.DataFrame(volatility_summary)\n",
    "\n",
    "# Plot the volatility scores\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x='Condition', y='Count', data=df_volatility, palette='viridis')\n",
    "\n",
    "plt.title('Volatility Score Analysis')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add labels to each bar\n",
    "for bar in ax.patches:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height(),\n",
    "            f'{int(bar.get_height())}',\n",
    "            ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b181f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple, Any, Optional\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Optional: SMOTE if you plan to oversample (safe fallback if not installed)\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE  # noqa\n",
    "except Exception:\n",
    "    SMOTE = None  # oversampling step will be skipped if None\n",
    "\n",
    "\n",
    "# ---------- Mitigation framework (no hard-coding of stages) ----------\n",
    "MitigationFn = Callable[\n",
    "    [Any, np.ndarray, np.ndarray, Dict[str, Any]],\n",
    "    Tuple[Any, np.ndarray, np.ndarray, Dict[str, Any]]\n",
    "]\n",
    "# Signature: fn(pipeline, X_train, y_train, context) -> (pipeline, X_train, y_train, fit_params_delta)\n",
    "\n",
    "def run_mitigations_and_predict(\n",
    "    pipeline: Any,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test:  np.ndarray,\n",
    "    y_test:  Optional[np.ndarray] = None,\n",
    "    mitigations: Optional[List[Dict[str, Any]]] = None,\n",
    "    base_fit_params: Optional[Dict[str, Any]] = None,\n",
    "    context: Optional[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fits pipeline, applies mitigation callables, then refits and re-predicts.\n",
    "    Returns pre/post metrics & predictions. No random post-hoc tweaks.\n",
    "    \"\"\"\n",
    "    base_fit_params = dict(base_fit_params or {})\n",
    "    context = dict(context or {})\n",
    "    mitigations = mitigations or []\n",
    "\n",
    "    # 1) Baseline fit & predict (pre-mitigation)\n",
    "    pipeline.fit(X_train, y_train, **base_fit_params)\n",
    "    y_pred_pre = pipeline.predict(X_test)\n",
    "\n",
    "    metrics_pre = {}\n",
    "    if y_test is not None:\n",
    "        metrics_pre[\"accuracy\"] = float(accuracy_score(y_test, y_pred_pre))\n",
    "\n",
    "    # 2) Apply mitigations (may update pipeline/data/fit_params)\n",
    "    applied = []\n",
    "    X_train_m, y_train_m = X_train, y_train\n",
    "    fit_params = dict(base_fit_params)\n",
    "\n",
    "    for step in mitigations:\n",
    "        name = step.get(\"name\", \"<unnamed>\")\n",
    "        fn: MitigationFn = step[\"fn\"]\n",
    "        params = step.get(\"params\", {})\n",
    "        ctx = dict(context); ctx.update(params)\n",
    "\n",
    "        pipeline, X_train_m, y_train_m, delta = fn(pipeline, X_train_m, y_train_m, ctx)\n",
    "        if delta:\n",
    "            fit_params.update(delta)\n",
    "        applied.append({\"name\": name, \"fit_params_delta_keys\": list((delta or {}).keys())})\n",
    "\n",
    "    # 3) Refit post-mitigation & predict\n",
    "    pipeline.fit(X_train_m, y_train_m, **fit_params)\n",
    "    y_pred_post = pipeline.predict(X_test)\n",
    "\n",
    "    metrics_post = {}\n",
    "    if y_test is not None:\n",
    "        metrics_post[\"accuracy\"] = float(accuracy_score(y_test, y_pred_post))\n",
    "\n",
    "    return {\n",
    "        \"y_pred_pre\": y_pred_pre,\n",
    "        \"y_pred_post\": y_pred_post,\n",
    "        \"metrics_pre\": metrics_pre,\n",
    "        \"metrics_post\": metrics_post,\n",
    "        \"applied_mitigations\": applied,\n",
    "        \"pipeline_final\": pipeline\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Example mitigation primitives (compose as you like) ----------\n",
    "def mitigation_reweighing(pipeline, X_train, y_train, context):\n",
    "    \"\"\"\n",
    "    Adds per-sample weights into fit params (no data mutation).\n",
    "    Provide either:\n",
    "      - context[\"sample_weights\"] (np.ndarray aligned with y_train), or\n",
    "      - context[\"weight_fn\"](X_train, y_train, context) -> np.ndarray\n",
    "    You can change which estimator receives weights via context[\"weight_param\"]\n",
    "      e.g., \"classifier__sample_weight\" or \"clf__sample_weight\".\n",
    "    \"\"\"\n",
    "    if \"sample_weights\" in context:\n",
    "        w = np.asarray(context[\"sample_weights\"])\n",
    "    elif callable(context.get(\"weight_fn\", None)):\n",
    "        w = np.asarray(context[\"weight_fn\"](X_train, y_train, context))\n",
    "    else:\n",
    "        raise ValueError(\"mitigation_reweighing needs 'sample_weights' or 'weight_fn' in context.\")\n",
    "\n",
    "    target_param = context.get(\"weight_param\", \"classifier__sample_weight\")\n",
    "    return pipeline, X_train, y_train, {target_param: w}\n",
    "\n",
    "\n",
    "def mitigation_oversample(pipeline, X_train, y_train, context):\n",
    "    \"\"\"\n",
    "    Uses a provided oversampler (e.g., SMOTE) from context[\"oversampler\"].\n",
    "    \"\"\"\n",
    "    oversampler = context.get(\"oversampler\", None)\n",
    "    if oversampler is None:\n",
    "        # If SMOTE not available or not provided, leave data unchanged\n",
    "        return pipeline, X_train, y_train, {}\n",
    "    X_res, y_res = oversampler.fit_resample(X_train, y_train)\n",
    "    return pipeline, X_res, y_res, {}\n",
    "\n",
    "\n",
    "def mitigation_encoder_tweak(pipeline, X_train, y_train, context):\n",
    "    \"\"\"\n",
    "    Placeholder to swap/modify an encoder inside your pipeline if needed.\n",
    "    Example:\n",
    "      pipeline.set_params(preprocess__encoder=YourEncoder(**context.get(\"encoder_params\", {})))\n",
    "    \"\"\"\n",
    "    return pipeline, X_train, y_train, {}\n",
    "\n",
    "\n",
    "# ---------- A simple, sensible reweighting function (group × label inverse prevalence) ----------\n",
    "def inverse_prevalence_weights(X, y, ctx):\n",
    "    \"\"\"\n",
    "    Compute weights ~ 1 / P(y, s) using ctx[\"s_train\"] (sensitive attribute, aligned with y).\n",
    "    \"\"\"\n",
    "    s = np.asarray(ctx.get(\"s_train\", None))\n",
    "    if s is None:\n",
    "        raise ValueError(\"inverse_prevalence_weights requires 's_train' in context.\")\n",
    "    y = np.asarray(y)\n",
    "    pairs, counts = np.unique(np.column_stack([y, s]), axis=0, return_counts=True)\n",
    "    freq = {tuple(k): v for k, v in zip(map(tuple, pairs), counts)}\n",
    "    total = len(y)\n",
    "    w = np.empty_like(y, dtype=float)\n",
    "    for i, (yy, ss) in enumerate(zip(y, s)):\n",
    "        p = freq[(yy, ss)] / total\n",
    "        w[i] = 1.0 / max(p, 1e-12)\n",
    "    # Normalize to mean 1.0 (optional)\n",
    "    w /= w.mean()\n",
    "    return w\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PRODUCE THE NEW PREDICTIONS\n",
    "# =========================\n",
    "# Assumptions: you already have:\n",
    "#   pipeline_1, pipeline_2, pipeline_3  (sklearn Pipelines/estimators)\n",
    "#   X_train, y_train, X_test, y_test     (arrays/Series)\n",
    "#   s_train                               (sensitive attribute aligned with y_train)\n",
    "#\n",
    "# If you don't want oversampling, just remove the mitigation_oversample step below.\n",
    "\n",
    "mitigations_cfg = [\n",
    "    {\"name\": \"Reweighing\", \"fn\": mitigation_reweighing,\n",
    "     \"params\": {\n",
    "         \"weight_param\": \"classifier__sample_weight\",   # adjust to your pipeline\n",
    "         \"weight_fn\": inverse_prevalence_weights\n",
    "     }},\n",
    "    {\"name\": \"Oversample\", \"fn\": mitigation_oversample,\n",
    "     \"params\": {\"oversampler\": SMOTE(random_state=42) if SMOTE is not None else None}},\n",
    "    {\"name\": \"Encoder tweak\", \"fn\": mitigation_encoder_tweak, \"params\": {}},\n",
    "]\n",
    "\n",
    "common_context = {\"s_train\": s_train, \"seed\": 42}\n",
    "\n",
    "res_1 = run_mitigations_and_predict(\n",
    "    pipeline=pipeline_1,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,  y_test=y_test,\n",
    "    mitigations=mitigations_cfg,\n",
    "    base_fit_params={},\n",
    "    context=common_context\n",
    ")\n",
    "y_pred_1n = res_1[\"y_pred_post\"]\n",
    "\n",
    "res_2 = run_mitigations_and_predict(\n",
    "    pipeline=pipeline_2,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,  y_test=y_test,\n",
    "    mitigations=mitigations_cfg,\n",
    "    base_fit_params={},\n",
    "    context=common_context\n",
    ")\n",
    "y_pred_2n = res_2[\"y_pred_post\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ad41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional, Sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers for stage outputs\n",
    "# =========================\n",
    "def _truncate_pipeline(pipeline, stage_name: str):\n",
    "    \"\"\"\n",
    "    Return a shallow-cloned pipeline that runs up to and including `stage_name`.\n",
    "    Special names:\n",
    "      - \"__raw__\": no transform, just return raw X\n",
    "      - \"__end__\": full pipeline\n",
    "    \"\"\"\n",
    "    if stage_name == \"__raw__\":\n",
    "        return None\n",
    "    if stage_name == \"__end__\":\n",
    "        return clone(pipeline)\n",
    "    steps = []\n",
    "    for name, est in pipeline.steps:\n",
    "        steps.append((name, est))\n",
    "        if name == stage_name:\n",
    "            break\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    return Pipeline(steps=steps)\n",
    "\n",
    "def _fit_transform_at_stage(pipeline, X: pd.DataFrame, stage_name: str) -> pd.DataFrame:\n",
    "    if stage_name == \"__raw__\":\n",
    "        return _ensure_df(X)\n",
    "    tp = _truncate_pipeline(pipeline, stage_name)\n",
    "    if tp is None:\n",
    "        return _ensure_df(X)\n",
    "    try:\n",
    "        tp.set_output(transform=\"pandas\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    Xt = tp.fit_transform(X)\n",
    "    return _ensure_df(Xt)\n",
    "\n",
    "def _ensure_df(X) -> pd.DataFrame:\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        X = X.toarray()\n",
    "    if isinstance(X, np.ndarray):\n",
    "        return pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "    return pd.DataFrame(X)\n",
    "\n",
    "def _expand_attributes_in_transformed(X_after: pd.DataFrame, attributes: Sequence[str]) -> List[str]:\n",
    "    \"\"\"Map requested raw attributes to transformed columns (handles one-hot by substring match).\"\"\"\n",
    "    cols = []\n",
    "    for a in attributes:\n",
    "        a_low = str(a).lower()\n",
    "        for c in X_after.columns:\n",
    "            if a_low in str(c).lower():\n",
    "                cols.append(c)\n",
    "    if not cols:\n",
    "        cols = [c for c in X_after.columns if c in attributes]\n",
    "    # dedupe\n",
    "    seen, out = set(), []\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            out.append(c); seen.add(c)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 1) Correlations before vs after any stage\n",
    "# ===========================================\n",
    "def compare_attribute_correlations(\n",
    "    pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    attributes: Sequence[str],\n",
    "    stage_name: str,\n",
    "    method: str = \"pearson\",\n",
    "    figsize: Tuple[int, int] = (10, 4)\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute & display correlation matrices for selected `attributes`\n",
    "    BEFORE vs AFTER a given preprocessing `stage_name`.\n",
    "    stage_name: \"__raw__\" | any pipeline step name | \"__end__\"\n",
    "    \"\"\"\n",
    "    X_before = _ensure_df(X)\n",
    "    X_after  = _fit_transform_at_stage(pipeline, X, stage_name)\n",
    "\n",
    "    before_cols = [c for c in X_before.columns if c in attributes]\n",
    "    after_cols  = _expand_attributes_in_transformed(X_after, attributes)\n",
    "\n",
    "    corr_before = _ensure_df(X_before[before_cols]).corr(method=method) if before_cols else pd.DataFrame()\n",
    "    corr_after  = _ensure_df(X_after[after_cols]).corr(method=method)   if after_cols  else pd.DataFrame()\n",
    "\n",
    "    if not corr_before.empty:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(corr_before, interpolation=\"nearest\")\n",
    "        plt.xticks(range(len(corr_before.columns)), corr_before.columns, rotation=90)\n",
    "        plt.yticks(range(len(corr_before.index)), corr_before.index)\n",
    "        plt.title(f\"Correlation BEFORE stage '{stage_name}' ({method})\")\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if not corr_after.empty:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(corr_after, interpolation=\"nearest\")\n",
    "        plt.xticks(range(len(corr_after.columns)), corr_after.columns, rotation=90)\n",
    "        plt.yticks(range(len(corr_after.index)), corr_after.index)\n",
    "        plt.title(f\"Correlation AFTER stage '{stage_name}' ({method})\")\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\"before\": corr_before, \"after\": corr_after}\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 2) Feature distribution before vs after any stage\n",
    "# ==================================================\n",
    "def compare_feature_distribution(\n",
    "    pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    feature: str,\n",
    "    stage_name: str,\n",
    "    bins: int = 20,\n",
    "    top_k_categories: int = 25,\n",
    "    figsize: Tuple[int, int] = (8, 4)\n",
    ") -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Display the distribution of a single `feature` BEFORE vs AFTER `stage_name`.\n",
    "    Numeric => histogram; Categorical => top-k bar counts.\n",
    "    If transformed into multiple one-hot columns, aggregate those columns into a categorical count vector.\n",
    "    \"\"\"\n",
    "    X_before = _ensure_df(X).copy()\n",
    "    X_after  = _fit_transform_at_stage(pipeline, X, stage_name)\n",
    "\n",
    "    # BEFORE\n",
    "    if feature in X_before.columns:\n",
    "        series_before = X_before[feature]\n",
    "    else:\n",
    "        matches = [c for c in X_before.columns if c.lower() == feature.lower()]\n",
    "        series_before = X_before[matches[0]] if matches else pd.Series(dtype=float, name=feature)\n",
    "\n",
    "    # AFTER (map to columns)\n",
    "    after_cols = _expand_attributes_in_transformed(X_after, [feature])\n",
    "\n",
    "    if len(after_cols) == 0:\n",
    "        series_after = pd.Series(dtype=float, name=feature)\n",
    "    elif len(after_cols) == 1 and after_cols[0] in X_after.columns:\n",
    "        series_after = X_after[after_cols[0]]\n",
    "    else:\n",
    "        sub = X_after[after_cols]\n",
    "        cat_names = [c.replace(f\"{feature}_\", \"\") for c in sub.columns]\n",
    "        counts = sub.apply(pd.Series.sum, axis=0)\n",
    "        counts.index = cat_names\n",
    "        series_after = counts\n",
    "\n",
    "    # Plot BEFORE\n",
    "    if pd.api.types.is_numeric_dtype(series_before):\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.hist(series_before.dropna(), bins=bins)\n",
    "        plt.title(f\"Distribution BEFORE stage '{stage_name}' — {feature}\")\n",
    "        plt.xlabel(feature); plt.ylabel(\"count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "        dist_before = series_before\n",
    "    else:\n",
    "        vc = series_before.astype(\"object\").fillna(\"<NA>\").value_counts().sort_values(ascending=False)\n",
    "        vc = vc.head(top_k_categories)\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.bar(vc.index.astype(str), vc.values)\n",
    "        plt.title(f\"Distribution BEFORE stage '{stage_name}' — {feature}\")\n",
    "        plt.xticks(rotation=90); plt.ylabel(\"count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "        dist_before = vc\n",
    "\n",
    "    # Plot AFTER\n",
    "    if isinstance(series_after, pd.Series) and pd.api.types.is_numeric_dtype(series_after) and series_after.shape[0] == X_after.shape[0]:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.hist(series_after.dropna(), bins=bins)\n",
    "        plt.title(f\"Distribution AFTER stage '{stage_name}' — {feature}\")\n",
    "        plt.xlabel(feature); plt.ylabel(\"count\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "        dist_after = series_after\n",
    "    else:\n",
    "        if isinstance(series_after, pd.Series) and series_after.index.size > 0:\n",
    "            counts = series_after.sort_values(ascending=False).head(top_k_categories)\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.bar(counts.index.astype(str), counts.values)\n",
    "            plt.title(f\"Distribution AFTER stage '{stage_name}' — {feature} (aggregated)\")\n",
    "            plt.xticks(rotation=90); plt.ylabel(\"count\")\n",
    "            plt.tight_layout(); plt.show()\n",
    "            dist_after = counts\n",
    "        else:\n",
    "            dist_after = pd.Series(dtype=float, name=feature)\n",
    "\n",
    "    return {\"before\": dist_before, \"after\": dist_after}\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3) Threshold guard that calls YOUR mitigation function\n",
    "#    - Uses your Fairness_Metrics_Computation(...)\n",
    "#    - Calls your run_mitigations_and_predict(...) if violated\n",
    "# ==========================================================\n",
    "def _check_thresholds(metrics: Dict[str, float], thresholds: Dict[str, Tuple]) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if ANY threshold is violated.\n",
    "    thresholds[k] = (op, val) or (\"between\", lo, hi)\n",
    "    \"\"\"\n",
    "    def violated_one(metric, rule):\n",
    "        op = rule[0]\n",
    "        vals = rule[1:]\n",
    "        m = metrics.get(metric, None)\n",
    "        if m is None:\n",
    "            return False\n",
    "        if op == \">=\":   return not (m >= vals[0])\n",
    "        if op == \"<=\":   return not (m <= vals[0])\n",
    "        if op == \">\":    return not (m >  vals[0])\n",
    "        if op == \"<\":    return not (m <  vals[0])\n",
    "        if op == \"between\":\n",
    "            lo, hi = vals\n",
    "            return not (lo <= m <= hi)\n",
    "        raise ValueError(f\"Unknown threshold op: {op}\")\n",
    "    return any(violated_one(k, v) for k, v in thresholds.items())\n",
    "\n",
    "def guard_and_mitigate_if_needed(\n",
    "    pipeline,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    sensitive_attr_test: np.ndarray,\n",
    "    thresholds: Dict[str, Tuple],\n",
    "    # ex:\n",
    "    # thresholds = {\n",
    "    #   \"accuracy\": (\">=\", 0.85),\n",
    "    #   \"disparate_impact_ratio\": (\">=\", 0.8),\n",
    "    #   \"demographic_parity_diff\": (\"between\", -0.1, 0.1)\n",
    "    # }\n",
    "    mitigations_cfg: List[Dict[str, Any]],\n",
    "    context: Dict[str, Any],\n",
    "    base_fit_params: Optional[Dict[str, Any]] = None,\n",
    "    fairness_compute_type: str = \"global\",\n",
    "    y2_scores: Optional[np.ndarray] = None,\n",
    "    positive_label=1\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Fit the baseline pipeline and compute metrics\n",
    "       - Accuracy via sklearn\n",
    "       - Fairness via your Fairness_Metrics_Computation(y1, y2, y, attribute, compute_type)\n",
    "         (y1 = y_pred, y2 = y2_scores or None, y = y_test, attribute = sensitive_attr_test)\n",
    "    2) If ANY threshold is violated, call your run_mitigations_and_predict(...)\n",
    "    3) Return both pre/post metrics and final predictions\n",
    "    \"\"\"\n",
    "    base_fit_params = dict(base_fit_params or {})\n",
    "    model = clone(pipeline)\n",
    "    model.fit(X_train, y_train, **base_fit_params)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # --- metrics (accuracy + fairness) ---\n",
    "    metrics = {\"accuracy\": float(accuracy_score(y_test, y_pred))}\n",
    "    fairness_dict = Fairness_Metrics_Computation(\n",
    "        y1=y_pred, y2=y2_scores, y=y_test, attribute=sensitive_attr_test, compute_type=fairness_compute_type\n",
    "    )\n",
    "    # Merge fairness metrics into metrics dict\n",
    "    metrics.update({k: float(v) for k, v in fairness_dict.items()})\n",
    "\n",
    "    violated = _check_thresholds(metrics, thresholds)\n",
    "\n",
    "    result = {\n",
    "        \"violated\": violated,\n",
    "        \"metrics_pre\": metrics,\n",
    "        \"y_pred_pre\": y_pred,\n",
    "        \"pipeline_pre\": model\n",
    "    }\n",
    "\n",
    "    if not violated:\n",
    "        result.update({\n",
    "            \"used_post_mitigation\": False,\n",
    "            \"y_pred_final\": y_pred,\n",
    "            \"metrics_final\": metrics,\n",
    "            \"pipeline_final\": model\n",
    "        })\n",
    "        return result\n",
    "\n",
    "    # --- run your mitigation routine (already defined in your codebase) ---\n",
    "    mit_res = run_mitigations_and_predict(\n",
    "        pipeline=model,\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test,  y_test=y_test,\n",
    "        mitigations=mitigations_cfg,\n",
    "        base_fit_params=base_fit_params,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # recompute fairness on post-mitigation predictions for apples-to-apples\n",
    "    y_pred_post = mit_res[\"y_pred_post\"]\n",
    "    metrics_post = {\"accuracy\": float(accuracy_score(y_test, y_pred_post))}\n",
    "    fairness_post = Fairness_Metrics_Computation(\n",
    "        y1=y_pred_post, y2=y2_scores, y=y_test, attribute=sensitive_attr_test, compute_type=fairness_compute_type\n",
    "    )\n",
    "    metrics_post.update({k: float(v) for k, v in fairness_post.items()})\n",
    "\n",
    "    result.update({\n",
    "        \"used_post_mitigation\": True,\n",
    "        \"mitigation_detail\": mit_res.get(\"applied_mitigations\", []),\n",
    "        \"y_pred_post\": y_pred_post,\n",
    "        \"metrics_post\": metrics_post,\n",
    "        \"y_pred_final\": y_pred_post,\n",
    "        \"metrics_final\": metrics_post,\n",
    "        \"pipeline_final\": mit_res[\"pipeline_final\"],\n",
    "    })\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_attribute_correlations(\n",
    "    pipeline=pipeline_1,\n",
    "    X=X_train,\n",
    "    attributes=[\"sex\"],\n",
    "    stage_name=\"preprocess\"    # or \"__raw__\", \"__end__\", or any step name in your pipeline\n",
    ")\n",
    "\n",
    "\n",
    "compare_feature_distribution(\n",
    "    pipeline=pipeline_2,\n",
    "    X=X_train,\n",
    "    feature=\"education\",\n",
    "    stage_name=\"preprocess\"\n",
    ")\n",
    "\n",
    "thresholds = {\n",
    "    \"accuracy\": (\">=\", 0.85),\n",
    "    \"disparate_impact_ratio\": (\">=\", 0.8),\n",
    "    \"demographic_parity_diff\": (\"between\", -0.1, 0.1),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad7b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0befb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ce102",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1n= accuracy_score(y_test, y_pred_1n)\n",
    "A2n= accuracy_score(y_test, y_pred_2n)\n",
    "f1n = f1_score(y_test, y_pred_1n)\n",
    "f2n=f1_score(y_test, y_pred_2n)\n",
    "A1, A1n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf268e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_biases(mispred_p1, mispred_p2, mispred_baseline):\n",
    "    \"\"\"\n",
    "    Calculate individual biases and total bias.\n",
    "    \"\"\"\n",
    "    # Calculating individual biases\n",
    "    BD = mispred_baseline\n",
    "    BStage = mispred_p1 - mispred_p2  # Preprocessing stage bias\n",
    "    BC = mispred_p2 - mispred_baseline  # Classifier bias\n",
    "\n",
    "    # Calculating Stage-Classifier Interaction term\n",
    "    Stage_Classifier_Interaction = mispred_p1 - (BD + BStage + BC)\n",
    "\n",
    "    # Total bias calculation\n",
    "    TB = BD + BStage + BC + Stage_Classifier_Interaction\n",
    "\n",
    "    return {\n",
    "        'BD': BD,\n",
    "        'BC': BC,\n",
    "        'BStage': BStage,\n",
    "        'Stage_Classifier_Interaction': Stage_Classifier_Interaction,\n",
    "        'TB': TB,\n",
    "        'TB matches mispred_p1': TB == mispred_p1\n",
    "    }\n",
    "\n",
    "def plot_biases(biases, mispred_p1):\n",
    "    \"\"\"\n",
    "    Plot the biases and observed mispredictions.\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        'Data (BD)', \n",
    "         \n",
    "        'Preprocessing Stage (BStage)', \n",
    "        'Stage-Classifier Interaction', \n",
    "        'Classifier (BC)',\n",
    "        'Total Bias (TB)', \n",
    "        'Observed Bias (mispred_p1)'\n",
    "    ]\n",
    "    \n",
    "    values = [\n",
    "        biases['BD'], \n",
    "         \n",
    "        biases['BStage'], \n",
    "        biases['Stage_Classifier_Interaction'], \n",
    "        biases['BC'],\n",
    "        biases['TB'], \n",
    "        mispred_p1\n",
    "    ]\n",
    "\n",
    "    colors = ['blue', 'orange', 'green', 'red', 'pink', 'gray']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(labels, values, color=colors)\n",
    "    plt.xlabel('Bias / Mispredictions')\n",
    "    plt.title('Bias Analysis and Observed Mispredictions - After Mitigation  ')\n",
    "\n",
    "    # Adding annotations\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        label_x_pos = width if width < 0 else width\n",
    "        plt.text(\n",
    "            label_x_pos, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{width:.2f}', ha='center', va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.7)\n",
    "        )\n",
    "\n",
    "    plt.axvline(0, color='black', linewidth=0.8)\n",
    "    plt.xlim(min(values) - 10, max(values) + 10)  # Adding some padding to the x-axis\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Baseline model for comparison\n",
    "\n",
    "\n",
    "# Calculate mispredictions\n",
    "aligned_indices = X_test.notna().all(axis=1)\n",
    "\n",
    "mispred_p1n = sum(y_test != y_pred_1n)\n",
    "mispred_p2n = sum(y_test != y_pred_2n)\n",
    "# mispred_baseline = sum(y_test != y_pred_baseline)\n",
    "\n",
    "# Calculate biases\n",
    "biases = calculate_biases(mispred_p1n, mispred_p2n, mispred_baseline)\n",
    "\n",
    "# Plot the biases\n",
    "plot_biases(biases, mispred_p1n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming y_pred_1, y_pred_2, y_pred_baseline, and y_test are already defined\n",
    "# Convert y_test and predictions to numpy arrays to avoid indexing issues\n",
    "y_test = np.array(y_test)\n",
    "y_pred_1 = np.array(y_pred_1n)\n",
    "y_pred_2 = np.array(y_pred_2n)\n",
    "# y_pred_baseline = np.array(y_pred_baseline)\n",
    "\n",
    "# Initialize volatility arrays\n",
    "volatility_LE = np.zeros(len(y_test))\n",
    "volatility_classifier = np.zeros(len(y_test))\n",
    "\n",
    "# Calculate Volatility Scores for LE (Vol_St1)\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1[i] == y_test[i] and y_pred_2[i] == y_test[i]:\n",
    "        volatility_LE[i] = 0\n",
    "    elif y_pred_1[i] == y_test[i] and y_pred_2[i] != y_test[i]:\n",
    "        volatility_LE[i] = 1\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_2[i] == y_test[i]:\n",
    "        volatility_LE[i] = 2\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_2[i] != y_test[i] and y_pred_1[i] == y_pred_2[i]:\n",
    "        volatility_LE[i] = 3\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_2[i] != y_test[i] and y_pred_1[i] != y_pred_2[i]:\n",
    "        volatility_LE[i] = 4\n",
    "\n",
    "# Volatility score calculation for Classifier (Vol_Cl)\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_1[i] == y_test[i] and y_pred_baseline[i] == y_test[i]:\n",
    "        volatility_classifier[i] = 0\n",
    "    elif y_pred_1[i] == y_test[i] and y_pred_baseline[i] != y_test[i]:\n",
    "        volatility_classifier[i] = 1\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_baseline[i] == y_test[i]:\n",
    "        volatility_classifier[i] = 2\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_baseline[i] != y_test[i] and y_pred_1[i] == y_pred_baseline[i]:\n",
    "        volatility_classifier[i] = 3\n",
    "    elif y_pred_1[i] != y_test[i] and y_pred_baseline[i] != y_test[i] and y_pred_1[i] != y_pred_baseline[i]:\n",
    "        volatility_classifier[i] = 4\n",
    "\n",
    "# Summarize volatility scores\n",
    "volatility_summary = {\n",
    "    'Condition': ['Vol_St1=0', 'Vol_St1=1', 'Vol_St1=2', 'Vol_St1=3', 'Vol_St1=4',\n",
    "                  'Vol_Cl=0', 'Vol_Cl=1', 'Vol_Cl=2', 'Vol_Cl=3', 'Vol_Cl=4'],\n",
    "    'Count': [np.sum(volatility_LE == 0), np.sum(volatility_LE == 1), np.sum(volatility_LE == 2), np.sum(volatility_LE == 3), np.sum(volatility_LE == 4),\n",
    "              np.sum(volatility_classifier == 0), np.sum(volatility_classifier == 1), np.sum(volatility_classifier == 2), np.sum(volatility_classifier == 3), np.sum(volatility_classifier == 4)]\n",
    "}\n",
    "\n",
    "df_volatility = pd.DataFrame(volatility_summary)\n",
    "\n",
    "# Plot the volatility scores\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x='Condition', y='Count', data=df_volatility, palette='viridis')\n",
    "\n",
    "plt.title('Volatility Score Analysis  After Mitigation')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add labels to each bar\n",
    "for bar in ax.patches:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height(),\n",
    "            f'{int(bar.get_height())}',\n",
    "            ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Function to calculate accuracy and F1 score\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, f1\n",
    "\n",
    "# Example computed metrics for illustration\n",
    "accuracy_1= A1\n",
    "accuracy_2 = np.abs(A1- A2)\n",
    "\n",
    "accuracy_1n = A1n \n",
    "accuracy_2n = np.abs(A1n-A2n)  # After mitigation accuracies\n",
    "f1_1 = f1\n",
    "f1_2 = np.abs(f1-f2)\n",
    "f1_1n= f1n\n",
    "f1_2n =np.abs(f1n-f2n)\n",
    "# Function to plot the accuracy and F1 scores for pipelines and components before and after mitigation\n",
    "def plot_metrics():\n",
    "    labels = ['Pipeline', 'RW Component']  # Update this if needed for component names\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    before_mitigation_accuracy = [accuracy_1, accuracy_2]\n",
    "    after_mitigation_accuracy = [accuracy_1n, accuracy_2n]\n",
    "    \n",
    "    bar_width = 0.35  # Width of the bars\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    plt.bar(x - bar_width / 2, before_mitigation_accuracy, width=bar_width, label='Before Mitigation', color='blue')\n",
    "    plt.bar(x + bar_width / 2, after_mitigation_accuracy, width=bar_width, label='After Mitigation', color='green')\n",
    "    \n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Before vs After Mitigation')\n",
    "    plt.legend()\n",
    "\n",
    "    # F1 Score plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    before_mitigation_f1 = [f1_1, f1_2]\n",
    "    after_mitigation_f1 = [f1_1n, f1_2n]\n",
    "    \n",
    "    plt.bar(x - bar_width / 2, before_mitigation_f1, width=bar_width, label='Before Mitigation', color='blue')\n",
    "    plt.bar(x + bar_width / 2, after_mitigation_f1, width=bar_width, label='After Mitigation', color='green')\n",
    "    \n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score Before vs After Mitigation')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed160d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'C:/Users/Saadia/FairPreprocessing/data/bank/bank-additional-full.csv'\n",
    "na_values = ['unknown', 'nan', '']  # More clearly defined missing value indicators\n",
    "df = pd.read_csv(file_path, sep=';', na_values=na_values)\n",
    "na_values = ['unknown']\n",
    "df = pd.read_csv(file_path, sep=';', na_values=na_values)\n",
    "df['age_mapped'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
    "# Apply custom transformations\n",
    "df['age'] = df['age'].apply(lambda x: float(x >= 25))  # Transforming 'age' into a binary feature\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "# Convert 'y' labels to numeric\n",
    "y = y.map({'no': 0, 'yes': 1})\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "# categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "# numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Define a column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), numerical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Manually compute sample weights for handling class imbalance\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# Define a pipeline incorporating Random Forest with class weight balancing\n",
    "pipeline_1 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_estimators=150, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Define a second pipeline without class weight balancing\n",
    "pipeline_2 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))\n",
    "])\n",
    "\n",
    "def compute_propensity_scores(X, treatment_column):\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Define the treatment variable\n",
    "    treatment = X_imputed[treatment_column].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    # Check if treatment has more than one class\n",
    "    if treatment.nunique() < 2:\n",
    "        raise ValueError(f\"The treatment column '{treatment_column}' contains only one class: {treatment.unique()}. Propensity score calculation requires at least two classes.\")\n",
    "    \n",
    "    # Drop the treatment column to create the covariate matrix\n",
    "    X_covariates = X_imputed.drop(columns=[treatment_column])\n",
    "\n",
    "    # One-hot encode the covariate matrix\n",
    "    X_encoded = pd.get_dummies(X_covariates, drop_first=True)\n",
    "\n",
    "    # Estimate propensity scores using logistic regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_encoded, treatment)\n",
    "    propensity_scores = lr.predict_proba(X_encoded)[:, 1]\n",
    "\n",
    "    return propensity_scores, treatment\n",
    "\n",
    "def perform_matching(propensity_scores, treatment):\n",
    "    # Define treated and control indices based on the treatment variable\n",
    "    treated_indices = np.where((treatment == 1))[0] \n",
    "    control_indices = np.where((treatment == 0))[0]  \n",
    "\n",
    "    # Fit the nearest neighbors model on control propensity scores\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(propensity_scores[control_indices].reshape(-1, 1))\n",
    "\n",
    "    # Find the nearest neighbors for treated units\n",
    "    distances, indices = nn.kneighbors(propensity_scores[treated_indices].reshape(-1, 1))\n",
    "\n",
    "    # Map control indices to matched treated indices\n",
    "    matched_control_indices = control_indices[indices.flatten()]\n",
    "\n",
    "    # Combine treated and matched control indices\n",
    "    matched_indices = np.concatenate([treated_indices, matched_control_indices])\n",
    "\n",
    "    # Ensure indices are unique and within bounds\n",
    "    matched_indices = np.unique(matched_indices)\n",
    "    matched_indices = matched_indices[matched_indices < len(treatment)]\n",
    "\n",
    "    return matched_indices\n",
    "\n",
    "\n",
    "# Paths to train and test datasets\n",
    "# Initialize lists to store results\n",
    "spd_list = []\n",
    "eod_list = []\n",
    "aod_list = []\n",
    "erd_list = []\n",
    "\n",
    "spd_cas_list = []\n",
    "eod_cas_list = []\n",
    "aod_cas_list = []\n",
    "erd_cas_list = []\n",
    "\n",
    "spd_cf_list = []\n",
    "eod_cf_list = []\n",
    "aod_cf_list = []\n",
    "erd_cf_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "    X_train_m = X_train.copy()\n",
    "    X_test_m = X_test.copy()\n",
    "    y_train_m = y_train.copy()\n",
    "    y_test_m = y_test.copy()\n",
    "\n",
    "    # Fit and predict using pipelines\n",
    "    pipeline_1.fit(X_train, y_train)\n",
    "    y_pred_1 = pipeline_1.predict(X_test)\n",
    "\n",
    "    pipeline_2.fit(X_train, y_train)\n",
    "    y_pred_2 = pipeline_2.predict(X_test)\n",
    "\n",
    "    # Fairness Metrics Computation Placeholder\n",
    "    SPD_mv, EOD_mv, AOD_mv, ERD_mv = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['age_mapped'])\n",
    "\n",
    "    spd_list.append(SPD_mv)\n",
    "    eod_list.append(EOD_mv)\n",
    "    aod_list.append(AOD_mv)\n",
    "    erd_list.append(ERD_mv)\n",
    "\n",
    "    # Counterfactual analysis: flip the age_mapped feature\n",
    "    X_test_flipped = X_test.copy()\n",
    "    X_test_flipped['age_mapped'] = flip_race(X_test['age_mapped'])\n",
    "\n",
    "    y_pred_cf_1 = pipeline_1.predict(X_test_flipped)\n",
    "    y_pred_cf_2 = pipeline_2.predict(X_test_flipped)\n",
    "\n",
    "    # Compute counterfactual fairness metrics\n",
    "    SPD_cf_mv, EOD_cf_mv, AOD_cf_mv, ERD_cf_mv = Fairness_Metrics_Computation(y_pred_cf_1, y_pred_cf_2, y_test, X_test_flipped['age_mapped'])\n",
    "\n",
    "    spd_cf_list.append(SPD_cf_mv)\n",
    "    eod_cf_list.append(EOD_cf_mv)\n",
    "    aod_cf_list.append(AOD_cf_mv)\n",
    "    erd_cf_list.append(ERD_cf_mv)\n",
    "\n",
    "    # Propensity score computation and matching\n",
    "    try:\n",
    "        propensity_scores1, treatment1 = compute_propensity_scores(X_test_m, 'age_mapped')\n",
    "        matched_indices1 = perform_matching(propensity_scores1, treatment1)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue  # Skip this iteration if the treatment column contains only one class\n",
    "\n",
    "    # Propensity score computation and matching for Pipeline 2\n",
    "    try:\n",
    "        propensity_scores2, treatment2 = compute_propensity_scores(X_test_m, 'age_mapped')\n",
    "        matched_indices2 = perform_matching(propensity_scores2, treatment2)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue  # Skip this iteration if the treatment column contains only one class\n",
    "\n",
    "    # Find common matched indices across all pipelines\n",
    "    common_matched_indices = set(matched_indices1).intersection(matched_indices2)\n",
    "    common_matched_indices = list(common_matched_indices)\n",
    "\n",
    "    # Ensure the indices are sorted to maintain order\n",
    "    common_matched_indices.sort()\n",
    "\n",
    "    # Applying matching to y_pred results and the actual y_test and age_mapped\n",
    "    y1_matched = y_pred_1[common_matched_indices]\n",
    "    y2_matched = y_pred_2[common_matched_indices]\n",
    "    y_test_matched = y_test_m.reset_index(drop=True)[common_matched_indices]\n",
    "    age_mapped_matched = X_test['age_mapped'].reset_index(drop=True)[common_matched_indices]\n",
    "\n",
    "    SPD_cas_mv, EOD_cas_mv, AOD_cas_mv, ERD_cas_mv = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, age_mapped_matched)\n",
    "\n",
    "    spd_cas_list.append(SPD_cas_mv)\n",
    "    eod_cas_list.append(EOD_cas_mv)\n",
    "    aod_cas_list.append(AOD_cas_mv)\n",
    "    erd_cas_list.append(ERD_cas_mv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_array = np.array(spd_list)\n",
    "eod_array = np.array(eod_list)\n",
    "aod_array = np.array(aod_list)\n",
    "erd_array = np.array(erd_list)\n",
    "\n",
    "spd_cas_array = np.array(spd_cas_list)\n",
    "eod_cas_array = np.array(eod_cas_list)\n",
    "aod_cas_array = np.array(aod_cas_list)\n",
    "erd_cas_array = np.array(erd_cas_list)\n",
    "\n",
    "spd_cf_array = np.array(spd_cf_list)\n",
    "eod_cf_array = np.array(eod_cf_list)\n",
    "aod_cf_array = np.array(aod_cf_list)\n",
    "erd_cf_array = np.array(erd_cf_list)\n",
    "\n",
    "\n",
    "spd_mean = spd_array.mean(axis=0)\n",
    "eod_mean = eod_array.mean(axis=0)\n",
    "aod_mean = aod_array.mean(axis=0)\n",
    "erd_mean = erd_array.mean(axis=0)\n",
    "\n",
    "spd_cas_mean = spd_cas_array.mean(axis=0)\n",
    "eod_cas_mean = eod_cas_array.mean(axis=0)\n",
    "aod_cas_mean = aod_cas_array.mean(axis=0)\n",
    "erd_cas_mean = erd_cas_array.mean(axis=0)\n",
    "\n",
    "spd_cf_mean = spd_cf_array.mean(axis=0)\n",
    "eod_cf_mean = eod_cf_array.mean(axis=0)\n",
    "aod_cf_mean = aod_cf_array.mean(axis=0)\n",
    "erd_cf_mean = erd_cf_array.mean(axis=0)\n",
    "\n",
    "# Calculate the standard errors for each metric and stage\n",
    "spd_se = spd_array.std(axis=0) / np.sqrt(spd_array.shape[0])\n",
    "eod_se = eod_array.std(axis=0) / np.sqrt(eod_array.shape[0])\n",
    "aod_se = aod_array.std(axis=0) / np.sqrt(aod_array.shape[0])\n",
    "erd_se = erd_array.std(axis=0) / np.sqrt(erd_array.shape[0])\n",
    "\n",
    "spd_cas_se = spd_cas_array.std(axis=0) / np.sqrt(spd_cas_array.shape[0])\n",
    "eod_cas_se = eod_cas_array.std(axis=0) / np.sqrt(eod_cas_array.shape[0])\n",
    "aod_cas_se = aod_cas_array.std(axis=0) / np.sqrt(aod_cas_array.shape[0])\n",
    "erd_cas_se = erd_cas_array.std(axis=0) / np.sqrt(erd_cas_array.shape[0])\n",
    "\n",
    "spd_cf_se = spd_cf_array.std(axis=0) / np.sqrt(spd_cf_array.shape[0])\n",
    "eod_cf_se = eod_cf_array.std(axis=0) / np.sqrt(eod_cf_array.shape[0])\n",
    "aod_cf_se = aod_cf_array.std(axis=0) / np.sqrt(aod_cf_array.shape[0])\n",
    "erd_cf_se = erd_cf_array.std(axis=0) / np.sqrt(erd_cf_array.shape[0])\n",
    "\n",
    "# Plotting function\n",
    "def plot_with_error_bars(ax, means, std_errors, title, metric_labels, colors, labels):\n",
    "    bar_width = 0.25\n",
    "    indices = np.arange(len(metric_labels))  # Fix this to use metric_labels length\n",
    "    \n",
    "    for i, (mean, std_error, color, label) in enumerate(zip(means, std_errors, colors, labels)):\n",
    "        ax.bar(indices + i * bar_width, mean, yerr=std_error, capsize=5, width=bar_width, align='center', color=color, label=label)\n",
    "    \n",
    "    ax.set_xticks(indices + bar_width)\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.set_xlabel('Fairness Metrics')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Metric labels\n",
    "metric_labels = ['SPD', 'EOD', 'AOD', 'ERD']\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 1, figsize=(18, 6))\n",
    "\n",
    "# Colors and labels for the bars\n",
    "colors = ['blue', 'orange', 'green']\n",
    "labels = ['Statistical', 'Causal', 'Counterfactual']\n",
    "\n",
    "# Prepare means and standard errors for plotting\n",
    "means_mv = [spd_mean, eod_mean, aod_mean, erd_mean]\n",
    "std_errors_mv = [spd_se, eod_se, aod_se, erd_se]\n",
    "means_cas_mv = [spd_cas_mean, eod_cas_mean, aod_cas_mean, erd_cas_mean]\n",
    "std_errors_cas_mv = [spd_cas_se, eod_cas_se, aod_cas_se, erd_cas_se]\n",
    "means_cf_mv = [spd_cf_mean, eod_cf_mean, aod_cf_mean, erd_cf_mean]\n",
    "std_errors_cf_mv = [spd_cf_se, eod_cf_se, aod_cf_se, erd_cf_se]\n",
    "\n",
    "# Plot for MV\n",
    "plot_with_error_bars(axs, [means_mv, means_cas_mv, means_cf_mv], [std_errors_mv, std_errors_cas_mv, std_errors_cf_mv], 'RW Impact', metric_labels, colors, labels)\n",
    "\n",
    "# Prepare means and standard errors for plotting FE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0884154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fairness_Metrics_Computation(y1, y2, y, attribute, compute_type='global'):\n",
    "    \"\"\"\n",
    "    Compute local or global fairness metrics: SPD, EOD, AOD, ERD\n",
    "    y1: Predictions from original pipeline (Y(P))\n",
    "    y2: Predictions from modified pipeline (Y(P*))\n",
    "    y: Ground truth labels\n",
    "    attribute: Sensitive attribute (race)\n",
    "    compute_type: 'local' for local metrics or 'global' for global metrics\n",
    "    \"\"\"\n",
    "    # Convert inputs to pandas Series for easier handling\n",
    "    y1 = pd.Series(y1).reset_index(drop=True)\n",
    "    y2 = pd.Series(y2).reset_index(drop=True)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "    attribute = pd.Series(attribute).reset_index(drop=True)\n",
    "\n",
    "    # Identify privileged and unprivileged groups\n",
    "    privileged = (attribute == 'male') | (attribute == 1)\n",
    "    unprivileged = ~privileged\n",
    "\n",
    "    if compute_type == 'local':\n",
    "        # Local calculation: Only consider instances where y1 != y2\n",
    "        local_indices = (y1 != y2).reset_index(drop=True)\n",
    "        y1 = y1[local_indices]\n",
    "        y2 = y2[local_indices]\n",
    "        y = y[local_indices]\n",
    "        attribute = attribute[local_indices]\n",
    "        privileged = privileged[local_indices]\n",
    "        unprivileged = unprivileged[local_indices]\n",
    "\n",
    "    # Counts of privileged and unprivileged groups\n",
    "    count_privileged = np.sum(privileged)\n",
    "    count_unprivileged = np.sum(unprivileged)\n",
    "\n",
    "    # SPD Calculation\n",
    "    SFC_SPD_P = np.sum([(1 if y1_i == 1 and y2_i == 0 else -1 if y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y1_i, y2_i, attr in zip(y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_SPD_NP = np.sum([(1 if y1_i == 1 and y2_i == 0 else -1 if y1_i == 0 and y2_i == 1 else 0)\n",
    "                         for y1_i, y2_i, attr in zip(y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "\n",
    "    SFR_SPD_p = SFC_SPD_P / count_privileged if count_privileged > 0 else 0\n",
    "    SFR_SPD_np = SFC_SPD_NP / count_unprivileged if count_unprivileged > 0 else 0\n",
    "    SPD = SFR_SPD_np - SFR_SPD_p\n",
    "\n",
    "    # EOD Calculation\n",
    "    P_Y1 = np.sum((y == 1) & privileged)\n",
    "    NP_Y1 = np.sum((y == 1) & unprivileged)\n",
    "\n",
    "    SFC_EOD_P = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_EOD_NP = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                         for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "\n",
    "    SFR_EOD_p = SFC_EOD_P / P_Y1 if P_Y1 > 0 else 0\n",
    "    SFR_EOD_np = SFC_EOD_NP / NP_Y1 if NP_Y1 > 0 else 0\n",
    "    EOD = SFR_EOD_np - SFR_EOD_p\n",
    "\n",
    "    # AOD Calculation\n",
    "    P_Y0 = np.sum((y == 0) & privileged)\n",
    "    NP_Y0 = np.sum((y == 0) & unprivileged)\n",
    "\n",
    "    SFC_TP_P = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_TP_NP = np.sum([(1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "    SFC_FP_P = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_FP_NP = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "\n",
    "    SFR_AOD_p = (SFC_TP_P / P_Y1 if P_Y1 > 0 else 0) + (SFC_FP_P / P_Y0 if P_Y0 > 0 else 0)\n",
    "    SFR_AOD_np = (SFC_TP_NP / NP_Y1 if NP_Y1 > 0 else 0) + (SFC_FP_NP / NP_Y0 if NP_Y0 > 0 else 0)\n",
    "    AOD = (SFR_AOD_np - SFR_AOD_p) / 2\n",
    "\n",
    "    # ERD Calculation\n",
    "    SFC_FN_P = np.sum([(-1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_FN_NP = np.sum([(-1 if y_true_i == 1 and y1_i == 0 and y2_i == 1 else 1 if y_true_i == 1 and y1_i == 1 and y2_i == 0 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "    SFC_FP_P = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                       for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr == 'male' or attr == 1])\n",
    "    SFC_FP_NP = np.sum([(1 if y_true_i == 0 and y1_i == 1 and y2_i == 0 else -1 if y_true_i == 0 and y1_i == 0 and y2_i == 1 else 0)\n",
    "                        for y_true_i, y1_i, y2_i, attr in zip(y, y1, y2, attribute) if attr != 'male' and attr != 1])\n",
    "\n",
    "    SFR_ERR_p = (SFC_FP_P / P_Y0 if P_Y0 > 0 else 0) + (SFC_FN_P / P_Y1 if P_Y1 > 0 else 0)\n",
    "    SFR_ERR_np = (SFC_FP_NP / NP_Y0 if NP_Y0 > 0 else 0) + (SFC_FN_NP / NP_Y1 if NP_Y1 > 0 else 0)\n",
    "    ERD = SFR_ERR_np - SFR_ERR_p\n",
    "\n",
    "    return SPD, EOD, AOD, ERD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'C:/Users/Saadia/FairPreprocessing/data/bank/bank-additional-full.csv'\n",
    "na_values = ['unknown']\n",
    "df = pd.read_csv(file_path, sep=';', na_values=na_values)\n",
    "df['age_mapped'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
    "\n",
    "# Separate Features and Target\n",
    "X = df.drop(['y'], axis=1)\n",
    "y = df['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Ensure 'age_mapped' is always present\n",
    "X['age_mapped'] = df['age_mapped']\n",
    "\n",
    "# Function to flip the binary race/age attribute\n",
    "def flip_race(attribute):\n",
    "    return attribute.apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# Custom LabelEncoder that handles unseen labels\n",
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        for col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X[col].astype(str))  # Ensure that all data is treated as strings\n",
    "            self.encoders[col] = le\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        for col, le in self.encoders.items():\n",
    "            unseen_label_value = -1\n",
    "            X_encoded[col] = X[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else unseen_label_value)\n",
    "        return X_encoded\n",
    "\n",
    "# Defining the pipelines\n",
    "# pipeline_1 = Pipeline(steps=[\n",
    "#     ('label_encoding', CustomLabelEncoder()),\n",
    "#     ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "# ])\n",
    "\n",
    "# pipeline_2 = Pipeline(steps=[\n",
    "#     ('preprocessor', ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num', 'passthrough', numerical_cols),\n",
    "#             ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "#         ]\n",
    "#     )),\n",
    "#     ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "# ])\n",
    "\n",
    "def compute_propensity_scores(X, treatment_column):\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Define the treatment variable\n",
    "    treatment = X_imputed[treatment_column].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    # Check if treatment has more than one class\n",
    "    if treatment.nunique() < 2:\n",
    "        raise ValueError(f\"The treatment column '{treatment_column}' contains only one class: {treatment.unique()}. Propensity score calculation requires at least two classes.\")\n",
    "    \n",
    "    # Drop the treatment column to create the covariate matrix\n",
    "    X_covariates = X_imputed.drop(columns=[treatment_column])\n",
    "\n",
    "    # One-hot encode the covariate matrix\n",
    "    X_encoded = pd.get_dummies(X_covariates, drop_first=True)\n",
    "\n",
    "    # Estimate propensity scores using logistic regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_encoded, treatment)\n",
    "    propensity_scores = lr.predict_proba(X_encoded)[:, 1]\n",
    "\n",
    "    return propensity_scores, treatment\n",
    "\n",
    "def perform_matching(propensity_scores, treatment):\n",
    "    # Define treated and control indices based on the treatment variable\n",
    "    treated_indices = np.where((treatment == 1))[0] \n",
    "    control_indices = np.where((treatment == 0))[0]  \n",
    "\n",
    "    # Fit the nearest neighbors model on control propensity scores\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(propensity_scores[control_indices].reshape(-1, 1))\n",
    "\n",
    "    # Find the nearest neighbors for treated units\n",
    "    distances, indices = nn.kneighbors(propensity_scores[treated_indices].reshape(-1, 1))\n",
    "\n",
    "    # Map control indices to matched treated indices\n",
    "    matched_control_indices = control_indices[indices.flatten()]\n",
    "\n",
    "    # Combine treated and matched control indices\n",
    "    matched_indices = np.concatenate([treated_indices, matched_control_indices])\n",
    "\n",
    "    # Ensure indices are unique and within bounds\n",
    "    matched_indices = np.unique(matched_indices)\n",
    "    matched_indices = matched_indices[matched_indices < len(treatment)]\n",
    "\n",
    "    return matched_indices\n",
    "\n",
    "\n",
    "# Paths to train and test datasets\n",
    "# Initialize lists to store results\n",
    "spd_list = []\n",
    "eod_list = []\n",
    "aod_list = []\n",
    "erd_list = []\n",
    "\n",
    "spd_cas_list = []\n",
    "eod_cas_list = []\n",
    "aod_cas_list = []\n",
    "erd_cas_list = []\n",
    "\n",
    "gspd_list = []\n",
    "geod_list = []\n",
    "gaod_list = []\n",
    "gerd_list = []\n",
    "\n",
    "gspd_cas_list = []\n",
    "geod_cas_list = []\n",
    "gaod_cas_list = []\n",
    "gerd_cas_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=X['age_mapped'])\n",
    "    X_train_m = X_train.copy()\n",
    "    X_test_m = X_test.copy()\n",
    "    y_train_m = y_train.copy()\n",
    "    y_test_m = y_test.copy()\n",
    "\n",
    "    # Fit and predict using pipelines\n",
    "    pipeline_1.fit(X_train, y_train)\n",
    "    y_pred_1 = pipeline_1.predict(X_test)\n",
    "\n",
    "    pipeline_2.fit(X_train, y_train)\n",
    "    y_pred_2 = pipeline_2.predict(X_test)\n",
    "\n",
    "    # Fairness Metrics Computation Placeholder\n",
    "    SPD_mv, EOD_mv, AOD_mv, ERD_mv = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['age_mapped'])\n",
    "\n",
    "    spd_list.append(SPD_mv)\n",
    "    eod_list.append(EOD_mv)\n",
    "    aod_list.append(AOD_mv)\n",
    "    erd_list.append(ERD_mv)\n",
    "\n",
    "    # Counterfactual analysis: flip the age_mapped feature\n",
    "    SPD_mv, EOD_mv, AOD_mv, ERD_mv = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['age_mapped'] , compute_type='local')\n",
    "   \n",
    "    spd_list.append((SPD_mv))\n",
    "    eod_list.append((EOD_mv))\n",
    "    aod_list.append((AOD_mv))\n",
    "    erd_list.append((ERD_mv))\n",
    "\n",
    "    gSPD_mv, gEOD_mv, gAOD_mv, gERD_mv = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['age_mapped'] , compute_type='global')\n",
    "   \n",
    "    gspd_list.append((gSPD_mv))\n",
    "    geod_list.append((gEOD_mv))\n",
    "    gaod_list.append((gAOD_mv))\n",
    "    gerd_list.append((gERD_mv))\n",
    "    \n",
    "    propensity_scores1, treatment1 = compute_propensity_scores(X_test, 'age_mapped')\n",
    "    matched_indices1 = perform_matching(propensity_scores1, treatment1)\n",
    "\n",
    "    # Propensity score computation and matching for Pipeline 2\n",
    "    propensity_scores2, treatment2 = compute_propensity_scores(X_test, 'age_mapped')\n",
    "    matched_indices2 = perform_matching(propensity_scores2, treatment2)\n",
    "\n",
    "    # Propensity score computation and matching for Pipeline 3\n",
    "  \n",
    "    # Find common matched indices across all pipelines\n",
    "    common_matched_indices = set(matched_indices1).intersection(matched_indices2)\n",
    "    common_matched_indices = list(common_matched_indices)\n",
    "\n",
    "    # Ensure the indices are sorted to maintain order\n",
    "    common_matched_indices.sort()\n",
    "\n",
    "    # Applying matching to y_pred results and the actual y_test and Sex\n",
    "    y1_matched = y_pred_1[common_matched_indices]\n",
    "    y2_matched = y_pred_2[common_matched_indices]\n",
    "\n",
    "    y_test_matched = y_test.reset_index(drop=True)[common_matched_indices]\n",
    "    sex_matched = X_test['age_mapped'].reset_index(drop=True)[common_matched_indices]\n",
    "\n",
    "    SPD_cas_mv, EOD_cas_mv, AOD_cas_mv, ERD_cas_mv = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched,compute_type='local')\n",
    "\n",
    "#     # Causal fairness metrics for Feature Engineering impact\n",
    "    spd_cas_list.append((SPD_cas_mv))\n",
    "    eod_cas_list.append((EOD_cas_mv))\n",
    "    aod_cas_list.append((AOD_cas_mv))\n",
    "    erd_cas_list.append((ERD_cas_mv))   \n",
    "\n",
    "    gSPD_cas_mv, gEOD_cas_mv, gAOD_cas_mv, gERD_cas_mv = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched,compute_type='global')\n",
    "\n",
    "#     # Causal fairness metrics for Feature Engineering impact\n",
    "    gspd_cas_list.append((gSPD_cas_mv))\n",
    "    geod_cas_list.append((gEOD_cas_mv))\n",
    "    gaod_cas_list.append((gAOD_cas_mv))\n",
    "    gerd_cas_list.append((gERD_cas_mv))   \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means_and_errors(array):\n",
    "    mean = array.mean(axis=0) * 0.1\n",
    "    se = (array.std(axis=0) / np.sqrt(array.shape[0])) * 0.1\n",
    "    return np.round(mean, 3), np.round(se, 3)\n",
    "\n",
    "\n",
    "spd_array = np.array(spd_list)\n",
    "eod_array = np.array(eod_list)\n",
    "aod_array = np.array(aod_list)\n",
    "erd_array = np.array(erd_list)\n",
    "\n",
    "gspd_array = np.array(gspd_list)\n",
    "geod_array = np.array(geod_list)\n",
    "gaod_array = np.array(gaod_list)\n",
    "gerd_array = np.array(gerd_list)\n",
    "\n",
    "spd_cas_array = np.array(spd_cas_list)\n",
    "eod_cas_array = np.array(eod_cas_list)\n",
    "aod_cas_array = np.array(aod_cas_list)\n",
    "erd_cas_array = np.array(erd_cas_list)\n",
    "\n",
    "gspd_cas_array = np.array(gspd_cas_list)\n",
    "geod_cas_array = np.array(geod_cas_list)\n",
    "gaod_cas_array = np.array(gaod_cas_list)\n",
    "gerd_cas_array = np.array(gerd_cas_list)\n",
    "\n",
    "\n",
    "spd_mean, spd_se = compute_means_and_errors(spd_array)\n",
    "eod_mean, eod_se = compute_means_and_errors(eod_array)\n",
    "aod_mean, aod_se = compute_means_and_errors(aod_array)\n",
    "erd_mean, erd_se = compute_means_and_errors(erd_array)\n",
    "\n",
    "gspd_mean, gspd_se = compute_means_and_errors(gspd_array)\n",
    "geod_mean, geod_se = compute_means_and_errors(geod_array)\n",
    "gaod_mean, gaod_se = compute_means_and_errors(gaod_array)\n",
    "gerd_mean, gerd_se = compute_means_and_errors(gerd_array)\n",
    "\n",
    "spd_cas_mean, spd_cas_se = compute_means_and_errors(spd_cas_array)\n",
    "eod_cas_mean, eod_cas_se = compute_means_and_errors(eod_cas_array)\n",
    "aod_cas_mean, aod_cas_se = compute_means_and_errors(aod_cas_array)\n",
    "erd_cas_mean, erd_cas_se = compute_means_and_errors(erd_cas_array)\n",
    "\n",
    "gspd_cas_mean, gspd_cas_se = compute_means_and_errors(gspd_cas_array)\n",
    "geod_cas_mean, geod_cas_se = compute_means_and_errors(geod_cas_array)\n",
    "gaod_cas_mean, gaod_cas_se = compute_means_and_errors(gaod_cas_array)\n",
    "gerd_cas_mean, gerd_cas_se = compute_means_and_errors(gerd_cas_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3edae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your fairness metrics arrays are correctly formatted\n",
    "# Metric labels\n",
    "metric_labels = ['SPD', 'EOD', 'AOD', 'ERD']\n",
    "\n",
    "# Colors for the bars\n",
    "colors = ['blue', 'orange']\n",
    "labels = ['Local', 'Global']\n",
    "\n",
    "# Plot function for fairness metrics\n",
    "def plot_fairness_metrics(ax, local_means, global_means, local_se, global_se, title, metric_labels):\n",
    "    x = np.arange(len(metric_labels))  # label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, local_means, width, label='Local', yerr=local_se, capsize=5)\n",
    "    rects2 = ax.bar(x + width/2, global_means, width, label='Global', yerr=global_se, capsize=5)\n",
    "\n",
    "    # Add labels, title, and custom ticks\n",
    "    ax.set_xlabel('Fairness Metrics')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.legend()\n",
    "\n",
    "# Replace these with the actual means and errors for statistical and causal fairness\n",
    "# Statistical Local and Global for SS\n",
    "local_mean_stage_1 = [spd_mean, eod_mean, aod_mean, erd_mean]\n",
    "local_se_stage_1 = [spd_se, eod_se, aod_se, erd_se]\n",
    "\n",
    "global_mean_stage_1 = [gspd_mean, geod_mean, gaod_mean, gerd_mean]\n",
    "global_se_stage_1 = [gspd_se, geod_se, gaod_se, gerd_se]\n",
    "\n",
    "# Causal Local and Global for SS\n",
    "causal_means_stage_1 = [spd_cas_mean, eod_cas_mean, aod_cas_mean, erd_cas_mean]\n",
    "causal_se_stage_1 = [spd_cas_se ,eod_cas_se, aod_cas_se, erd_cas_se]\n",
    "\n",
    "gcausal_means_stage_1 = [gspd_cas_mean, geod_cas_mean, gaod_cas_mean, gerd_cas_mean]\n",
    "gcausal_se_stage_1 = [gspd_cas_se, geod_cas_se, gaod_cas_se, gerd_cas_se]\n",
    "\n",
    "\n",
    "\n",
    "# Create subplots with 2 rows and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Statistical Local and Global Fairness for MV\n",
    "plot_fairness_metrics(axs[1], local_mean_stage_1, global_mean_stage_1, local_se_stage_1, global_se_stage_1, \n",
    "                      'Causal Local and Global Fairness - RW', metric_labels)\n",
    "\n",
    "# Plot 2: Causal Local and Global Fairness for MV\n",
    "plot_fairness_metrics(axs[0], causal_means_stage_1, gcausal_means_stage_1, causal_se_stage_1, gcausal_se_stage_1, \n",
    "                      'Statistical Local and Global Fairness - RW', metric_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'C:/Users/Saadia/FairPreprocessing/data/bank/bank-additional-full.csv'\n",
    "na_values = ['unknown']\n",
    "df = pd.read_csv(file_path, sep=';', na_values=na_values)\n",
    "df['age_mapped'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
    "\n",
    "# Separate Features and Target\n",
    "X = df.drop(['y'], axis=1)\n",
    "y = df['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Ensure 'age_mapped' is always present\n",
    "X['age_mapped'] = df['age_mapped']\n",
    "\n",
    "# Function to flip the binary race/age attribute\n",
    "def flip_race(attribute):\n",
    "    return attribute.apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# Custom LabelEncoder that handles unseen labels\n",
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        for col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X[col].astype(str))  # Ensure that all data is treated as strings\n",
    "            self.encoders[col] = le\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        for col, le in self.encoders.items():\n",
    "            unseen_label_value = -1\n",
    "            X_encoded[col] = X[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else unseen_label_value)\n",
    "        return X_encoded\n",
    "\n",
    "# Defining the pipelines\n",
    "# pipeline_1 = Pipeline(steps=[\n",
    "#     ('label_encoding', CustomLabelEncoder()),\n",
    "#     ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "# ])\n",
    "\n",
    "# pipeline_2 = Pipeline(steps=[\n",
    "#     ('preprocessor', ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num', 'passthrough', numerical_cols),\n",
    "#             ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "#         ]\n",
    "#     )),\n",
    "#     ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "# ])\n",
    "\n",
    "def compute_propensity_scores(X, treatment_column):\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Define the treatment variable\n",
    "    treatment = X_imputed[treatment_column].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    # Check if treatment has more than one class\n",
    "    if treatment.nunique() < 2:\n",
    "        raise ValueError(f\"The treatment column '{treatment_column}' contains only one class: {treatment.unique()}. Propensity score calculation requires at least two classes.\")\n",
    "    \n",
    "    # Drop the treatment column to create the covariate matrix\n",
    "    X_covariates = X_imputed.drop(columns=[treatment_column])\n",
    "\n",
    "    # One-hot encode the covariate matrix\n",
    "    X_encoded = pd.get_dummies(X_covariates, drop_first=True)\n",
    "\n",
    "    # Estimate propensity scores using logistic regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_encoded, treatment)\n",
    "    propensity_scores = lr.predict_proba(X_encoded)[:, 1]\n",
    "\n",
    "    return propensity_scores, treatment\n",
    "\n",
    "def perform_matching(propensity_scores, treatment):\n",
    "    # Define treated and control indices based on the treatment variable\n",
    "    treated_indices = np.where((treatment == 1))[0] \n",
    "    control_indices = np.where((treatment == 0))[0]  \n",
    "\n",
    "    # Fit the nearest neighbors model on control propensity scores\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(propensity_scores[control_indices].reshape(-1, 1))\n",
    "\n",
    "    # Find the nearest neighbors for treated units\n",
    "    distances, indices = nn.kneighbors(propensity_scores[treated_indices].reshape(-1, 1))\n",
    "\n",
    "    # Map control indices to matched treated indices\n",
    "    matched_control_indices = control_indices[indices.flatten()]\n",
    "\n",
    "    # Combine treated and matched control indices\n",
    "    matched_indices = np.concatenate([treated_indices, matched_control_indices])\n",
    "\n",
    "    # Ensure indices are unique and within bounds\n",
    "    matched_indices = np.unique(matched_indices)\n",
    "    matched_indices = matched_indices[matched_indices < len(treatment)]\n",
    "\n",
    "    return matched_indices\n",
    "\n",
    "\n",
    "# Paths to train and test datasets\n",
    "# Initialize lists to store results\n",
    "spd_list = []\n",
    "eod_list = []\n",
    "aod_list = []\n",
    "erd_list = []\n",
    "\n",
    "spd_cas_list = []\n",
    "eod_cas_list = []\n",
    "aod_cas_list = []\n",
    "erd_cas_list = []\n",
    "\n",
    "gspd_list = []\n",
    "geod_list = []\n",
    "gaod_list = []\n",
    "gerd_list = []\n",
    "\n",
    "gspd_cas_list = []\n",
    "geod_cas_list = []\n",
    "gaod_cas_list = []\n",
    "gerd_cas_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=X['age_mapped'])\n",
    "    X_train_m = X_train.copy()\n",
    "    X_test_m = X_test.copy()\n",
    "    y_train_m = y_train.copy()\n",
    "    y_test_m = y_test.copy()\n",
    "\n",
    "    # Fit and predict using pipelines\n",
    "    pipeline_1.fit(X_train, y_train)\n",
    "    y_pred_1 = pipeline_1.predict(X_test)\n",
    "\n",
    "    pipeline_2.fit(X_train, y_train)\n",
    "    y_pred_2 = pipeline_2.predict(X_test)\n",
    "    \n",
    "    res1 = guard_and_mitigate_if_needed(\n",
    "        pipeline=pipeline_1,\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test,   y_test=y_test,\n",
    "        sensitive_attr_test=s_test,\n",
    "        thresholds=thresholds,\n",
    "        mitigations_cfg=mitigations_cfg,  # the same list you pass to run_mitigations_and_predict\n",
    "        context=common_context,           # e.g., {\"s_train\": s_train, ...}\n",
    "        fairness_compute_type=\"global\",   # or \"local\"\n",
    "        y2_scores=None                    # provide scores if your fairness function needs them\n",
    "    )\n",
    "    y_pred_1 = res1[\"y_pred_final\"]\n",
    "\n",
    "    res2 = guard_and_mitigate_if_needed(\n",
    "        pipeline=pipeline_2, X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test, y_test=y_test, sensitive_attr_test=s_test,\n",
    "        thresholds=thresholds, mitigations_cfg=mitigations_cfg, context=common_context\n",
    "    )\n",
    "    y_pred_2 = res2[\"y_pred_final\"]\n",
    "\n",
    "\n",
    "    # Fairness Metrics Computation Placeholder\n",
    "    SPD_mv, EOD_mv, AOD_mv, ERD_mv = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['age_mapped'])\n",
    "\n",
    "    spd_list.append(SPD_mv)\n",
    "    eod_list.append(EOD_mv)\n",
    "    aod_list.append(AOD_mv)\n",
    "    erd_list.append(ERD_mv)\n",
    "\n",
    "    # Counterfactual analysis: flip the age_mapped feature\n",
    "    SPD_mv, EOD_mv, AOD_mv, ERD_mv = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['age_mapped'] , compute_type='local')\n",
    "   \n",
    "    spd_list.append((SPD_mv))\n",
    "    eod_list.append((EOD_mv))\n",
    "    aod_list.append((AOD_mv))\n",
    "    erd_list.append((ERD_mv))\n",
    "\n",
    "    gSPD_mv, gEOD_mv, gAOD_mv, gERD_mv = Fairness_Metrics_Computation(y_pred_1, y_pred_2, y_test, X_test['age_mapped'] , compute_type='global')\n",
    "   \n",
    "    gspd_list.append((gSPD_mv))\n",
    "    geod_list.append((gEOD_mv))\n",
    "    gaod_list.append((gAOD_mv))\n",
    "    gerd_list.append((gERD_mv))\n",
    "    \n",
    "    propensity_scores1, treatment1 = compute_propensity_scores(X_test, 'age_mapped')\n",
    "    matched_indices1 = perform_matching(propensity_scores1, treatment1)\n",
    "\n",
    "    # Propensity score computation and matching for Pipeline 2\n",
    "    propensity_scores2, treatment2 = compute_propensity_scores(X_test, 'age_mapped')\n",
    "    matched_indices2 = perform_matching(propensity_scores2, treatment2)\n",
    "\n",
    "    # Propensity score computation and matching for Pipeline 3\n",
    "  \n",
    "    # Find common matched indices across all pipelines\n",
    "    common_matched_indices = set(matched_indices1).intersection(matched_indices2)\n",
    "    common_matched_indices = list(common_matched_indices)\n",
    "\n",
    "    # Ensure the indices are sorted to maintain order\n",
    "    common_matched_indices.sort()\n",
    "\n",
    "    # Applying matching to y_pred results and the actual y_test and Sex\n",
    "    y1_matched = y_pred_1[common_matched_indices]\n",
    "    y2_matched = y_pred_2[common_matched_indices]\n",
    "\n",
    "    y_test_matched = y_test.reset_index(drop=True)[common_matched_indices]\n",
    "    sex_matched = X_test['age_mapped'].reset_index(drop=True)[common_matched_indices]\n",
    "\n",
    "    SPD_cas_mv, EOD_cas_mv, AOD_cas_mv, ERD_cas_mv = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched,compute_type='local')\n",
    "\n",
    "#     # Causal fairness metrics for Feature Engineering impact\n",
    "    spd_cas_list.append((SPD_cas_mv))\n",
    "    eod_cas_list.append((EOD_cas_mv))\n",
    "    aod_cas_list.append((AOD_cas_mv))\n",
    "    erd_cas_list.append((ERD_cas_mv))   \n",
    "\n",
    "    gSPD_cas_mv, gEOD_cas_mv, gAOD_cas_mv, gERD_cas_mv = Fairness_Metrics_Computation(y1_matched, y2_matched, y_test_matched, sex_matched,compute_type='global')\n",
    "\n",
    "#     # Causal fairness metrics for Feature Engineering impact\n",
    "    gspd_cas_list.append((gSPD_cas_mv))\n",
    "    geod_cas_list.append((gEOD_cas_mv))\n",
    "    gaod_cas_list.append((gAOD_cas_mv))\n",
    "    gerd_cas_list.append((gERD_cas_mv))   \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means_and_errors(array):\n",
    "    mean = array.mean(axis=0) * 0.1\n",
    "    se = (array.std(axis=0) / np.sqrt(array.shape[0])) * 0.1\n",
    "    return np.round(mean, 3), np.round(se, 3)\n",
    "\n",
    "\n",
    "spd_array = np.array(spd_list)\n",
    "eod_array = np.array(eod_list)\n",
    "aod_array = np.array(aod_list)\n",
    "erd_array = np.array(erd_list)\n",
    "\n",
    "gspd_array = np.array(gspd_list)\n",
    "geod_array = np.array(geod_list)\n",
    "gaod_array = np.array(gaod_list)\n",
    "gerd_array = np.array(gerd_list)\n",
    "\n",
    "spd_cas_array = np.array(spd_cas_list)\n",
    "eod_cas_array = np.array(eod_cas_list)\n",
    "aod_cas_array = np.array(aod_cas_list)\n",
    "erd_cas_array = np.array(erd_cas_list)\n",
    "\n",
    "gspd_cas_array = np.array(gspd_cas_list)\n",
    "geod_cas_array = np.array(geod_cas_list)\n",
    "gaod_cas_array = np.array(gaod_cas_list)\n",
    "gerd_cas_array = np.array(gerd_cas_list)\n",
    "\n",
    "\n",
    "spd_mean, spd_se = compute_means_and_errors(spd_array)\n",
    "eod_mean, eod_se = compute_means_and_errors(eod_array)\n",
    "aod_mean, aod_se = compute_means_and_errors(aod_array)\n",
    "erd_mean, erd_se = compute_means_and_errors(erd_array)\n",
    "\n",
    "gspd_mean, gspd_se = compute_means_and_errors(gspd_array)\n",
    "geod_mean, geod_se = compute_means_and_errors(geod_array)\n",
    "gaod_mean, gaod_se = compute_means_and_errors(gaod_array)\n",
    "gerd_mean, gerd_se = compute_means_and_errors(gerd_array)\n",
    "\n",
    "spd_cas_mean, spd_cas_se = compute_means_and_errors(spd_cas_array)\n",
    "eod_cas_mean, eod_cas_se = compute_means_and_errors(eod_cas_array)\n",
    "aod_cas_mean, aod_cas_se = compute_means_and_errors(aod_cas_array)\n",
    "erd_cas_mean, erd_cas_se = compute_means_and_errors(erd_cas_array)\n",
    "\n",
    "gspd_cas_mean, gspd_cas_se = compute_means_and_errors(gspd_cas_array)\n",
    "geod_cas_mean, geod_cas_se = compute_means_and_errors(geod_cas_array)\n",
    "gaod_cas_mean, gaod_cas_se = compute_means_and_errors(gaod_cas_array)\n",
    "gerd_cas_mean, gerd_cas_se = compute_means_and_errors(gerd_cas_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b7a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b325a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your fairness metrics arrays are correctly formatted\n",
    "# Metric labels\n",
    "metric_labels = ['SPD', 'EOD', 'AOD', 'ERD']\n",
    "\n",
    "# Colors for the bars\n",
    "colors = ['blue', 'orange']\n",
    "labels = ['Local', 'Global']\n",
    "\n",
    "# Plot function for fairness metrics\n",
    "def plot_fairness_metrics(ax, local_means, global_means, local_se, global_se, title, metric_labels):\n",
    "    x = np.arange(len(metric_labels))  # label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, local_means, width, label='Local', yerr=local_se, capsize=5)\n",
    "    rects2 = ax.bar(x + width/2, global_means, width, label='Global', yerr=global_se, capsize=5)\n",
    "\n",
    "    # Add labels, title, and custom ticks\n",
    "    ax.set_xlabel('Fairness Metrics')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.legend()\n",
    "\n",
    "# Replace these with the actual means and errors for statistical and causal fairness\n",
    "# Statistical Local and Global for SS\n",
    "local_mean_stage_1 = [spd_mean, eod_mean, aod_mean, erd_mean]\n",
    "local_se_stage_1 = [spd_se, eod_se, aod_se, erd_se]\n",
    "\n",
    "global_mean_stage_1 = [gspd_mean, geod_mean, gaod_mean, gerd_mean]\n",
    "global_se_stage_1 = [gspd_se, geod_se, gaod_se, gerd_se]\n",
    "\n",
    "# Causal Local and Global for SS\n",
    "causal_means_stage_1 = [spd_cas_mean, eod_cas_mean, aod_cas_mean, erd_cas_mean]\n",
    "causal_se_stage_1 = [spd_cas_se ,eod_cas_se, aod_cas_se, erd_cas_se]\n",
    "\n",
    "gcausal_means_stage_1 = [gspd_cas_mean, geod_cas_mean, gaod_cas_mean, gerd_cas_mean]\n",
    "gcausal_se_stage_1 = [gspd_cas_se, geod_cas_se, gaod_cas_se, gerd_cas_se]\n",
    "\n",
    "\n",
    "\n",
    "# Create subplots with 2 rows and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Statistical Local and Global Fairness for MV\n",
    "plot_fairness_metrics(axs[1], local_mean_stage_1, global_mean_stage_1, local_se_stage_1, global_se_stage_1, \n",
    "                      'Causal Local and Global Fairness - RW', metric_labels)\n",
    "\n",
    "# Plot 2: Causal Local and Global Fairness for MV\n",
    "plot_fairness_metrics(axs[0], causal_means_stage_1, gcausal_means_stage_1, causal_se_stage_1, gcausal_se_stage_1, \n",
    "                      'Statistical Local and Global Fairness - RW', metric_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb323b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
